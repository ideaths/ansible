2025-10-25 00:21:58,121 p=2920471 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ****
2025-10-25 00:22:00,679 p=2920471 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 00:22:00,680 p=2920471 u=root n=ansible | ok: [k8s-m1]
2025-10-25 00:22:00,725 p=2920471 u=root n=ansible | ok: [k8s-w02]
2025-10-25 00:22:00,740 p=2920471 u=root n=ansible | ok: [k8s-m2]
2025-10-25 00:22:00,815 p=2920471 u=root n=ansible | ok: [k8s-m3]
2025-10-25 00:22:01,084 p=2920471 u=root n=ansible | ok: [k8s-w01]
2025-10-25 00:22:03,303 p=2920471 u=root n=ansible | ok: [k8s-w03]
2025-10-25 00:22:03,507 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 00:22:03,508 p=2920471 u=root n=ansible | ok: [k8s-m1]
2025-10-25 00:22:03,508 p=2920471 u=root n=ansible | ok: [k8s-m2]
2025-10-25 00:22:03,508 p=2920471 u=root n=ansible | ok: [k8s-m3]
2025-10-25 00:22:03,523 p=2920471 u=root n=ansible | ok: [k8s-w01]
2025-10-25 00:22:03,538 p=2920471 u=root n=ansible | ok: [k8s-w02]
2025-10-25 00:22:03,555 p=2920471 u=root n=ansible | ok: [k8s-w03]
2025-10-25 00:22:04,794 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] **********************
2025-10-25 00:22:04,794 p=2920471 u=root n=ansible | changed: [k8s-m2]
2025-10-25 00:22:04,799 p=2920471 u=root n=ansible | changed: [k8s-w01]
2025-10-25 00:22:04,801 p=2920471 u=root n=ansible | changed: [k8s-m1]
2025-10-25 00:22:04,804 p=2920471 u=root n=ansible | changed: [k8s-m3]
2025-10-25 00:22:04,852 p=2920471 u=root n=ansible | changed: [k8s-w02]
2025-10-25 00:22:05,592 p=2920471 u=root n=ansible | changed: [k8s-w03]
2025-10-25 00:22:05,822 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] ******************
2025-10-25 00:22:05,938 p=2920471 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 00:22:06,799 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Create RKE2 artifacts folder] ******************************
2025-10-25 00:22:06,800 p=2920471 u=root n=ansible | changed: [k8s-m1]
2025-10-25 00:22:06,801 p=2920471 u=root n=ansible | changed: [k8s-w01]
2025-10-25 00:22:06,801 p=2920471 u=root n=ansible | changed: [k8s-m2]
2025-10-25 00:22:06,805 p=2920471 u=root n=ansible | changed: [k8s-m3]
2025-10-25 00:22:06,805 p=2920471 u=root n=ansible | changed: [k8s-w02]
2025-10-25 00:22:07,128 p=2920471 u=root n=ansible | changed: [k8s-w03]
2025-10-25 00:22:09,209 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Download sha256 checksum file ( airgap mode )] *************
2025-10-25 00:22:09,210 p=2920471 u=root n=ansible | [WARNING]: Module remote_tmp /root/.ansible/tmp did not exist and was created
with a mode of 0700, this may cause issues when running as another user. To
avoid this, create the remote_tmp dir with the correct permissions manually

2025-10-25 00:22:09,210 p=2920471 u=root n=ansible | changed: [k8s-w01]
2025-10-25 00:22:09,211 p=2920471 u=root n=ansible | changed: [k8s-w02]
2025-10-25 00:22:09,212 p=2920471 u=root n=ansible | changed: [k8s-m3]
2025-10-25 00:22:09,213 p=2920471 u=root n=ansible | changed: [k8s-m2]
2025-10-25 00:22:09,267 p=2920471 u=root n=ansible | changed: [k8s-m1]
2025-10-25 00:22:09,864 p=2920471 u=root n=ansible | changed: [k8s-w03]
2025-10-25 00:22:11,687 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 artifacts and compare with checksums ( airgap mode )] ***
2025-10-25 00:22:11,687 p=2920471 u=root n=ansible | changed: [k8s-w01] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 00:22:11,838 p=2920471 u=root n=ansible | changed: [k8s-m1] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 00:22:11,840 p=2920471 u=root n=ansible | changed: [k8s-w02] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 00:22:11,853 p=2920471 u=root n=ansible | changed: [k8s-m2] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 00:22:11,876 p=2920471 u=root n=ansible | changed: [k8s-m3] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 00:24:02,776 p=2920471 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 00:24:02,778 p=2920471 u=root n=ansible | changed: [k8s-m3] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 00:24:02,779 p=2920471 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 00:24:02,780 p=2920471 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 00:24:02,781 p=2920471 u=root n=ansible | changed: [k8s-m2] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 00:24:18,937 p=2920471 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 00:24:18,953 p=2920471 u=root n=ansible | changed: [k8s-m3] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 00:24:18,978 p=2920471 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 00:24:19,053 p=2920471 u=root n=ansible | changed: [k8s-m2] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 00:24:19,142 p=2920471 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 00:24:20,788 p=2920471 u=root n=ansible | changed: [k8s-w03] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 00:24:40,850 p=2920471 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 00:24:51,391 p=2920471 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 00:24:52,026 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 install script ( airgap mode )] **************
2025-10-25 00:24:52,027 p=2920471 u=root n=ansible | changed: [k8s-m3]
2025-10-25 00:24:52,034 p=2920471 u=root n=ansible | changed: [k8s-m2]
2025-10-25 00:24:52,034 p=2920471 u=root n=ansible | changed: [k8s-w01]
2025-10-25 00:24:52,040 p=2920471 u=root n=ansible | changed: [k8s-m1]
2025-10-25 00:24:52,042 p=2920471 u=root n=ansible | changed: [k8s-w02]
2025-10-25 00:24:52,568 p=2920471 u=root n=ansible | changed: [k8s-w03]
2025-10-25 00:24:58,717 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Populate service facts] ************************************
2025-10-25 00:24:58,718 p=2920471 u=root n=ansible | ok: [k8s-w02]
2025-10-25 00:24:58,741 p=2920471 u=root n=ansible | ok: [k8s-w01]
2025-10-25 00:24:58,747 p=2920471 u=root n=ansible | ok: [k8s-m2]
2025-10-25 00:24:58,832 p=2920471 u=root n=ansible | ok: [k8s-m1]
2025-10-25 00:24:58,885 p=2920471 u=root n=ansible | ok: [k8s-m3]
2025-10-25 00:25:04,062 p=2920471 u=root n=ansible | ok: [k8s-w03]
2025-10-25 00:25:04,625 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Get stats of the FS object] ********************************
2025-10-25 00:25:04,625 p=2920471 u=root n=ansible | ok: [k8s-m3]
2025-10-25 00:25:04,635 p=2920471 u=root n=ansible | ok: [k8s-w01]
2025-10-25 00:25:04,635 p=2920471 u=root n=ansible | ok: [k8s-w02]
2025-10-25 00:25:04,641 p=2920471 u=root n=ansible | ok: [k8s-m2]
2025-10-25 00:25:04,647 p=2920471 u=root n=ansible | ok: [k8s-m1]
2025-10-25 00:25:04,996 p=2920471 u=root n=ansible | ok: [k8s-w03]
2025-10-25 00:25:05,584 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Check if separate partition] *******************************
2025-10-25 00:25:05,585 p=2920471 u=root n=ansible | ok: [k8s-m2]
2025-10-25 00:25:05,588 p=2920471 u=root n=ansible | ok: [k8s-m1]
2025-10-25 00:25:05,597 p=2920471 u=root n=ansible | ok: [k8s-w01]
2025-10-25 00:25:05,606 p=2920471 u=root n=ansible | ok: [k8s-w02]
2025-10-25 00:25:05,615 p=2920471 u=root n=ansible | ok: [k8s-m3]
2025-10-25 00:25:05,917 p=2920471 u=root n=ansible | ok: [k8s-w03]
2025-10-25 00:25:06,107 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 bin path] *****************************************
2025-10-25 00:25:06,108 p=2920471 u=root n=ansible | ok: [k8s-m1]
2025-10-25 00:25:06,109 p=2920471 u=root n=ansible | ok: [k8s-m2]
2025-10-25 00:25:06,109 p=2920471 u=root n=ansible | ok: [k8s-w02]
2025-10-25 00:25:06,109 p=2920471 u=root n=ansible | ok: [k8s-w01]
2025-10-25 00:25:06,110 p=2920471 u=root n=ansible | ok: [k8s-m3]
2025-10-25 00:25:06,144 p=2920471 u=root n=ansible | ok: [k8s-w03]
2025-10-25 00:25:06,530 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Check RKE2 version] ****************************************
2025-10-25 00:25:06,531 p=2920471 u=root n=ansible | ok: [k8s-m1]
2025-10-25 00:25:06,577 p=2920471 u=root n=ansible | ok: [k8s-m2]
2025-10-25 00:25:06,587 p=2920471 u=root n=ansible | ok: [k8s-w01]
2025-10-25 00:25:06,590 p=2920471 u=root n=ansible | ok: [k8s-m3]
2025-10-25 00:25:06,593 p=2920471 u=root n=ansible | ok: [k8s-w02]
2025-10-25 00:25:06,893 p=2920471 u=root n=ansible | ok: [k8s-w03]
2025-10-25 00:25:06,976 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 versions] *****************************************
2025-10-25 00:25:06,977 p=2920471 u=root n=ansible | ok: [k8s-m1]
2025-10-25 00:25:06,997 p=2920471 u=root n=ansible | ok: [k8s-m2]
2025-10-25 00:25:07,020 p=2920471 u=root n=ansible | ok: [k8s-m3]
2025-10-25 00:25:07,021 p=2920471 u=root n=ansible | ok: [k8s-w01]
2025-10-25 00:25:07,034 p=2920471 u=root n=ansible | ok: [k8s-w02]
2025-10-25 00:25:07,055 p=2920471 u=root n=ansible | ok: [k8s-w03]
2025-10-25 00:26:04,524 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Run RKE2 install script with airgap variables] *************
2025-10-25 00:26:04,525 p=2920471 u=root n=ansible | ok: [k8s-m3]
2025-10-25 00:26:05,479 p=2920471 u=root n=ansible | ok: [k8s-m1]
2025-10-25 00:26:05,665 p=2920471 u=root n=ansible | ok: [k8s-m2]
2025-10-25 00:26:27,135 p=2920471 u=root n=ansible | ok: [k8s-w02]
2025-10-25 00:26:28,473 p=2920471 u=root n=ansible | ok: [k8s-w01]
2025-10-25 00:27:12,498 p=2920471 u=root n=ansible | ok: [k8s-w03]
2025-10-25 00:27:13,361 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Find Active Server] ****************************************
2025-10-25 00:27:13,509 p=2920471 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/find_active_server.yml for k8s-m1, k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 00:27:18,650 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Populate services facts] ***********************************
2025-10-25 00:27:18,650 p=2920471 u=root n=ansible | ok: [k8s-m1]
2025-10-25 00:27:18,664 p=2920471 u=root n=ansible | ok: [k8s-m3]
2025-10-25 00:27:18,700 p=2920471 u=root n=ansible | ok: [k8s-w01]
2025-10-25 00:27:18,717 p=2920471 u=root n=ansible | ok: [k8s-w02]
2025-10-25 00:27:18,731 p=2920471 u=root n=ansible | ok: [k8s-m2]
2025-10-25 00:27:24,337 p=2920471 u=root n=ansible | ok: [k8s-w03]
2025-10-25 00:27:25,141 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Copy kube-vip manifests to the masternode] *****************
2025-10-25 00:27:25,228 p=2920471 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/kubevip.yml for k8s-m1
2025-10-25 00:27:25,632 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Create the RKE2 manifests directory] ***********************
2025-10-25 00:27:25,632 p=2920471 u=root n=ansible | changed: [k8s-m1]
2025-10-25 00:27:26,890 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Copy kube-vip files to first server] ***********************
2025-10-25 00:27:26,891 p=2920471 u=root n=ansible | changed: [k8s-m1] => (item=/opt/git/ansible/roles/rke2-1.49.0/templates/kube-vip/kube-vip.yml.j2)
2025-10-25 00:27:27,708 p=2920471 u=root n=ansible | changed: [k8s-m1] => (item=/opt/git/ansible/roles/rke2-1.49.0/templates/kube-vip/kube-vip-rbac.yml.j2)
2025-10-25 00:27:28,004 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Prepare very first server node in the cluster] *************
2025-10-25 00:27:28,096 p=2920471 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/first_server.yml for k8s-m1
2025-10-25 00:27:28,533 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Create the RKE2 config dir] ********************************
2025-10-25 00:27:28,533 p=2920471 u=root n=ansible | changed: [k8s-m1]
2025-10-25 00:27:28,593 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Set server taints] *****************************************
2025-10-25 00:27:28,594 p=2920471 u=root n=ansible | ok: [k8s-m1]
2025-10-25 00:27:29,894 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Copy rke2 config] ******************************************
2025-10-25 00:27:29,895 p=2920471 u=root n=ansible | changed: [k8s-m1]
2025-10-25 00:27:30,968 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Copy Containerd Registry Configuration file] ***************
2025-10-25 00:27:30,969 p=2920471 u=root n=ansible | changed: [k8s-m1]
2025-10-25 00:27:33,456 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Start RKE2 service on the first server] ********************
2025-10-25 00:27:33,457 p=2920471 u=root n=ansible | fatal: [k8s-m1]: FAILED! => changed=false 
  msg: |-
    Unable to start service rke2-server.service: Job for rke2-server.service failed because the control process exited with error code.
    See "systemctl status rke2-server.service" and "journalctl -xeu rke2-server.service" for details.
2025-10-25 00:27:34,678 p=2920471 u=root n=ansible | TASK [rke2-1.49.0 : Final steps] ***********************************************
2025-10-25 00:27:34,806 p=2920471 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/summary.yml for k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 00:27:35,404 p=2920471 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 00:27:35,405 p=2920471 u=root n=ansible | k8s-m1                     : ok=25   changed=10   unreachable=0    failed=1    skipped=29   rescued=0    ignored=0   
2025-10-25 00:27:35,405 p=2920471 u=root n=ansible | k8s-m2                     : ok=18   changed=5    unreachable=0    failed=0    skipped=31   rescued=0    ignored=0   
2025-10-25 00:27:35,405 p=2920471 u=root n=ansible | k8s-m3                     : ok=18   changed=5    unreachable=0    failed=0    skipped=31   rescued=0    ignored=0   
2025-10-25 00:27:35,405 p=2920471 u=root n=ansible | k8s-w01                    : ok=18   changed=5    unreachable=0    failed=0    skipped=31   rescued=0    ignored=0   
2025-10-25 00:27:35,405 p=2920471 u=root n=ansible | k8s-w02                    : ok=18   changed=5    unreachable=0    failed=0    skipped=31   rescued=0    ignored=0   
2025-10-25 00:27:35,405 p=2920471 u=root n=ansible | k8s-w03                    : ok=18   changed=5    unreachable=0    failed=0    skipped=31   rescued=0    ignored=0   
2025-10-25 07:29:00,847 p=3045863 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 07:29:03,533 p=3045863 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 07:29:03,535 p=3045863 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:29:03,539 p=3045863 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:29:03,601 p=3045863 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:29:03,764 p=3045863 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:29:04,858 p=3045863 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:29:06,287 p=3045863 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:29:07,712 p=3045863 u=root n=ansible | TASK [Gather facts] ************************************************************
2025-10-25 07:29:07,713 p=3045863 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:29:07,728 p=3045863 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:29:07,731 p=3045863 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:29:07,735 p=3045863 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:29:07,740 p=3045863 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:29:08,999 p=3045863 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:29:10,587 p=3045863 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] *********************
2025-10-25 07:29:10,588 p=3045863 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:29:11,133 p=3045863 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:29:11,212 p=3045863 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:29:11,213 p=3045863 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:29:11,268 p=3045863 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:29:12,630 p=3045863 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:29:12,706 p=3045863 u=root n=ansible | TASK [Validate required variables] *********************************************
2025-10-25 07:29:12,707 p=3045863 u=root n=ansible | fatal: [k8s-m1]: FAILED! => changed=false 
  assertion: netplan_interface is defined
  evaluated_to: false
  msg: 'Missing required vars: netplan_interface, netplan_address, netplan_prefix'
2025-10-25 07:29:12,721 p=3045863 u=root n=ansible | fatal: [k8s-m2]: FAILED! => changed=false 
  assertion: netplan_interface is defined
  evaluated_to: false
  msg: 'Missing required vars: netplan_interface, netplan_address, netplan_prefix'
2025-10-25 07:29:12,722 p=3045863 u=root n=ansible | fatal: [k8s-m3]: FAILED! => changed=false 
  assertion: netplan_interface is defined
  evaluated_to: false
  msg: 'Missing required vars: netplan_interface, netplan_address, netplan_prefix'
2025-10-25 07:29:12,723 p=3045863 u=root n=ansible | fatal: [k8s-w01]: FAILED! => changed=false 
  assertion: netplan_interface is defined
  evaluated_to: false
  msg: 'Missing required vars: netplan_interface, netplan_address, netplan_prefix'
2025-10-25 07:29:12,728 p=3045863 u=root n=ansible | fatal: [k8s-w02]: FAILED! => changed=false 
  assertion: netplan_interface is defined
  evaluated_to: false
  msg: 'Missing required vars: netplan_interface, netplan_address, netplan_prefix'
2025-10-25 07:29:12,743 p=3045863 u=root n=ansible | fatal: [k8s-w03]: FAILED! => changed=false 
  assertion: netplan_interface is defined
  evaluated_to: false
  msg: 'Missing required vars: netplan_interface, netplan_address, netplan_prefix'
2025-10-25 07:29:12,744 p=3045863 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 07:29:12,745 p=3045863 u=root n=ansible | k8s-m1                     : ok=3    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:29:12,745 p=3045863 u=root n=ansible | k8s-m2                     : ok=3    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:29:12,745 p=3045863 u=root n=ansible | k8s-m3                     : ok=3    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:29:12,745 p=3045863 u=root n=ansible | k8s-w01                    : ok=3    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:29:12,745 p=3045863 u=root n=ansible | k8s-w02                    : ok=3    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:29:12,745 p=3045863 u=root n=ansible | k8s-w03                    : ok=3    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:31:32,305 p=3048671 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 07:31:33,986 p=3048671 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 07:31:33,987 p=3048671 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:31:33,993 p=3048671 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:31:33,997 p=3048671 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:31:34,001 p=3048671 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:31:34,014 p=3048671 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:31:35,318 p=3048671 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:31:36,750 p=3048671 u=root n=ansible | TASK [Gather facts] ************************************************************
2025-10-25 07:31:36,751 p=3048671 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:31:36,770 p=3048671 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:31:36,801 p=3048671 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:31:36,804 p=3048671 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:31:36,858 p=3048671 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:31:38,042 p=3048671 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:31:39,499 p=3048671 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] *********************
2025-10-25 07:31:39,500 p=3048671 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:31:39,541 p=3048671 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:31:39,554 p=3048671 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:31:39,574 p=3048671 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:31:39,581 p=3048671 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:31:40,759 p=3048671 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:31:40,832 p=3048671 u=root n=ansible | TASK [Set default values for netplan variables] ********************************
2025-10-25 07:31:40,834 p=3048671 u=root n=ansible | fatal: [k8s-m1]: FAILED! => 
  msg: |-
    The task includes an option with an undefined variable. The error was: 'dict object' has no attribute 'interface'. 'dict object' has no attribute 'interface'
  
    The error appears to be in '/opt/git/ansible/playbooks/configure-static-ip.yml': line 20, column 7, but may
    be elsewhere in the file depending on the exact syntax problem.
  
    The offending line appears to be:
  
  
        - name: Set default values for netplan variables
          ^ here
2025-10-25 07:31:40,834 p=3048671 u=root n=ansible | fatal: [k8s-m2]: FAILED! => 
  msg: |-
    The task includes an option with an undefined variable. The error was: 'dict object' has no attribute 'interface'. 'dict object' has no attribute 'interface'
  
    The error appears to be in '/opt/git/ansible/playbooks/configure-static-ip.yml': line 20, column 7, but may
    be elsewhere in the file depending on the exact syntax problem.
  
    The offending line appears to be:
  
  
        - name: Set default values for netplan variables
          ^ here
2025-10-25 07:31:40,835 p=3048671 u=root n=ansible | fatal: [k8s-m3]: FAILED! => 
  msg: |-
    The task includes an option with an undefined variable. The error was: 'dict object' has no attribute 'interface'. 'dict object' has no attribute 'interface'
  
    The error appears to be in '/opt/git/ansible/playbooks/configure-static-ip.yml': line 20, column 7, but may
    be elsewhere in the file depending on the exact syntax problem.
  
    The offending line appears to be:
  
  
        - name: Set default values for netplan variables
          ^ here
2025-10-25 07:31:40,841 p=3048671 u=root n=ansible | fatal: [k8s-w01]: FAILED! => 
  msg: |-
    The task includes an option with an undefined variable. The error was: 'dict object' has no attribute 'interface'. 'dict object' has no attribute 'interface'
  
    The error appears to be in '/opt/git/ansible/playbooks/configure-static-ip.yml': line 20, column 7, but may
    be elsewhere in the file depending on the exact syntax problem.
  
    The offending line appears to be:
  
  
        - name: Set default values for netplan variables
          ^ here
2025-10-25 07:31:40,847 p=3048671 u=root n=ansible | fatal: [k8s-w02]: FAILED! => 
  msg: |-
    The task includes an option with an undefined variable. The error was: 'dict object' has no attribute 'interface'. 'dict object' has no attribute 'interface'
  
    The error appears to be in '/opt/git/ansible/playbooks/configure-static-ip.yml': line 20, column 7, but may
    be elsewhere in the file depending on the exact syntax problem.
  
    The offending line appears to be:
  
  
        - name: Set default values for netplan variables
          ^ here
2025-10-25 07:31:40,861 p=3048671 u=root n=ansible | fatal: [k8s-w03]: FAILED! => 
  msg: |-
    The task includes an option with an undefined variable. The error was: 'dict object' has no attribute 'interface'. 'dict object' has no attribute 'interface'
  
    The error appears to be in '/opt/git/ansible/playbooks/configure-static-ip.yml': line 20, column 7, but may
    be elsewhere in the file depending on the exact syntax problem.
  
    The offending line appears to be:
  
  
        - name: Set default values for netplan variables
          ^ here
2025-10-25 07:31:40,863 p=3048671 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 07:31:40,864 p=3048671 u=root n=ansible | k8s-m1                     : ok=3    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:31:40,864 p=3048671 u=root n=ansible | k8s-m2                     : ok=3    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:31:40,864 p=3048671 u=root n=ansible | k8s-m3                     : ok=3    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:31:40,865 p=3048671 u=root n=ansible | k8s-w01                    : ok=3    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:31:40,865 p=3048671 u=root n=ansible | k8s-w02                    : ok=3    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:31:40,865 p=3048671 u=root n=ansible | k8s-w03                    : ok=3    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:34:48,114 p=3052530 u=root n=ansible | ERROR! the playbook: ansible/playbooks/configure-static-ip.yml could not be found
2025-10-25 07:35:18,455 p=3053235 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 07:35:20,125 p=3053235 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 07:35:20,126 p=3053235 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:35:20,133 p=3053235 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:35:20,137 p=3053235 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:35:20,140 p=3053235 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:35:21,094 p=3053235 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:35:21,463 p=3053235 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:35:22,843 p=3053235 u=root n=ansible | TASK [Gather facts] ************************************************************
2025-10-25 07:35:22,844 p=3053235 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:35:22,853 p=3053235 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:35:22,857 p=3053235 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:35:22,897 p=3053235 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:35:22,965 p=3053235 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:35:25,127 p=3053235 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:35:26,594 p=3053235 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] *********************
2025-10-25 07:35:26,595 p=3053235 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:35:26,597 p=3053235 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:35:26,643 p=3053235 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:35:26,675 p=3053235 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:35:26,714 p=3053235 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:35:27,775 p=3053235 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:35:28,295 p=3053235 u=root n=ansible | TASK [Detect default interface via ip route] ***********************************
2025-10-25 07:35:28,296 p=3053235 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:35:28,297 p=3053235 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:35:28,304 p=3053235 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:35:28,313 p=3053235 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:35:28,313 p=3053235 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:35:28,626 p=3053235 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:35:28,736 p=3053235 u=root n=ansible | TASK [Set default values for netplan variables] ********************************
2025-10-25 07:35:28,736 p=3053235 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:35:28,740 p=3053235 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:35:28,740 p=3053235 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:35:28,756 p=3053235 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:35:28,767 p=3053235 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:35:28,789 p=3053235 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:35:28,839 p=3053235 u=root n=ansible | TASK [Validate required variables] *********************************************
2025-10-25 07:35:28,841 p=3053235 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: All assertions passed
2025-10-25 07:35:28,870 p=3053235 u=root n=ansible | ok: [k8s-m3] => changed=false 
  msg: All assertions passed
2025-10-25 07:35:28,871 p=3053235 u=root n=ansible | ok: [k8s-m2] => changed=false 
  msg: All assertions passed
2025-10-25 07:35:28,871 p=3053235 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: All assertions passed
2025-10-25 07:35:28,879 p=3053235 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: All assertions passed
2025-10-25 07:35:28,894 p=3053235 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: All assertions passed
2025-10-25 07:35:29,283 p=3053235 u=root n=ansible | TASK [Backup existing netplan config if present] *******************************
2025-10-25 07:35:29,283 p=3053235 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:35:29,292 p=3053235 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:35:29,292 p=3053235 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:35:29,295 p=3053235 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:35:29,311 p=3053235 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:35:29,623 p=3053235 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:35:30,789 p=3053235 u=root n=ansible | TASK [Render static netplan config] ********************************************
2025-10-25 07:35:30,789 p=3053235 u=root n=ansible | changed: [k8s-m3]
2025-10-25 07:35:30,794 p=3053235 u=root n=ansible | changed: [k8s-m1]
2025-10-25 07:35:30,803 p=3053235 u=root n=ansible | changed: [k8s-w02]
2025-10-25 07:35:30,805 p=3053235 u=root n=ansible | changed: [k8s-m2]
2025-10-25 07:35:30,820 p=3053235 u=root n=ansible | changed: [k8s-w01]
2025-10-25 07:35:31,611 p=3053235 u=root n=ansible | changed: [k8s-w03]
2025-10-25 07:35:32,255 p=3053235 u=root n=ansible | TASK [Apply netplan] ***********************************************************
2025-10-25 07:35:32,257 p=3053235 u=root n=ansible | fatal: [k8s-m1]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.291783'
  end: '2025-10-25 00:35:33.703258'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 00:35:33.411475'
  stderr: |2-
  
    ** (generate:2664): WARNING **: 00:35:33.674: Permissions for /etc/netplan/01-static.yaml are too open. Netplan configuration should NOT be accessible by others.
    /etc/netplan/01-static.yaml:13:13: Error in network definition: unknown renderer 'systemd-networkd'
      renderer: systemd-networkd
                ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 07:35:32,288 p=3053235 u=root n=ansible | fatal: [k8s-m2]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.299597'
  end: '2025-10-25 00:35:33.738472'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 00:35:33.438875'
  stderr: |2-
  
    ** (generate:3284): WARNING **: 00:35:33.708: Permissions for /etc/netplan/01-static.yaml are too open. Netplan configuration should NOT be accessible by others.
    /etc/netplan/01-static.yaml:13:13: Error in network definition: unknown renderer 'systemd-networkd'
      renderer: systemd-networkd
                ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 07:35:32,289 p=3053235 u=root n=ansible | fatal: [k8s-m3]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.288450'
  end: '2025-10-25 00:35:33.744227'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 00:35:33.455777'
  stderr: |2-
  
    ** (generate:2415): WARNING **: 00:35:33.712: Permissions for /etc/netplan/01-static.yaml are too open. Netplan configuration should NOT be accessible by others.
    /etc/netplan/01-static.yaml:13:13: Error in network definition: unknown renderer 'systemd-networkd'
      renderer: systemd-networkd
                ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 07:35:32,318 p=3053235 u=root n=ansible | fatal: [k8s-w01]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.300073'
  end: '2025-10-25 00:35:33.770184'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 00:35:33.470111'
  stderr: |2-
  
    ** (generate:2386): WARNING **: 00:35:33.736: Permissions for /etc/netplan/01-static.yaml are too open. Netplan configuration should NOT be accessible by others.
    /etc/netplan/01-static.yaml:13:13: Error in network definition: unknown renderer 'systemd-networkd'
      renderer: systemd-networkd
                ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 07:35:32,329 p=3053235 u=root n=ansible | fatal: [k8s-w02]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.289684'
  end: '2025-10-25 00:35:33.784812'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 00:35:33.495128'
  stderr: |2-
  
    ** (generate:2504): WARNING **: 00:35:33.751: Permissions for /etc/netplan/01-static.yaml are too open. Netplan configuration should NOT be accessible by others.
    /etc/netplan/01-static.yaml:13:13: Error in network definition: unknown renderer 'systemd-networkd'
      renderer: systemd-networkd
                ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 07:35:32,941 p=3053235 u=root n=ansible | fatal: [k8s-w03]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.360805'
  end: '2025-10-25 00:35:34.388184'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 00:35:34.027379'
  stderr: |2-
  
    ** (generate:2525): WARNING **: 00:35:34.360: Permissions for /etc/netplan/01-static.yaml are too open. Netplan configuration should NOT be accessible by others.
    /etc/netplan/01-static.yaml:13:13: Error in network definition: unknown renderer 'systemd-networkd'
      renderer: systemd-networkd
                ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 07:35:32,943 p=3053235 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 07:35:32,943 p=3053235 u=root n=ansible | k8s-m1                     : ok=8    changed=1    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:35:32,944 p=3053235 u=root n=ansible | k8s-m2                     : ok=8    changed=1    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:35:32,944 p=3053235 u=root n=ansible | k8s-m3                     : ok=8    changed=1    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:35:32,944 p=3053235 u=root n=ansible | k8s-w01                    : ok=8    changed=1    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:35:32,944 p=3053235 u=root n=ansible | k8s-w02                    : ok=8    changed=1    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:35:32,944 p=3053235 u=root n=ansible | k8s-w03                    : ok=8    changed=1    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:37:49,307 p=3055894 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 07:37:50,924 p=3055894 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 07:37:50,925 p=3055894 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:37:50,930 p=3055894 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:37:50,953 p=3055894 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:37:50,959 p=3055894 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:37:50,967 p=3055894 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:37:53,376 p=3055894 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:37:54,825 p=3055894 u=root n=ansible | TASK [Gather facts] ************************************************************
2025-10-25 07:37:54,825 p=3055894 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:37:54,829 p=3055894 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:37:54,832 p=3055894 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:37:54,837 p=3055894 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:37:54,840 p=3055894 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:37:57,215 p=3055894 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:37:58,676 p=3055894 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] *********************
2025-10-25 07:37:58,677 p=3055894 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:37:58,698 p=3055894 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:37:58,736 p=3055894 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:37:58,795 p=3055894 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:37:58,837 p=3055894 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:38:00,021 p=3055894 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:38:00,495 p=3055894 u=root n=ansible | TASK [Detect default interface via ip route] ***********************************
2025-10-25 07:38:00,495 p=3055894 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:38:00,496 p=3055894 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:38:00,498 p=3055894 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:38:00,499 p=3055894 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:38:00,499 p=3055894 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:38:00,844 p=3055894 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:38:00,929 p=3055894 u=root n=ansible | TASK [Set default values for netplan variables] ********************************
2025-10-25 07:38:00,930 p=3055894 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:38:00,930 p=3055894 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:38:00,930 p=3055894 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:38:00,947 p=3055894 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:38:00,949 p=3055894 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:38:00,979 p=3055894 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:38:01,044 p=3055894 u=root n=ansible | TASK [Validate required variables] *********************************************
2025-10-25 07:38:01,045 p=3055894 u=root n=ansible | ok: [k8s-m2] => changed=false 
  msg: All assertions passed
2025-10-25 07:38:01,059 p=3055894 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: All assertions passed
2025-10-25 07:38:01,062 p=3055894 u=root n=ansible | ok: [k8s-m3] => changed=false 
  msg: All assertions passed
2025-10-25 07:38:01,062 p=3055894 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: All assertions passed
2025-10-25 07:38:01,082 p=3055894 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: All assertions passed
2025-10-25 07:38:01,084 p=3055894 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: All assertions passed
2025-10-25 07:38:01,470 p=3055894 u=root n=ansible | TASK [Backup existing netplan config if present] *******************************
2025-10-25 07:38:01,471 p=3055894 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:38:01,488 p=3055894 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:38:01,506 p=3055894 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:38:01,520 p=3055894 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:38:01,526 p=3055894 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:38:01,855 p=3055894 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:38:02,992 p=3055894 u=root n=ansible | TASK [Render static netplan config] ********************************************
2025-10-25 07:38:02,992 p=3055894 u=root n=ansible | changed: [k8s-m1]
2025-10-25 07:38:02,995 p=3055894 u=root n=ansible | changed: [k8s-w01]
2025-10-25 07:38:02,996 p=3055894 u=root n=ansible | changed: [k8s-m3]
2025-10-25 07:38:02,996 p=3055894 u=root n=ansible | changed: [k8s-m2]
2025-10-25 07:38:03,007 p=3055894 u=root n=ansible | changed: [k8s-w02]
2025-10-25 07:38:03,884 p=3055894 u=root n=ansible | changed: [k8s-w03]
2025-10-25 07:38:05,234 p=3055894 u=root n=ansible | TASK [Apply netplan] ***********************************************************
2025-10-25 07:38:05,235 p=3055894 u=root n=ansible | changed: [k8s-m1]
2025-10-25 07:38:05,279 p=3055894 u=root n=ansible | changed: [k8s-w01]
2025-10-25 07:38:05,301 p=3055894 u=root n=ansible | changed: [k8s-w02]
2025-10-25 07:38:05,411 p=3055894 u=root n=ansible | changed: [k8s-m3]
2025-10-25 07:38:05,440 p=3055894 u=root n=ansible | changed: [k8s-m2]
2025-10-25 07:38:06,494 p=3055894 u=root n=ansible | changed: [k8s-w03]
2025-10-25 07:38:06,906 p=3055894 u=root n=ansible | TASK [Verify network configuration] ********************************************
2025-10-25 07:38:06,906 p=3055894 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:38:06,916 p=3055894 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:38:06,921 p=3055894 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:38:06,967 p=3055894 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:38:07,288 p=3055894 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:38:07,626 p=3055894 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:38:07,662 p=3055894 u=root n=ansible | TASK [Show verification output] ************************************************
2025-10-25 07:38:07,664 p=3055894 u=root n=ansible | ok: [k8s-m1] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.12/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.12
2025-10-25 07:38:07,683 p=3055894 u=root n=ansible | ok: [k8s-m2] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.16/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
        inet 10.0.6.11/24 metric 100 brd 10.0.6.255 scope global secondary dynamic ens34
           valid_lft 1799sec preferred_lft 1799sec
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.16
    10.0.6.1 dev ens34 proto dhcp scope link src 10.0.6.11 metric 100
2025-10-25 07:38:07,699 p=3055894 u=root n=ansible | ok: [k8s-m3] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.13/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.13
2025-10-25 07:38:07,713 p=3055894 u=root n=ansible | ok: [k8s-w01] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.14/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.14
2025-10-25 07:38:07,728 p=3055894 u=root n=ansible | ok: [k8s-w02] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.15/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.15
2025-10-25 07:38:07,729 p=3055894 u=root n=ansible | ok: [k8s-w03] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.16/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
        inet 10.0.6.11/24 metric 100 brd 10.0.6.255 scope global secondary dynamic ens34
           valid_lft 1800sec preferred_lft 1800sec
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.16
    10.0.6.1 dev ens34 proto dhcp scope link src 10.0.6.11 metric 100
2025-10-25 07:38:07,872 p=3055894 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 07:38:07,873 p=3055894 u=root n=ansible | k8s-m1                     : ok=11   changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 07:38:07,873 p=3055894 u=root n=ansible | k8s-m2                     : ok=11   changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 07:38:07,873 p=3055894 u=root n=ansible | k8s-m3                     : ok=11   changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 07:38:07,873 p=3055894 u=root n=ansible | k8s-w01                    : ok=11   changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 07:38:07,873 p=3055894 u=root n=ansible | k8s-w02                    : ok=11   changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 07:38:07,873 p=3055894 u=root n=ansible | k8s-w03                    : ok=11   changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 07:47:14,087 p=3068064 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 07:47:17,072 p=3068064 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 07:47:17,074 p=3068064 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:47:17,242 p=3068064 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:47:17,500 p=3068064 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:47:17,503 p=3068064 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:47:17,618 p=3068064 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:47:19,285 p=3068064 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:47:20,721 p=3068064 u=root n=ansible | TASK [Gather facts] ************************************************************
2025-10-25 07:47:20,721 p=3068064 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:47:20,724 p=3068064 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:47:20,728 p=3068064 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:47:20,731 p=3068064 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:47:20,748 p=3068064 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:47:22,080 p=3068064 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:47:23,558 p=3068064 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] *********************
2025-10-25 07:47:23,558 p=3068064 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:47:23,565 p=3068064 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:47:23,587 p=3068064 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:47:23,593 p=3068064 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:47:23,648 p=3068064 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:47:24,855 p=3068064 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:47:25,360 p=3068064 u=root n=ansible | TASK [Detect default interface via ip route] ***********************************
2025-10-25 07:47:25,361 p=3068064 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:47:25,362 p=3068064 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:47:25,362 p=3068064 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:47:25,368 p=3068064 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:47:25,368 p=3068064 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:47:25,680 p=3068064 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:47:25,777 p=3068064 u=root n=ansible | TASK [Set default values for netplan variables] ********************************
2025-10-25 07:47:25,777 p=3068064 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:47:25,777 p=3068064 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:47:25,778 p=3068064 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:47:25,793 p=3068064 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:47:25,802 p=3068064 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:47:25,811 p=3068064 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:47:25,866 p=3068064 u=root n=ansible | TASK [Validate required variables] *********************************************
2025-10-25 07:47:25,867 p=3068064 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: All assertions passed
2025-10-25 07:47:25,879 p=3068064 u=root n=ansible | ok: [k8s-m2] => changed=false 
  msg: All assertions passed
2025-10-25 07:47:25,897 p=3068064 u=root n=ansible | ok: [k8s-m3] => changed=false 
  msg: All assertions passed
2025-10-25 07:47:25,897 p=3068064 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: All assertions passed
2025-10-25 07:47:25,903 p=3068064 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: All assertions passed
2025-10-25 07:47:25,920 p=3068064 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: All assertions passed
2025-10-25 07:47:26,320 p=3068064 u=root n=ansible | TASK [Move existing netplan YAMLs to timestamped backup directory] *************
2025-10-25 07:47:26,321 p=3068064 u=root n=ansible | changed: [k8s-m2]
2025-10-25 07:47:26,330 p=3068064 u=root n=ansible | changed: [k8s-m1]
2025-10-25 07:47:26,330 p=3068064 u=root n=ansible | changed: [k8s-m3]
2025-10-25 07:47:26,335 p=3068064 u=root n=ansible | changed: [k8s-w02]
2025-10-25 07:47:26,336 p=3068064 u=root n=ansible | changed: [k8s-w01]
2025-10-25 07:47:26,709 p=3068064 u=root n=ansible | changed: [k8s-w03]
2025-10-25 07:47:27,784 p=3068064 u=root n=ansible | TASK [Render static netplan config] ********************************************
2025-10-25 07:47:27,785 p=3068064 u=root n=ansible | changed: [k8s-m3]
2025-10-25 07:47:27,792 p=3068064 u=root n=ansible | changed: [k8s-w01]
2025-10-25 07:47:27,794 p=3068064 u=root n=ansible | changed: [k8s-w02]
2025-10-25 07:47:27,801 p=3068064 u=root n=ansible | changed: [k8s-m1]
2025-10-25 07:47:27,811 p=3068064 u=root n=ansible | changed: [k8s-m2]
2025-10-25 07:47:28,567 p=3068064 u=root n=ansible | changed: [k8s-w03]
2025-10-25 07:47:29,851 p=3068064 u=root n=ansible | TASK [Apply netplan] ***********************************************************
2025-10-25 07:47:29,851 p=3068064 u=root n=ansible | changed: [k8s-m3]
2025-10-25 07:47:29,900 p=3068064 u=root n=ansible | changed: [k8s-m1]
2025-10-25 07:47:29,933 p=3068064 u=root n=ansible | changed: [k8s-w01]
2025-10-25 07:47:30,351 p=3068064 u=root n=ansible | changed: [k8s-m2]
2025-10-25 07:47:31,098 p=3068064 u=root n=ansible | changed: [k8s-w03]
2025-10-25 07:48:30,767 p=3068064 u=root n=ansible |  [ERROR]: User interrupted execution

2025-10-25 07:51:15,605 p=3073883 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 07:51:17,317 p=3073883 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 07:51:17,318 p=3073883 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:51:17,347 p=3073883 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:51:17,350 p=3073883 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:51:18,683 p=3073883 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:51:30,422 p=3073883 u=root n=ansible |  [ERROR]: User interrupted execution

2025-10-25 07:52:07,433 p=3075264 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 07:52:09,084 p=3075264 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 07:52:09,085 p=3075264 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:52:09,093 p=3075264 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:52:09,099 p=3075264 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:52:10,017 p=3075264 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:52:10,043 p=3075264 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:52:10,428 p=3075264 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:52:11,846 p=3075264 u=root n=ansible | TASK [Gather facts] ************************************************************
2025-10-25 07:52:11,847 p=3075264 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:52:11,858 p=3075264 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:52:11,875 p=3075264 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:52:11,886 p=3075264 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:52:12,985 p=3075264 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:52:13,379 p=3075264 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:52:14,845 p=3075264 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] *********************
2025-10-25 07:52:14,846 p=3075264 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:52:14,973 p=3075264 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:52:15,135 p=3075264 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:52:15,156 p=3075264 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:52:15,330 p=3075264 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:52:16,056 p=3075264 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:52:16,592 p=3075264 u=root n=ansible | TASK [Detect default interface via ip route] ***********************************
2025-10-25 07:52:16,592 p=3075264 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:52:16,593 p=3075264 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:52:16,593 p=3075264 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:52:16,594 p=3075264 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:52:16,597 p=3075264 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:52:16,928 p=3075264 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:52:17,006 p=3075264 u=root n=ansible | TASK [Set default values for netplan variables] ********************************
2025-10-25 07:52:17,007 p=3075264 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:52:17,022 p=3075264 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:52:17,033 p=3075264 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:52:17,038 p=3075264 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:52:17,045 p=3075264 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:52:17,068 p=3075264 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:52:17,114 p=3075264 u=root n=ansible | TASK [Validate required variables] *********************************************
2025-10-25 07:52:17,116 p=3075264 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: All assertions passed
2025-10-25 07:52:17,128 p=3075264 u=root n=ansible | ok: [k8s-m2] => changed=false 
  msg: All assertions passed
2025-10-25 07:52:17,148 p=3075264 u=root n=ansible | ok: [k8s-m3] => changed=false 
  msg: All assertions passed
2025-10-25 07:52:17,149 p=3075264 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: All assertions passed
2025-10-25 07:52:17,152 p=3075264 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: All assertions passed
2025-10-25 07:52:17,169 p=3075264 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: All assertions passed
2025-10-25 07:52:17,542 p=3075264 u=root n=ansible | TASK [Move existing netplan YAMLs to timestamped backup directory] *************
2025-10-25 07:52:17,543 p=3075264 u=root n=ansible | changed: [k8s-m1]
2025-10-25 07:52:17,554 p=3075264 u=root n=ansible | changed: [k8s-m2]
2025-10-25 07:52:17,587 p=3075264 u=root n=ansible | changed: [k8s-w02]
2025-10-25 07:52:17,594 p=3075264 u=root n=ansible | changed: [k8s-m3]
2025-10-25 07:52:17,597 p=3075264 u=root n=ansible | changed: [k8s-w01]
2025-10-25 07:52:17,874 p=3075264 u=root n=ansible | changed: [k8s-w03]
2025-10-25 07:52:18,983 p=3075264 u=root n=ansible | TASK [Render static netplan config] ********************************************
2025-10-25 07:52:18,984 p=3075264 u=root n=ansible | changed: [k8s-w02]
2025-10-25 07:52:18,989 p=3075264 u=root n=ansible | changed: [k8s-w01]
2025-10-25 07:52:18,994 p=3075264 u=root n=ansible | changed: [k8s-m1]
2025-10-25 07:52:18,995 p=3075264 u=root n=ansible | changed: [k8s-m2]
2025-10-25 07:52:18,997 p=3075264 u=root n=ansible | changed: [k8s-m3]
2025-10-25 07:52:19,804 p=3075264 u=root n=ansible | changed: [k8s-w03]
2025-10-25 07:52:21,082 p=3075264 u=root n=ansible | TASK [Apply netplan] ***********************************************************
2025-10-25 07:52:21,083 p=3075264 u=root n=ansible | changed: [k8s-m1]
2025-10-25 07:52:21,117 p=3075264 u=root n=ansible | changed: [k8s-m3]
2025-10-25 07:52:21,128 p=3075264 u=root n=ansible | changed: [k8s-w02]
2025-10-25 07:52:21,142 p=3075264 u=root n=ansible | changed: [k8s-w01]
2025-10-25 07:52:21,149 p=3075264 u=root n=ansible | changed: [k8s-m2]
2025-10-25 07:52:22,353 p=3075264 u=root n=ansible | changed: [k8s-w03]
2025-10-25 07:52:22,781 p=3075264 u=root n=ansible | TASK [Verify network configuration] ********************************************
2025-10-25 07:52:22,782 p=3075264 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:52:22,790 p=3075264 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:52:22,797 p=3075264 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:52:22,810 p=3075264 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:52:22,817 p=3075264 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:52:23,138 p=3075264 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:52:23,180 p=3075264 u=root n=ansible | TASK [Show verification output] ************************************************
2025-10-25 07:52:23,182 p=3075264 u=root n=ansible | ok: [k8s-m1] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.12/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.12
2025-10-25 07:52:23,203 p=3075264 u=root n=ansible | ok: [k8s-m2] => 
  net_verify.stdout: |-
    3: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.11/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.11
2025-10-25 07:52:23,218 p=3075264 u=root n=ansible | ok: [k8s-m3] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.13/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.13
2025-10-25 07:52:23,237 p=3075264 u=root n=ansible | ok: [k8s-w01] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.14/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.14
2025-10-25 07:52:23,238 p=3075264 u=root n=ansible | ok: [k8s-w02] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.15/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.15
2025-10-25 07:52:23,252 p=3075264 u=root n=ansible | ok: [k8s-w03] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.16/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.16
2025-10-25 07:52:23,384 p=3075264 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 07:52:23,384 p=3075264 u=root n=ansible | k8s-m1                     : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 07:52:23,385 p=3075264 u=root n=ansible | k8s-m2                     : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 07:52:23,385 p=3075264 u=root n=ansible | k8s-m3                     : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 07:52:23,385 p=3075264 u=root n=ansible | k8s-w01                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 07:52:23,385 p=3075264 u=root n=ansible | k8s-w02                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 07:52:23,386 p=3075264 u=root n=ansible | k8s-w03                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 07:54:51,700 p=3079079 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ****
2025-10-25 07:54:53,494 p=3079079 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 07:54:53,496 p=3079079 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:54:53,550 p=3079079 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:54:53,554 p=3079079 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:54:54,344 p=3079079 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:54:55,019 p=3079079 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:54:55,342 p=3079079 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:54:55,473 p=3079079 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 07:54:55,474 p=3079079 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:54:55,491 p=3079079 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:54:55,508 p=3079079 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:54:55,524 p=3079079 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:54:55,536 p=3079079 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:54:55,544 p=3079079 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:54:56,722 p=3079079 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] **********************
2025-10-25 07:54:56,723 p=3079079 u=root n=ansible | changed: [k8s-m3]
2025-10-25 07:54:56,726 p=3079079 u=root n=ansible | changed: [k8s-m1]
2025-10-25 07:54:56,731 p=3079079 u=root n=ansible | changed: [k8s-w01]
2025-10-25 07:54:56,796 p=3079079 u=root n=ansible | changed: [k8s-m2]
2025-10-25 07:54:57,508 p=3079079 u=root n=ansible | changed: [k8s-w03]
2025-10-25 07:54:57,716 p=3079079 u=root n=ansible | changed: [k8s-w02]
2025-10-25 07:54:57,877 p=3079079 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] ******************
2025-10-25 07:54:57,991 p=3079079 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 07:54:58,786 p=3079079 u=root n=ansible | TASK [rke2-1.49.0 : Create RKE2 artifacts folder] ******************************
2025-10-25 07:54:58,787 p=3079079 u=root n=ansible | changed: [k8s-m3]
2025-10-25 07:54:58,788 p=3079079 u=root n=ansible | changed: [k8s-w02]
2025-10-25 07:54:58,788 p=3079079 u=root n=ansible | changed: [k8s-m1]
2025-10-25 07:54:58,788 p=3079079 u=root n=ansible | changed: [k8s-m2]
2025-10-25 07:54:58,815 p=3079079 u=root n=ansible | changed: [k8s-w01]
2025-10-25 07:54:59,112 p=3079079 u=root n=ansible | changed: [k8s-w03]
2025-10-25 07:54:59,874 p=3079079 u=root n=ansible | TASK [rke2-1.49.0 : Download sha256 checksum file ( airgap mode )] *************
2025-10-25 07:54:59,875 p=3079079 u=root n=ansible | [WARNING]: Module remote_tmp /root/.ansible/tmp did not exist and was created
with a mode of 0700, this may cause issues when running as another user. To
avoid this, create the remote_tmp dir with the correct permissions manually

2025-10-25 07:54:59,875 p=3079079 u=root n=ansible | changed: [k8s-w01]
2025-10-25 07:54:59,876 p=3079079 u=root n=ansible | changed: [k8s-m1]
2025-10-25 07:54:59,876 p=3079079 u=root n=ansible | changed: [k8s-m2]
2025-10-25 07:54:59,877 p=3079079 u=root n=ansible | changed: [k8s-m3]
2025-10-25 07:54:59,877 p=3079079 u=root n=ansible | changed: [k8s-w02]
2025-10-25 07:55:00,412 p=3079079 u=root n=ansible | changed: [k8s-w03]
2025-10-25 07:55:02,029 p=3079079 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 artifacts and compare with checksums ( airgap mode )] ***
2025-10-25 07:55:02,030 p=3079079 u=root n=ansible | changed: [k8s-m3] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 07:55:02,081 p=3079079 u=root n=ansible | changed: [k8s-m1] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 07:55:02,082 p=3079079 u=root n=ansible | changed: [k8s-w01] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 07:55:02,140 p=3079079 u=root n=ansible | changed: [k8s-w02] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 07:55:02,183 p=3079079 u=root n=ansible | changed: [k8s-m2] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 07:56:10,150 p=3080902 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ****
2025-10-25 07:56:11,990 p=3080902 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 07:56:11,992 p=3080902 u=root n=ansible | ok: [k8s-m2]
2025-10-25 07:56:12,002 p=3080902 u=root n=ansible | ok: [k8s-w01]
2025-10-25 07:56:12,189 p=3080902 u=root n=ansible | ok: [k8s-m3]
2025-10-25 07:56:12,260 p=3080902 u=root n=ansible | ok: [k8s-m1]
2025-10-25 07:56:12,598 p=3080902 u=root n=ansible | ok: [k8s-w02]
2025-10-25 07:56:13,381 p=3080902 u=root n=ansible | ok: [k8s-w03]
2025-10-25 07:56:13,545 p=3080902 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 07:56:13,547 p=3080902 u=root n=ansible | fatal: [k8s-m2]: FAILED! => 
  msg: 'The task includes an option with an undefined variable. The error was: ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address'''
2025-10-25 07:56:13,566 p=3080902 u=root n=ansible | fatal: [k8s-m1]: FAILED! => 
  msg: 'The task includes an option with an undefined variable. The error was: ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address'''
2025-10-25 07:56:13,566 p=3080902 u=root n=ansible | fatal: [k8s-m3]: FAILED! => 
  msg: 'The task includes an option with an undefined variable. The error was: ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address'''
2025-10-25 07:56:13,578 p=3080902 u=root n=ansible | fatal: [k8s-w02]: FAILED! => 
  msg: 'The task includes an option with an undefined variable. The error was: ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address'''
2025-10-25 07:56:13,584 p=3080902 u=root n=ansible | fatal: [k8s-w01]: FAILED! => 
  msg: 'The task includes an option with an undefined variable. The error was: ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address'''
2025-10-25 07:56:13,607 p=3080902 u=root n=ansible | fatal: [k8s-w03]: FAILED! => 
  msg: 'The task includes an option with an undefined variable. The error was: ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address''. ''dict object'' has no attribute ''address'''
2025-10-25 07:56:13,609 p=3080902 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 07:56:13,610 p=3080902 u=root n=ansible | k8s-m1                     : ok=1    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:56:13,610 p=3080902 u=root n=ansible | k8s-m2                     : ok=1    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:56:13,610 p=3080902 u=root n=ansible | k8s-m3                     : ok=1    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:56:13,610 p=3080902 u=root n=ansible | k8s-w01                    : ok=1    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:56:13,610 p=3080902 u=root n=ansible | k8s-w02                    : ok=1    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 07:56:13,610 p=3080902 u=root n=ansible | k8s-w03                    : ok=1    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:00:24,925 p=3090331 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 08:00:26,637 p=3090331 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 08:00:26,638 p=3090331 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:00:26,642 p=3090331 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:00:26,685 p=3090331 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:00:26,697 p=3090331 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:00:26,804 p=3090331 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:00:27,982 p=3090331 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:00:29,445 p=3090331 u=root n=ansible | TASK [Gather facts] ************************************************************
2025-10-25 08:00:29,446 p=3090331 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:00:29,452 p=3090331 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:00:29,457 p=3090331 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:00:29,461 p=3090331 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:00:29,465 p=3090331 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:00:30,775 p=3090331 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:00:32,229 p=3090331 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] *********************
2025-10-25 08:00:32,229 p=3090331 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:00:32,392 p=3090331 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:00:32,416 p=3090331 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:00:32,464 p=3090331 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:00:32,485 p=3090331 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:00:33,455 p=3090331 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:00:33,958 p=3090331 u=root n=ansible | TASK [Detect default interface via ip route] ***********************************
2025-10-25 08:00:33,959 p=3090331 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:00:33,979 p=3090331 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:00:33,980 p=3090331 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:00:34,004 p=3090331 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:00:34,049 p=3090331 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:00:34,306 p=3090331 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:00:34,600 p=3090331 u=root n=ansible | TASK [Set default values for netplan variables] ********************************
2025-10-25 08:00:34,601 p=3090331 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:00:34,627 p=3090331 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:00:34,642 p=3090331 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:00:34,654 p=3090331 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:00:34,675 p=3090331 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:00:34,687 p=3090331 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:00:34,757 p=3090331 u=root n=ansible | TASK [Validate required variables] *********************************************
2025-10-25 08:00:34,759 p=3090331 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: All assertions passed
2025-10-25 08:00:34,777 p=3090331 u=root n=ansible | ok: [k8s-m2] => changed=false 
  msg: All assertions passed
2025-10-25 08:00:34,792 p=3090331 u=root n=ansible | ok: [k8s-m3] => changed=false 
  msg: All assertions passed
2025-10-25 08:00:34,792 p=3090331 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: All assertions passed
2025-10-25 08:00:34,802 p=3090331 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: All assertions passed
2025-10-25 08:00:34,817 p=3090331 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: All assertions passed
2025-10-25 08:00:35,224 p=3090331 u=root n=ansible | TASK [Move existing netplan YAMLs to timestamped backup directory] *************
2025-10-25 08:00:35,224 p=3090331 u=root n=ansible | changed: [k8s-m2]
2025-10-25 08:00:35,267 p=3090331 u=root n=ansible | changed: [k8s-w01]
2025-10-25 08:00:35,915 p=3090331 u=root n=ansible | changed: [k8s-m3]
2025-10-25 08:00:35,916 p=3090331 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:00:35,916 p=3090331 u=root n=ansible | changed: [k8s-w02]
2025-10-25 08:00:36,262 p=3090331 u=root n=ansible | changed: [k8s-w03]
2025-10-25 08:00:37,654 p=3090331 u=root n=ansible | TASK [Render static netplan config] ********************************************
2025-10-25 08:00:37,655 p=3090331 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:00:37,679 p=3090331 u=root n=ansible | changed: [k8s-m3]
2025-10-25 08:00:37,725 p=3090331 u=root n=ansible | changed: [k8s-w02]
2025-10-25 08:00:37,796 p=3090331 u=root n=ansible | changed: [k8s-m2]
2025-10-25 08:00:37,905 p=3090331 u=root n=ansible | changed: [k8s-w01]
2025-10-25 08:00:38,545 p=3090331 u=root n=ansible | changed: [k8s-w03]
2025-10-25 08:00:39,157 p=3090331 u=root n=ansible | TASK [Apply netplan] ***********************************************************
2025-10-25 08:00:39,159 p=3090331 u=root n=ansible | fatal: [k8s-m1]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.253993'
  end: '2025-10-25 01:00:40.613173'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:00:40.359180'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:00:39,171 p=3090331 u=root n=ansible | fatal: [k8s-m2]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.253923'
  end: '2025-10-25 01:00:40.640297'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:00:40.386374'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:00:39,203 p=3090331 u=root n=ansible | fatal: [k8s-m3]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.289841'
  end: '2025-10-25 01:00:40.673457'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:00:40.383616'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:00:39,209 p=3090331 u=root n=ansible | fatal: [k8s-w01]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.260895'
  end: '2025-10-25 01:00:40.683076'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:00:40.422181'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:00:39,216 p=3090331 u=root n=ansible | fatal: [k8s-w02]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.253326'
  end: '2025-10-25 01:00:40.094113'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:00:39.840787'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:00:39,776 p=3090331 u=root n=ansible | fatal: [k8s-w03]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.281037'
  end: '2025-10-25 01:00:41.235389'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:00:40.954352'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:00:39,778 p=3090331 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 08:00:39,778 p=3090331 u=root n=ansible | k8s-m1                     : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:00:39,779 p=3090331 u=root n=ansible | k8s-m2                     : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:00:39,779 p=3090331 u=root n=ansible | k8s-m3                     : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:00:39,780 p=3090331 u=root n=ansible | k8s-w01                    : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:00:39,780 p=3090331 u=root n=ansible | k8s-w02                    : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:00:39,780 p=3090331 u=root n=ansible | k8s-w03                    : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:02:09,244 p=3094422 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 08:02:10,862 p=3094422 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 08:02:10,863 p=3094422 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:02:10,873 p=3094422 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:02:10,884 p=3094422 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:02:10,890 p=3094422 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:02:10,895 p=3094422 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:02:13,223 p=3094422 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:02:14,640 p=3094422 u=root n=ansible | TASK [Gather facts] ************************************************************
2025-10-25 08:02:14,641 p=3094422 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:02:14,655 p=3094422 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:02:14,658 p=3094422 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:02:14,678 p=3094422 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:02:14,692 p=3094422 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:02:16,051 p=3094422 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:02:17,641 p=3094422 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] *********************
2025-10-25 08:02:17,641 p=3094422 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:02:17,643 p=3094422 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:02:17,654 p=3094422 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:02:17,656 p=3094422 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:02:17,664 p=3094422 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:02:18,833 p=3094422 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:02:19,308 p=3094422 u=root n=ansible | TASK [Detect default interface via ip route] ***********************************
2025-10-25 08:02:19,308 p=3094422 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:02:19,311 p=3094422 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:02:19,323 p=3094422 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:02:19,341 p=3094422 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:02:19,342 p=3094422 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:02:19,662 p=3094422 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:02:19,751 p=3094422 u=root n=ansible | TASK [Set default values for netplan variables] ********************************
2025-10-25 08:02:19,751 p=3094422 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:02:19,751 p=3094422 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:02:19,751 p=3094422 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:02:19,754 p=3094422 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:02:19,779 p=3094422 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:02:19,784 p=3094422 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:02:19,834 p=3094422 u=root n=ansible | TASK [Validate required variables] *********************************************
2025-10-25 08:02:19,835 p=3094422 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: All assertions passed
2025-10-25 08:02:19,849 p=3094422 u=root n=ansible | ok: [k8s-m2] => changed=false 
  msg: All assertions passed
2025-10-25 08:02:19,863 p=3094422 u=root n=ansible | ok: [k8s-m3] => changed=false 
  msg: All assertions passed
2025-10-25 08:02:19,864 p=3094422 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: All assertions passed
2025-10-25 08:02:19,873 p=3094422 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: All assertions passed
2025-10-25 08:02:19,887 p=3094422 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: All assertions passed
2025-10-25 08:02:20,259 p=3094422 u=root n=ansible | TASK [Move existing netplan YAMLs to timestamped backup directory] *************
2025-10-25 08:02:20,260 p=3094422 u=root n=ansible | changed: [k8s-m2]
2025-10-25 08:02:20,272 p=3094422 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:02:20,280 p=3094422 u=root n=ansible | changed: [k8s-w01]
2025-10-25 08:02:20,291 p=3094422 u=root n=ansible | changed: [k8s-m3]
2025-10-25 08:02:20,308 p=3094422 u=root n=ansible | changed: [k8s-w02]
2025-10-25 08:02:20,595 p=3094422 u=root n=ansible | changed: [k8s-w03]
2025-10-25 08:02:21,706 p=3094422 u=root n=ansible | TASK [Render static netplan config] ********************************************
2025-10-25 08:02:21,706 p=3094422 u=root n=ansible | changed: [k8s-m3]
2025-10-25 08:02:21,715 p=3094422 u=root n=ansible | changed: [k8s-w02]
2025-10-25 08:02:21,718 p=3094422 u=root n=ansible | changed: [k8s-w01]
2025-10-25 08:02:21,718 p=3094422 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:02:21,722 p=3094422 u=root n=ansible | changed: [k8s-m2]
2025-10-25 08:02:22,508 p=3094422 u=root n=ansible | changed: [k8s-w03]
2025-10-25 08:02:23,141 p=3094422 u=root n=ansible | TASK [Apply netplan] ***********************************************************
2025-10-25 08:02:23,143 p=3094422 u=root n=ansible | fatal: [k8s-m2]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.253971'
  end: '2025-10-25 01:02:24.608786'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:02:24.354815'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:02:23,150 p=3094422 u=root n=ansible | fatal: [k8s-m3]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.265020'
  end: '2025-10-25 01:02:24.622821'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:02:24.357801'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:02:23,166 p=3094422 u=root n=ansible | fatal: [k8s-w01]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.270969'
  end: '2025-10-25 01:02:24.637613'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:02:24.366644'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:02:23,167 p=3094422 u=root n=ansible | fatal: [k8s-m1]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.276037'
  end: '2025-10-25 01:02:24.635790'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:02:24.359753'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:02:23,182 p=3094422 u=root n=ansible | fatal: [k8s-w02]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.232828'
  end: '2025-10-25 01:02:24.063976'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:02:23.831148'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:02:23,704 p=3094422 u=root n=ansible | fatal: [k8s-w03]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.247390'
  end: '2025-10-25 01:02:25.173197'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:02:24.925807'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:02:23,706 p=3094422 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 08:02:23,706 p=3094422 u=root n=ansible | k8s-m1                     : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:02:23,706 p=3094422 u=root n=ansible | k8s-m2                     : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:02:23,706 p=3094422 u=root n=ansible | k8s-m3                     : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:02:23,706 p=3094422 u=root n=ansible | k8s-w01                    : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:02:23,706 p=3094422 u=root n=ansible | k8s-w02                    : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:02:23,706 p=3094422 u=root n=ansible | k8s-w03                    : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:02:45,698 p=3096174 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 08:02:47,353 p=3096174 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 08:02:47,354 p=3096174 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:02:47,381 p=3096174 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:02:47,391 p=3096174 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:02:47,394 p=3096174 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:02:47,410 p=3096174 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:02:48,616 p=3096174 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:02:49,970 p=3096174 u=root n=ansible | TASK [Gather facts] ************************************************************
2025-10-25 08:02:49,970 p=3096174 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:02:50,010 p=3096174 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:02:50,017 p=3096174 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:02:50,023 p=3096174 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:02:50,026 p=3096174 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:02:51,341 p=3096174 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:02:52,805 p=3096174 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] *********************
2025-10-25 08:02:52,805 p=3096174 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:02:52,828 p=3096174 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:02:52,861 p=3096174 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:02:52,881 p=3096174 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:02:52,910 p=3096174 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:02:53,984 p=3096174 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:02:54,474 p=3096174 u=root n=ansible | TASK [Detect default interface via ip route] ***********************************
2025-10-25 08:02:54,475 p=3096174 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:02:54,478 p=3096174 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:02:54,478 p=3096174 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:02:54,480 p=3096174 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:02:54,485 p=3096174 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:02:54,822 p=3096174 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:02:54,929 p=3096174 u=root n=ansible | TASK [Set default values for netplan variables] ********************************
2025-10-25 08:02:54,931 p=3096174 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:02:54,931 p=3096174 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:02:54,931 p=3096174 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:02:54,931 p=3096174 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:02:54,954 p=3096174 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:02:54,961 p=3096174 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:02:55,045 p=3096174 u=root n=ansible | TASK [Validate required variables] *********************************************
2025-10-25 08:02:55,046 p=3096174 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: All assertions passed
2025-10-25 08:02:55,046 p=3096174 u=root n=ansible | ok: [k8s-m2] => changed=false 
  msg: All assertions passed
2025-10-25 08:02:55,046 p=3096174 u=root n=ansible | ok: [k8s-m3] => changed=false 
  msg: All assertions passed
2025-10-25 08:02:55,048 p=3096174 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: All assertions passed
2025-10-25 08:02:55,048 p=3096174 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: All assertions passed
2025-10-25 08:02:55,069 p=3096174 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: All assertions passed
2025-10-25 08:02:55,468 p=3096174 u=root n=ansible | TASK [Move existing netplan YAMLs to timestamped backup directory] *************
2025-10-25 08:02:55,469 p=3096174 u=root n=ansible | changed: [k8s-w01]
2025-10-25 08:02:55,469 p=3096174 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:02:55,470 p=3096174 u=root n=ansible | changed: [k8s-m3]
2025-10-25 08:02:55,479 p=3096174 u=root n=ansible | changed: [k8s-m2]
2025-10-25 08:02:55,494 p=3096174 u=root n=ansible | changed: [k8s-w02]
2025-10-25 08:02:55,868 p=3096174 u=root n=ansible | changed: [k8s-w03]
2025-10-25 08:02:56,974 p=3096174 u=root n=ansible | TASK [Render static netplan config] ********************************************
2025-10-25 08:02:56,975 p=3096174 u=root n=ansible | changed: [k8s-m2]
2025-10-25 08:02:56,977 p=3096174 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:02:56,989 p=3096174 u=root n=ansible | changed: [k8s-w02]
2025-10-25 08:02:56,989 p=3096174 u=root n=ansible | changed: [k8s-m3]
2025-10-25 08:02:57,051 p=3096174 u=root n=ansible | changed: [k8s-w01]
2025-10-25 08:02:57,759 p=3096174 u=root n=ansible | changed: [k8s-w03]
2025-10-25 08:02:58,339 p=3096174 u=root n=ansible | TASK [Apply netplan] ***********************************************************
2025-10-25 08:02:58,341 p=3096174 u=root n=ansible | fatal: [k8s-m1]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.234962'
  end: '2025-10-25 01:02:59.806697'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:02:59.571735'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:02:58,368 p=3096174 u=root n=ansible | fatal: [k8s-m2]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.235604'
  end: '2025-10-25 01:02:59.842130'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:02:59.606526'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:02:58,370 p=3096174 u=root n=ansible | fatal: [k8s-w01]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.226697'
  end: '2025-10-25 01:02:59.839458'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:02:59.612761'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:02:58,403 p=3096174 u=root n=ansible | fatal: [k8s-m3]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.256632'
  end: '2025-10-25 01:02:59.876675'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:02:59.620043'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:02:58,406 p=3096174 u=root n=ansible | fatal: [k8s-w02]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.238734'
  end: '2025-10-25 01:02:59.290311'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:02:59.051577'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:02:58,912 p=3096174 u=root n=ansible | fatal: [k8s-w03]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.233121'
  end: '2025-10-25 01:03:00.368882'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 01:03:00.135761'
  stderr: |-
    /etc/netplan/01-static.yaml:24:11: Invalid YAML: did not find expected node content:
              - "
              ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 08:02:58,913 p=3096174 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 08:02:58,913 p=3096174 u=root n=ansible | k8s-m1                     : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:02:58,914 p=3096174 u=root n=ansible | k8s-m2                     : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:02:58,914 p=3096174 u=root n=ansible | k8s-m3                     : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:02:58,914 p=3096174 u=root n=ansible | k8s-w01                    : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:02:58,914 p=3096174 u=root n=ansible | k8s-w02                    : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:02:58,914 p=3096174 u=root n=ansible | k8s-w03                    : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 08:03:36,801 p=3098417 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 08:03:38,437 p=3098417 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 08:03:38,438 p=3098417 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:03:38,479 p=3098417 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:03:38,488 p=3098417 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:03:38,519 p=3098417 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:03:39,502 p=3098417 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:03:39,795 p=3098417 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:03:41,244 p=3098417 u=root n=ansible | TASK [Gather facts] ************************************************************
2025-10-25 08:03:41,244 p=3098417 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:03:41,248 p=3098417 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:03:41,251 p=3098417 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:03:41,256 p=3098417 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:03:41,259 p=3098417 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:03:43,561 p=3098417 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:03:45,040 p=3098417 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] *********************
2025-10-25 08:03:45,041 p=3098417 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:03:45,061 p=3098417 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:03:45,097 p=3098417 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:03:45,108 p=3098417 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:03:45,149 p=3098417 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:03:46,272 p=3098417 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:03:46,774 p=3098417 u=root n=ansible | TASK [Detect default interface via ip route] ***********************************
2025-10-25 08:03:46,775 p=3098417 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:03:46,777 p=3098417 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:03:46,784 p=3098417 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:03:46,810 p=3098417 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:03:46,823 p=3098417 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:03:47,114 p=3098417 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:03:47,202 p=3098417 u=root n=ansible | TASK [Set default values for netplan variables] ********************************
2025-10-25 08:03:47,203 p=3098417 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:03:47,203 p=3098417 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:03:47,203 p=3098417 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:03:47,230 p=3098417 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:03:47,233 p=3098417 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:03:47,239 p=3098417 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:03:47,287 p=3098417 u=root n=ansible | TASK [Validate required variables] *********************************************
2025-10-25 08:03:47,288 p=3098417 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: All assertions passed
2025-10-25 08:03:47,299 p=3098417 u=root n=ansible | ok: [k8s-m2] => changed=false 
  msg: All assertions passed
2025-10-25 08:03:47,314 p=3098417 u=root n=ansible | ok: [k8s-m3] => changed=false 
  msg: All assertions passed
2025-10-25 08:03:47,315 p=3098417 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: All assertions passed
2025-10-25 08:03:47,330 p=3098417 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: All assertions passed
2025-10-25 08:03:47,357 p=3098417 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: All assertions passed
2025-10-25 08:03:47,728 p=3098417 u=root n=ansible | TASK [Move existing netplan YAMLs to timestamped backup directory] *************
2025-10-25 08:03:47,728 p=3098417 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:03:47,740 p=3098417 u=root n=ansible | changed: [k8s-m3]
2025-10-25 08:03:47,759 p=3098417 u=root n=ansible | changed: [k8s-w01]
2025-10-25 08:03:47,760 p=3098417 u=root n=ansible | changed: [k8s-m2]
2025-10-25 08:03:47,778 p=3098417 u=root n=ansible | changed: [k8s-w02]
2025-10-25 08:03:48,064 p=3098417 u=root n=ansible | changed: [k8s-w03]
2025-10-25 08:03:49,125 p=3098417 u=root n=ansible | TASK [Render static netplan config] ********************************************
2025-10-25 08:03:49,126 p=3098417 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:03:49,130 p=3098417 u=root n=ansible | changed: [k8s-m3]
2025-10-25 08:03:49,135 p=3098417 u=root n=ansible | changed: [k8s-w01]
2025-10-25 08:03:49,137 p=3098417 u=root n=ansible | changed: [k8s-w02]
2025-10-25 08:03:49,143 p=3098417 u=root n=ansible | changed: [k8s-m2]
2025-10-25 08:03:49,899 p=3098417 u=root n=ansible | changed: [k8s-w03]
2025-10-25 08:03:51,224 p=3098417 u=root n=ansible | TASK [Apply netplan] ***********************************************************
2025-10-25 08:03:51,225 p=3098417 u=root n=ansible | changed: [k8s-w02]
2025-10-25 08:03:51,227 p=3098417 u=root n=ansible | changed: [k8s-w01]
2025-10-25 08:03:51,235 p=3098417 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:03:51,247 p=3098417 u=root n=ansible | changed: [k8s-m3]
2025-10-25 08:03:51,252 p=3098417 u=root n=ansible | changed: [k8s-m2]
2025-10-25 08:03:52,495 p=3098417 u=root n=ansible | changed: [k8s-w03]
2025-10-25 08:03:53,951 p=3098417 u=root n=ansible | TASK [Verify network configuration] ********************************************
2025-10-25 08:03:53,951 p=3098417 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:03:53,952 p=3098417 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:03:53,973 p=3098417 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:03:53,974 p=3098417 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:03:53,986 p=3098417 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:03:55,411 p=3098417 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:03:55,462 p=3098417 u=root n=ansible | TASK [Show verification output] ************************************************
2025-10-25 08:03:55,464 p=3098417 u=root n=ansible | ok: [k8s-m1] => 
  net_verify.stdout: |-
    3: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.12/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    default via 10.0.6.1 dev ens34 proto static
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.12
  
    PING 10.0.6.1 (10.0.6.1) 56(84) bytes of data.
    64 bytes from 10.0.6.1: icmp_seq=1 ttl=128 time=0.680 ms
    64 bytes from 10.0.6.1: icmp_seq=2 ttl=128 time=0.863 ms
  
    --- 10.0.6.1 ping statistics ---
    2 packets transmitted, 2 received, 0% packet loss, time 1042ms
    rtt min/avg/max/mdev = 0.680/0.771/0.863/0.091 ms
2025-10-25 08:03:55,493 p=3098417 u=root n=ansible | ok: [k8s-m2] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.11/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    default via 10.0.6.1 dev ens34 proto static
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.11
  
    PING 10.0.6.1 (10.0.6.1) 56(84) bytes of data.
    64 bytes from 10.0.6.1: icmp_seq=1 ttl=128 time=0.588 ms
    64 bytes from 10.0.6.1: icmp_seq=2 ttl=128 time=0.514 ms
  
    --- 10.0.6.1 ping statistics ---
    2 packets transmitted, 2 received, 0% packet loss, time 1006ms
    rtt min/avg/max/mdev = 0.514/0.551/0.588/0.037 ms
2025-10-25 08:03:55,508 p=3098417 u=root n=ansible | ok: [k8s-m3] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.13/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    default via 10.0.6.1 dev ens34 proto static
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.13
  
    PING 10.0.6.1 (10.0.6.1) 56(84) bytes of data.
    64 bytes from 10.0.6.1: icmp_seq=1 ttl=128 time=0.825 ms
    64 bytes from 10.0.6.1: icmp_seq=2 ttl=128 time=0.777 ms
  
    --- 10.0.6.1 ping statistics ---
    2 packets transmitted, 2 received, 0% packet loss, time 1012ms
    rtt min/avg/max/mdev = 0.777/0.801/0.825/0.024 ms
2025-10-25 08:03:55,509 p=3098417 u=root n=ansible | ok: [k8s-w01] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.14/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    default via 10.0.6.1 dev ens34 proto static
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.14
  
    PING 10.0.6.1 (10.0.6.1) 56(84) bytes of data.
    64 bytes from 10.0.6.1: icmp_seq=1 ttl=128 time=0.592 ms
    64 bytes from 10.0.6.1: icmp_seq=2 ttl=128 time=0.736 ms
  
    --- 10.0.6.1 ping statistics ---
    2 packets transmitted, 2 received, 0% packet loss, time 1003ms
    rtt min/avg/max/mdev = 0.592/0.664/0.736/0.072 ms
2025-10-25 08:03:55,512 p=3098417 u=root n=ansible | ok: [k8s-w02] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.15/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    default via 10.0.6.1 dev ens34 proto static
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.15
  
    PING 10.0.6.1 (10.0.6.1) 56(84) bytes of data.
    64 bytes from 10.0.6.1: icmp_seq=1 ttl=128 time=0.496 ms
    64 bytes from 10.0.6.1: icmp_seq=2 ttl=128 time=0.731 ms
  
    --- 10.0.6.1 ping statistics ---
    2 packets transmitted, 2 received, 0% packet loss, time 1008ms
    rtt min/avg/max/mdev = 0.496/0.613/0.731/0.117 ms
2025-10-25 08:03:55,526 p=3098417 u=root n=ansible | ok: [k8s-w03] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.16/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    default via 10.0.6.1 dev ens34 proto static
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.16
  
    PING 10.0.6.1 (10.0.6.1) 56(84) bytes of data.
    64 bytes from 10.0.6.1: icmp_seq=1 ttl=128 time=0.596 ms
    64 bytes from 10.0.6.1: icmp_seq=2 ttl=128 time=0.719 ms
  
    --- 10.0.6.1 ping statistics ---
    2 packets transmitted, 2 received, 0% packet loss, time 1012ms
    rtt min/avg/max/mdev = 0.596/0.657/0.719/0.061 ms
2025-10-25 08:03:55,677 p=3098417 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 08:03:55,678 p=3098417 u=root n=ansible | k8s-m1                     : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 08:03:55,678 p=3098417 u=root n=ansible | k8s-m2                     : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 08:03:55,678 p=3098417 u=root n=ansible | k8s-m3                     : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 08:03:55,679 p=3098417 u=root n=ansible | k8s-w01                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 08:03:55,679 p=3098417 u=root n=ansible | k8s-w02                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 08:03:55,679 p=3098417 u=root n=ansible | k8s-w03                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 08:04:19,193 p=3100301 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ****
2025-10-25 08:04:20,817 p=3100301 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 08:04:20,818 p=3100301 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:04:20,828 p=3100301 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:04:20,832 p=3100301 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:04:20,836 p=3100301 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:04:20,854 p=3100301 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:04:23,208 p=3100301 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:04:23,383 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 08:04:23,384 p=3100301 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:04:23,406 p=3100301 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:04:23,407 p=3100301 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:04:23,430 p=3100301 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:04:23,480 p=3100301 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:04:23,481 p=3100301 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:04:24,653 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] **********************
2025-10-25 08:04:24,653 p=3100301 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:04:24,656 p=3100301 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:04:24,662 p=3100301 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:04:24,678 p=3100301 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:04:24,682 p=3100301 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:04:25,408 p=3100301 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:04:25,550 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] ******************
2025-10-25 08:04:25,658 p=3100301 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 08:04:26,444 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Create RKE2 artifacts folder] ******************************
2025-10-25 08:04:26,444 p=3100301 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:04:26,447 p=3100301 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:04:26,452 p=3100301 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:04:26,453 p=3100301 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:04:26,471 p=3100301 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:04:26,790 p=3100301 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:04:27,542 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Download sha256 checksum file ( airgap mode )] *************
2025-10-25 08:04:27,542 p=3100301 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:04:27,544 p=3100301 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:04:27,545 p=3100301 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:04:27,578 p=3100301 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:04:27,584 p=3100301 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:04:28,087 p=3100301 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:04:28,798 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 artifacts and compare with checksums ( airgap mode )] ***
2025-10-25 08:04:28,798 p=3100301 u=root n=ansible | ok: [k8s-m2] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 08:04:28,858 p=3100301 u=root n=ansible | ok: [k8s-w01] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 08:04:28,866 p=3100301 u=root n=ansible | ok: [k8s-m1] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 08:04:28,879 p=3100301 u=root n=ansible | ok: [k8s-m3] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 08:04:28,889 p=3100301 u=root n=ansible | ok: [k8s-w02] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 08:04:32,588 p=3100301 u=root n=ansible | ok: [k8s-w01] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 08:04:32,603 p=3100301 u=root n=ansible | ok: [k8s-m2] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 08:04:32,632 p=3100301 u=root n=ansible | ok: [k8s-m3] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 08:04:32,636 p=3100301 u=root n=ansible | ok: [k8s-w02] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 08:04:32,810 p=3100301 u=root n=ansible | ok: [k8s-m1] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 08:04:46,673 p=3100301 u=root n=ansible | changed: [k8s-m3] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 08:04:46,816 p=3100301 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 08:04:47,592 p=3100301 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 08:04:47,858 p=3100301 u=root n=ansible | changed: [k8s-m2] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 08:04:47,901 p=3100301 u=root n=ansible | changed: [k8s-w03] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 08:04:47,993 p=3100301 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 08:09:02,126 p=3100301 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 08:09:13,700 p=3100301 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 08:09:14,284 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 install script ( airgap mode )] **************
2025-10-25 08:09:14,287 p=3100301 u=root n=ansible | changed: [k8s-m2]
2025-10-25 08:09:14,287 p=3100301 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:09:14,313 p=3100301 u=root n=ansible | changed: [k8s-m3]
2025-10-25 08:09:14,321 p=3100301 u=root n=ansible | changed: [k8s-w02]
2025-10-25 08:09:14,321 p=3100301 u=root n=ansible | changed: [k8s-w01]
2025-10-25 08:09:14,831 p=3100301 u=root n=ansible | changed: [k8s-w03]
2025-10-25 08:09:21,073 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Populate service facts] ************************************
2025-10-25 08:09:21,073 p=3100301 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:09:21,082 p=3100301 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:09:21,091 p=3100301 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:09:21,098 p=3100301 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:09:21,106 p=3100301 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:09:29,217 p=3100301 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:09:29,775 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Get stats of the FS object] ********************************
2025-10-25 08:09:29,778 p=3100301 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:09:29,779 p=3100301 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:09:29,779 p=3100301 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:09:29,780 p=3100301 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:09:29,910 p=3100301 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:09:30,178 p=3100301 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:09:30,767 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Check if separate partition] *******************************
2025-10-25 08:09:30,768 p=3100301 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:09:30,769 p=3100301 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:09:30,776 p=3100301 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:09:30,778 p=3100301 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:09:30,785 p=3100301 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:09:31,113 p=3100301 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:09:31,172 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 bin path] *****************************************
2025-10-25 08:09:31,175 p=3100301 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:09:31,195 p=3100301 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:09:31,216 p=3100301 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:09:31,237 p=3100301 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:09:31,237 p=3100301 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:09:31,255 p=3100301 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:09:31,648 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Check RKE2 version] ****************************************
2025-10-25 08:09:31,649 p=3100301 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:09:31,654 p=3100301 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:09:31,664 p=3100301 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:09:31,688 p=3100301 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:09:31,715 p=3100301 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:09:31,982 p=3100301 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:09:32,067 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 versions] *****************************************
2025-10-25 08:09:32,068 p=3100301 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:09:32,087 p=3100301 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:09:32,109 p=3100301 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:09:32,110 p=3100301 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:09:32,128 p=3100301 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:09:32,144 p=3100301 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:10:16,156 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Run RKE2 install script with airgap variables] *************
2025-10-25 08:10:16,156 p=3100301 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:10:21,881 p=3100301 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:10:23,332 p=3100301 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:10:36,831 p=3100301 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:10:37,853 p=3100301 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:11:22,935 p=3100301 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:11:23,766 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Find Active Server] ****************************************
2025-10-25 08:11:23,927 p=3100301 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/find_active_server.yml for k8s-m1, k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 08:11:29,069 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Populate services facts] ***********************************
2025-10-25 08:11:29,070 p=3100301 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:11:29,108 p=3100301 u=root n=ansible | ok: [k8s-m2]
2025-10-25 08:11:29,137 p=3100301 u=root n=ansible | ok: [k8s-w02]
2025-10-25 08:11:29,156 p=3100301 u=root n=ansible | ok: [k8s-w01]
2025-10-25 08:11:29,174 p=3100301 u=root n=ansible | ok: [k8s-m3]
2025-10-25 08:11:34,487 p=3100301 u=root n=ansible | ok: [k8s-w03]
2025-10-25 08:11:35,176 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Copy kube-vip manifests to the masternode] *****************
2025-10-25 08:11:35,227 p=3100301 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/kubevip.yml for k8s-m1
2025-10-25 08:11:35,595 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Create the RKE2 manifests directory] ***********************
2025-10-25 08:11:35,596 p=3100301 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:11:36,523 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Copy kube-vip files to first server] ***********************
2025-10-25 08:11:36,523 p=3100301 u=root n=ansible | changed: [k8s-m1] => (item=/opt/git/ansible/roles/rke2-1.49.0/templates/kube-vip/kube-vip.yml.j2)
2025-10-25 08:11:37,294 p=3100301 u=root n=ansible | changed: [k8s-m1] => (item=/opt/git/ansible/roles/rke2-1.49.0/templates/kube-vip/kube-vip-rbac.yml.j2)
2025-10-25 08:11:37,587 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Prepare very first server node in the cluster] *************
2025-10-25 08:11:37,673 p=3100301 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/first_server.yml for k8s-m1
2025-10-25 08:11:38,077 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Create the RKE2 config dir] ********************************
2025-10-25 08:11:38,077 p=3100301 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:11:38,118 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Set server taints] *****************************************
2025-10-25 08:11:38,119 p=3100301 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:11:39,082 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Copy rke2 config] ******************************************
2025-10-25 08:11:39,083 p=3100301 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:11:40,009 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Copy Containerd Registry Configuration file] ***************
2025-10-25 08:11:40,009 p=3100301 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:13:06,232 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Start RKE2 service on the first server] ********************
2025-10-25 08:13:06,233 p=3100301 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:13:07,639 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Mask RKE2 agent service on the first server] ***************
2025-10-25 08:13:07,639 p=3100301 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:20:28,958 p=3133885 u=root n=ansible | k8s-m1 | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
2025-10-25 08:20:56,381 p=3134596 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
 rke2-server.service - Rancher Kubernetes Engine v2 (server)
     Loaded: loaded (/usr/local/lib/systemd/system/rke2-server.service; enabled; preset: enabled)
     Active: active (running) since Sat 2025-10-25 01:13:07 UTC; 7min ago
       Docs: https://github.com/rancher/rke2#readme
   Main PID: 6685 (rke2)
      Tasks: 123
     Memory: 2.0G (peak: 2.2G swap: 192.0K swap peak: 192.0K)
        CPU: 3min 20.926s
     CGroup: /system.slice/rke2-server.service
             6685 "/usr/local/bin/rke2 server"
             6705 containerd -c /var/lib/rancher/rke2/agent/etc/containerd/config.toml
             6733 kubelet --volume-plugin-dir=/var/lib/kubelet/volumeplugins --file-check-frequency=5s --sync-frequency=30s --config-dir=/var/lib/rancher/rke2/agent/etc/kubelet.conf.d --containerd=/run/k3s/containerd/containerd.sock --hostname-override=k8s-m1 --kubeconfig=/var/lib/rancher/rke2/agent/kubelet.kubeconfig --max-pods=100 --node-ip=10.0.6.12 --node-labels= --node-status-update-frequency=4s --read-only-port=0
             6779 /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/bin/containerd-shim-runc-v2 -namespace k8s.io -id e7eed802c33224076f380b070639027045294916ca5139640952731458e1547f -address /run/k3s/containerd/containerd.sock
             6865 /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/bin/containerd-shim-runc-v2 -namespace k8s.io -id 7db3f9770d96c63e39462cbfa7bb2e4e1226c8cd6c84d9d33f18e78fe3387396 -address /run/k3s/containerd/containerd.sock
             7010 /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/bin/containerd-shim-runc-v2 -namespace k8s.io -id a7991366042a7a6bfe8ecf059f9e15fa4821a9f31936bdb1a17c6879a76b054e -address /run/k3s/containerd/containerd.sock
             7014 /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/bin/containerd-shim-runc-v2 -namespace k8s.io -id 5468a63a8a1b7fd47eba723869a6f2546b06daa04e1f7a3b978626f3380ef313 -address /run/k3s/containerd/containerd.sock
             7281 /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/bin/containerd-shim-runc-v2 -namespace k8s.io -id 38c7913b9089c9a4c536fe56f1511999ceca2a1e9b13d175c26263fd823d8014 -address /run/k3s/containerd/containerd.sock
             7686 /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/bin/containerd-shim-runc-v2 -namespace k8s.io -id 8fe3090425e154d942f5619556f255698d2a613042047b447c9e81899d5774ce -address /run/k3s/containerd/containerd.sock

Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/hardened-calico:v3.30.3-build20250909"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/hardened-coredns:v1.12.3-build20250909"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/hardened-etcd:v3.6.4-k3s3-build20250908"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/hardened-k8s-metrics-server:v0.8.0-build20250909"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/klipper-helm:v0.9.8-build20250709"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/mirrored-sig-storage-snapshot-controller:v8.2.0"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/mirrored-pause:3.6"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.6.2"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/rke2-cloud-provider:v1.33.0-rc1.0.20250905195603-857412ae5891-build20250908"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported images from /var/lib/rancher/rke2/agent/images/rke2-images.linux-amd64.tar in 48.805855655s"

2025-10-25 08:21:23,811 p=3135027 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
Oct 25 01:11:43 k8s-m1 systemd[1]: Starting rke2-server.service - Rancher Kubernetes Engine v2 (server)...
Oct 25 01:11:43 k8s-m1 sh[6676]: + /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=warning msg="not running in CIS mode"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="Applying Pod Security Admission Configuration"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="Starting rke2 v1.34.1+rke2r1 (98b87c78e2c5a09fd8ff07bcaf4f102db1894a93)"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="Managed etcd cluster initializing"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="generated self-signed CA certificate CN=rke2-client-ca@1761354704: notBefore=2025-10-25 01:11:44.047351889 +0000 UTC notAfter=2035-10-23 01:11:44.047351889 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="certificate CN=system:admin,O=system:masters signed by CN=rke2-client-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:44 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="certificate CN=system:rke2-supervisor,O=system:masters signed by CN=rke2-client-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:44 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="certificate CN=system:kube-controller-manager signed by CN=rke2-client-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:44 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="certificate CN=system:kube-scheduler signed by CN=rke2-client-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:44 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="certificate CN=system:apiserver,O=system:masters signed by CN=rke2-client-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:44 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="certificate CN=rke2-cloud-controller-manager signed by CN=rke2-client-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:44 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="generated self-signed CA certificate CN=rke2-server-ca@1761354704: notBefore=2025-10-25 01:11:44.057138368 +0000 UTC notAfter=2035-10-23 01:11:44.057138368 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="certificate CN=kube-apiserver signed by CN=rke2-server-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:44 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="certificate CN=kube-scheduler signed by CN=rke2-server-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:44 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="certificate CN=kube-controller-manager signed by CN=rke2-server-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:44 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="generated self-signed CA certificate CN=rke2-request-header-ca@1761354704: notBefore=2025-10-25 01:11:44.062212498 +0000 UTC notAfter=2035-10-23 01:11:44.062212498 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="certificate CN=system:auth-proxy signed by CN=rke2-request-header-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:44 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="generated self-signed CA certificate CN=etcd-server-ca@1761354704: notBefore=2025-10-25 01:11:44.064213942 +0000 UTC notAfter=2035-10-23 01:11:44.064213942 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="certificate CN=etcd-client signed by CN=etcd-server-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:44 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="generated self-signed CA certificate CN=etcd-peer-ca@1761354704: notBefore=2025-10-25 01:11:44.066092629 +0000 UTC notAfter=2035-10-23 01:11:44.066092629 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="certificate CN=etcd-peer signed by CN=etcd-peer-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:44 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="certificate CN=etcd-server signed by CN=etcd-server-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:44 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="certificate CN=rke2,O=rke2 signed by CN=rke2-server-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:44 +0000 UTC"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=warning msg="dynamiclistener [::]:9345: no cached certificate available for preload - deferring certificate load until storage initialization or first client request"
Oct 25 01:11:44 k8s-m1 rke2[6685]: time="2025-10-25T01:11:44Z" level=info msg="Active TLS secret / (ver=) (count 12): map[listener.cattle.io/cn-10.0.6.100:10.0.6.100 listener.cattle.io/cn-10.0.6.12:10.0.6.12 listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-cluster.local:cluster.local listener.cattle.io/cn-k8s-m1:k8s-m1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/fingerprint:SHA1=0EE3D8B365EEE088F9DBB7AFE83734D79EEC7E72]"
Oct 25 01:11:45 k8s-m1 rke2[6685]: time="2025-10-25T01:11:45Z" level=info msg="Password verified locally for node k8s-m1"
Oct 25 01:11:45 k8s-m1 rke2[6685]: time="2025-10-25T01:11:45Z" level=info msg="certificate CN=k8s-m1 signed by CN=rke2-server-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:45 +0000 UTC"
Oct 25 01:11:45 k8s-m1 rke2[6685]: time="2025-10-25T01:11:45Z" level=info msg="certificate CN=system:node:k8s-m1,O=system:nodes signed by CN=rke2-client-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:45 +0000 UTC"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="certificate CN=system:kube-proxy signed by CN=rke2-client-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:46 +0000 UTC"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="certificate CN=system:rke2-controller signed by CN=rke2-client-ca@1761354704: notBefore=2025-10-25 01:11:44 +0000 UTC notAfter=2026-10-25 01:11:46 +0000 UTC"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=warning msg="Host resolv.conf includes loopback, multicast, or link-local nameservers - kubelet will use autogenerated resolv.conf with nameservers 8.8.8.8 2001:4860:4860::8888"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="Using private registry config file at /etc/rancher/rke2/registries.yaml"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="Module overlay was already loaded"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="Module br_netfilter was already loaded"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=warning msg="Failed to load kernel module nft-expr-counter with modprobe"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_max' to 131072"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="Set sysctl 'net/ipv4/conf/all/forwarding' to 1"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="Checking local image archives in /var/lib/rancher/rke2/agent/images for index.docker.io/rancher/rke2-runtime:v1.34.1-rke2r1"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="Waiting for cri connection: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/k3s/containerd/containerd.sock: connect: no such file or directory\""
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="Failed to find index.docker.io/rancher/rke2-runtime:v1.34.1-rke2r1 in /var/lib/rancher/rke2/agent/images/rke2-images-cilium.linux-amd64.tar.gz: tag rancher/rke2-runtime:v1.34.1-rke2r1 not found in tarball"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="Creating directory /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/bin"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="Extracting file bin/containerd to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/bin/containerd"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="Extracting file bin/containerd-shim-runc-v2 to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/bin/containerd-shim-runc-v2"
Oct 25 01:11:46 k8s-m1 rke2[6685]: time="2025-10-25T01:11:46Z" level=info msg="Extracting file bin/crictl to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/bin/crictl"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file bin/ctr to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/bin/ctr"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file bin/kubectl to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/bin/kubectl"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file bin/kubelet to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/bin/kubelet"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file bin/runc to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/bin/runc"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Creating directory /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/harvester-cloud-provider.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/harvester-cloud-provider.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/harvester-csi-driver.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/harvester-csi-driver.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rancher-vsphere-cpi.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rancher-vsphere-cpi.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rancher-vsphere-csi.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rancher-vsphere-csi.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rke2-calico-crd.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rke2-calico-crd.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rke2-calico.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rke2-calico.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rke2-canal.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rke2-canal.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rke2-cilium.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rke2-cilium.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rke2-coredns.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rke2-coredns.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rke2-flannel.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rke2-flannel.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rke2-ingress-nginx.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rke2-ingress-nginx.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rke2-metrics-server.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rke2-metrics-server.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rke2-multus.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rke2-multus.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rke2-runtimeclasses.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rke2-runtimeclasses.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rke2-snapshot-controller-crd.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rke2-snapshot-controller-crd.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rke2-snapshot-controller.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rke2-snapshot-controller.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rke2-snapshot-validation-webhook.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rke2-snapshot-validation-webhook.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rke2-traefik-crd.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rke2-traefik-crd.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Extracting file charts/rke2-traefik.yaml to /var/lib/rancher/rke2/data/v1.34.1-rke2r1-8522b4227a96/charts/rke2-traefik.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rke2-metrics-server.yaml to set cluster configuration values"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rke2-multus.yaml to set cluster configuration values"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rke2-snapshot-controller-crd.yaml to set cluster configuration values"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="No cluster configuration value changes necessary for manifest /var/lib/rancher/rke2/server/manifests/rke2-snapshot-validation-webhook.yaml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/harvester-csi-driver.yaml to set cluster configuration values"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="No cluster configuration value changes necessary for manifest /var/lib/rancher/rke2/server/manifests/kube-vip-rbac.yml"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rancher-vsphere-cpi.yaml to set cluster configuration values"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rke2-calico-crd.yaml to set cluster configuration values"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rke2-cilium.yaml to set cluster configuration values"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rke2-traefik-crd.yaml to set cluster configuration values"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/harvester-cloud-provider.yaml to set cluster configuration values"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rancher-vsphere-csi.yaml to set cluster configuration values"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rke2-calico.yaml to set cluster configuration values"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rke2-traefik.yaml to set cluster configuration values"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rke2-ingress-nginx.yaml to set cluster configuration values"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rke2-runtimeclasses.yaml to set cluster configuration values"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rke2-snapshot-controller.yaml to set cluster configuration values"
Oct 25 01:11:47 k8s-m1 rke2[6685]: time="2025-10-25T01:11:47Z" level=info msg="No cluster configuration value changes necessary for manifest /var/lib/rancher/rke2/server/manifests/kube-vip.yml"
Oct 25 01:11:48 k8s-m1 rke2[6685]: time="2025-10-25T01:11:48Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rke2-canal.yaml to set cluster configuration values"
Oct 25 01:11:48 k8s-m1 rke2[6685]: time="2025-10-25T01:11:48Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rke2-coredns.yaml to set cluster configuration values"
Oct 25 01:11:48 k8s-m1 rke2[6685]: time="2025-10-25T01:11:48Z" level=info msg="Updated manifest /var/lib/rancher/rke2/server/manifests/rke2-flannel.yaml to set cluster configuration values"
Oct 25 01:11:48 k8s-m1 rke2[6685]: time="2025-10-25T01:11:48Z" level=info msg="Logging containerd to /var/lib/rancher/rke2/agent/containerd/containerd.log"
Oct 25 01:11:48 k8s-m1 rke2[6685]: time="2025-10-25T01:11:48Z" level=info msg="Running containerd -c /var/lib/rancher/rke2/agent/etc/containerd/config.toml"
Oct 25 01:11:49 k8s-m1 rke2[6685]: time="2025-10-25T01:11:49Z" level=info msg="containerd is now running"
Oct 25 01:11:49 k8s-m1 rke2[6685]: time="2025-10-25T01:11:49Z" level=info msg="Importing images from /var/lib/rancher/rke2/agent/images/rke2-images-cilium.linux-amd64.tar.gz"
Oct 25 01:12:06 k8s-m1 rke2[6685]: time="2025-10-25T01:12:06Z" level=info msg="Polling for API server readiness: GET /readyz failed: Get \"https://127.0.0.1:6443/readyz?timeout=15s&verbose=\": dial tcp 127.0.0.1:6443: connect: connection refused"
Oct 25 01:12:32 k8s-m1 rke2[6685]: time="2025-10-25T01:12:32Z" level=info msg="Imported docker.io/rancher/mirrored-cilium-hubble-ui:v0.13.2"
Oct 25 01:12:32 k8s-m1 rke2[6685]: time="2025-10-25T01:12:32Z" level=info msg="Imported docker.io/rancher/mirrored-cilium-operator-aws:v1.18.1"
Oct 25 01:12:32 k8s-m1 rke2[6685]: time="2025-10-25T01:12:32Z" level=info msg="Imported docker.io/rancher/mirrored-cilium-operator-azure:v1.18.1"
Oct 25 01:12:32 k8s-m1 rke2[6685]: time="2025-10-25T01:12:32Z" level=info msg="Imported docker.io/rancher/mirrored-cilium-operator-generic:v1.18.1"
Oct 25 01:12:32 k8s-m1 rke2[6685]: time="2025-10-25T01:12:32Z" level=info msg="Imported docker.io/rancher/hardened-cni-plugins:v1.8.0-build20250909"
Oct 25 01:12:32 k8s-m1 rke2[6685]: time="2025-10-25T01:12:32Z" level=info msg="Imported docker.io/rancher/mirrored-cilium-cilium-envoy:v1.34.4-1754895458-68cffdfa568b6b226d70a7ef81fc65dda3b890bf"
Oct 25 01:12:32 k8s-m1 rke2[6685]: time="2025-10-25T01:12:32Z" level=info msg="Imported docker.io/rancher/mirrored-cilium-clustermesh-apiserver:v1.18.1"
Oct 25 01:12:32 k8s-m1 rke2[6685]: time="2025-10-25T01:12:32Z" level=info msg="Imported docker.io/rancher/mirrored-cilium-hubble-relay:v1.18.1"
Oct 25 01:12:32 k8s-m1 rke2[6685]: time="2025-10-25T01:12:32Z" level=info msg="Imported docker.io/rancher/mirrored-cilium-certgen:v0.2.4"
Oct 25 01:12:32 k8s-m1 rke2[6685]: time="2025-10-25T01:12:32Z" level=info msg="Imported docker.io/rancher/mirrored-cilium-cilium:v1.18.1"
Oct 25 01:12:32 k8s-m1 rke2[6685]: time="2025-10-25T01:12:32Z" level=info msg="Imported docker.io/rancher/mirrored-cilium-hubble-ui-backend:v0.13.2"
Oct 25 01:12:32 k8s-m1 rke2[6685]: time="2025-10-25T01:12:32Z" level=info msg="Imported images from /var/lib/rancher/rke2/agent/images/rke2-images-cilium.linux-amd64.tar.gz in 43.238304138s"
Oct 25 01:12:32 k8s-m1 rke2[6685]: time="2025-10-25T01:12:32Z" level=info msg="Importing images from /var/lib/rancher/rke2/agent/images/rke2-images.linux-amd64.tar"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Connecting to proxy" url="wss://127.0.0.1:9345/v1-rke2/connect"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Creating rke2-cert-monitor event broadcaster"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Running kubelet --alsologtostderr=false --config-dir=/var/lib/rancher/rke2/agent/etc/kubelet.conf.d --containerd=/run/k3s/containerd/containerd.sock --hostname-override=k8s-m1 --kubeconfig=/var/lib/rancher/rke2/agent/kubelet.kubeconfig --log-file=/var/lib/rancher/rke2/agent/logs/kubelet.log --log-file-max-size=50 --logtostderr=false --max-pods=100 --node-ip=10.0.6.12 --node-labels= --node-status-update-frequency=4s --read-only-port=0 --stderrthreshold=FATAL"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Handling backend connection request [k8s-m1]"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Connected to proxy" url="wss://127.0.0.1:9345/v1-rke2/connect"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Remotedialer connected to proxy" url="wss://127.0.0.1:9345/v1-rke2/connect"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Starting etcd for new cluster, cluster-reset=false"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg=start
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="schedule, now=2025-10-25T01:12:33Z, entry=1, next=2025-10-25T12:00:00Z"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Failed to test etcd connection: failed to get etcd status: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\""
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Running kube-apiserver --advertise-port=6443 --allow-privileged=true --anonymous-auth=false --api-audiences=https://kubernetes.default.svc.cluster.local,rke2 --authorization-mode=Node,RBAC --bind-address=0.0.0.0 --cert-dir=/var/lib/rancher/rke2/server/tls/temporary-certs --client-ca-file=/var/lib/rancher/rke2/server/tls/client-ca.crt --default-not-ready-toleration-seconds=60 --default-unreachable-toleration-seconds=40 --egress-selector-config-file=/var/lib/rancher/rke2/server/etc/egress-selector-config.yaml --enable-admission-plugins=NodeRestriction --enable-aggregator-routing=true --enable-bootstrap-token-auth=true --encryption-provider-config=/var/lib/rancher/rke2/server/cred/encryption-config.json --encryption-provider-config-automatic-reload=true --etcd-cafile=/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt --etcd-certfile=/var/lib/rancher/rke2/server/tls/etcd/client.crt --etcd-keyfile=/var/lib/rancher/rke2/server/tls/etcd/client.key --etcd-servers=https://127.0.0.1:2379 --kubelet-certificate-authority=/var/lib/rancher/rke2/server/tls/server-ca.crt --kubelet-client-certificate=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.crt --kubelet-client-key=/var/lib/rancher/rke2/server/tls/client-kube-apiserver.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --profiling=false --proxy-client-cert-file=/var/lib/rancher/rke2/server/tls/client-auth-proxy.crt --proxy-client-key-file=/var/lib/rancher/rke2/server/tls/client-auth-proxy.key --requestheader-allowed-names=system:auth-proxy --requestheader-client-ca-file=/var/lib/rancher/rke2/server/tls/request-header-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/rancher/rke2/server/tls/service.key --service-account-signing-key-file=/var/lib/rancher/rke2/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --tls-cert-file=/var/lib/rancher/rke2/server/tls/serving-kube-apiserver.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-private-key-file=/var/lib/rancher/rke2/server/tls/serving-kube-apiserver.key"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Removed kube-apiserver static pod manifest"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Running kube-scheduler --authentication-kubeconfig=/var/lib/rancher/rke2/server/cred/scheduler.kubeconfig --authorization-kubeconfig=/var/lib/rancher/rke2/server/cred/scheduler.kubeconfig --bind-address=0.0.0.0 --kubeconfig=/var/lib/rancher/rke2/server/cred/scheduler.kubeconfig --profiling=false --secure-port=10259 --tls-cert-file=/var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.crt --tls-private-key-file=/var/lib/rancher/rke2/server/tls/kube-scheduler/kube-scheduler.key"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Running kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/rke2/server/cred/controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/rke2/server/cred/controller.kubeconfig --bind-address=0.0.0.0 --cluster-cidr=10.42.0.0/16 --cluster-signing-kube-apiserver-client-cert-file=/var/lib/rancher/rke2/server/tls/client-ca.nochain.crt --cluster-signing-kube-apiserver-client-key-file=/var/lib/rancher/rke2/server/tls/client-ca.key --cluster-signing-kubelet-client-cert-file=/var/lib/rancher/rke2/server/tls/client-ca.nochain.crt --cluster-signing-kubelet-client-key-file=/var/lib/rancher/rke2/server/tls/client-ca.key --cluster-signing-kubelet-serving-cert-file=/var/lib/rancher/rke2/server/tls/server-ca.nochain.crt --cluster-signing-kubelet-serving-key-file=/var/lib/rancher/rke2/server/tls/server-ca.key --cluster-signing-legacy-unknown-cert-file=/var/lib/rancher/rke2/server/tls/server-ca.nochain.crt --cluster-signing-legacy-unknown-key-file=/var/lib/rancher/rke2/server/tls/server-ca.key --controllers=*,tokencleaner --kubeconfig=/var/lib/rancher/rke2/server/cred/controller.kubeconfig --node-monitor-period=4s --profiling=false --root-ca-file=/var/lib/rancher/rke2/server/tls/server-ca.crt --secure-port=10257 --service-account-private-key-file=/var/lib/rancher/rke2/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --terminated-pod-gc-threshold=50 --tls-cert-file=/var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.crt --tls-private-key-file=/var/lib/rancher/rke2/server/tls/kube-controller-manager/kube-controller-manager.key --use-service-account-credentials=true"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Server node token is available at /var/lib/rancher/rke2/server/token"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="To join server node to cluster: rke2 server -s https://10.0.6.12:9345 -t ${SERVER_NODE_TOKEN}"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Agent node token is available at /var/lib/rancher/rke2/server/agent-token"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="To join agent node to cluster: rke2 agent -s https://10.0.6.12:9345 -t ${AGENT_NODE_TOKEN}"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Wrote kubeconfig /etc/rancher/rke2/rke2.yaml"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=info msg="Run: rke2 kubectl"
Oct 25 01:12:33 k8s-m1 rke2[6685]: time="2025-10-25T01:12:33Z" level=error msg="Sending HTTP/1.1 503 response to 127.0.0.1:36212: runtime core not ready"
Oct 25 01:12:38 k8s-m1 rke2[6685]: time="2025-10-25T01:12:38Z" level=info msg="Failed to test etcd connection: failed to get etcd status: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\""
Oct 25 01:12:43 k8s-m1 rke2[6685]: time="2025-10-25T01:12:43Z" level=info msg="Connected to etcd v3.6.4 - datastore using 16384 of 20480 bytes"
Oct 25 01:12:43 k8s-m1 rke2[6685]: time="2025-10-25T01:12:43Z" level=info msg="Defragmenting etcd database"
Oct 25 01:12:43 k8s-m1 rke2[6685]: time="2025-10-25T01:12:43Z" level=info msg="Datastore using 12288 of 20480 bytes after defragment"
Oct 25 01:12:43 k8s-m1 rke2[6685]: time="2025-10-25T01:12:43Z" level=info msg="Connection to etcd is ready"
Oct 25 01:12:43 k8s-m1 rke2[6685]: time="2025-10-25T01:12:43Z" level=info msg="ETCD server is now running"
Oct 25 01:12:43 k8s-m1 rke2[6685]: time="2025-10-25T01:12:43Z" level=info msg="Saving cluster bootstrap data to datastore"
Oct 25 01:12:48 k8s-m1 rke2[6685]: time="2025-10-25T01:12:48Z" level=warning msg="Failed to list nodes with etcd role: runtime core not ready"
Oct 25 01:13:01 k8s-m1 rke2[6685]: time="2025-10-25T01:13:01Z" level=info msg="Polling for API server readiness: GET /readyz failed: unknown"
Oct 25 01:13:02 k8s-m1 rke2[6685]: time="2025-10-25T01:13:02Z" level=info msg="Polling for API server readiness: GET /readyz failed: an error on the server (\"[+]ping ok\\n[+]log ok\\n[+]etcd ok\\n[+]etcd-readiness ok\\n[+]kms-providers ok\\n[+]informer-sync ok\\n[+]poststarthook/start-encryption-provider-config-automatic-reload ok\\n[+]poststarthook/start-apiserver-admission-initializer ok\\n[+]poststarthook/generic-apiserver-start-informers ok\\n[+]poststarthook/priority-and-fairness-config-consumer ok\\n[+]poststarthook/priority-and-fairness-filter ok\\n[+]poststarthook/storage-object-count-tracker-hook ok\\n[+]poststarthook/start-apiextensions-informers ok\\n[+]poststarthook/start-apiextensions-controllers ok\\n[+]poststarthook/crd-informer-synced ok\\n[+]poststarthook/start-system-namespaces-controller ok\\n[+]poststarthook/start-cluster-authentication-info-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok\\n[+]poststarthook/start-legacy-token-tracking-controller ok\\n[+]poststarthook/start-service-ip-repair-controllers ok\\n[-]poststarthook/rbac/bootstrap-roles failed: reason withheld\\n[+]poststarthook/scheduling/bootstrap-system-priority-classes ok\\n[+]poststarthook/priority-and-fairness-config-producer ok\\n[+]poststarthook/bootstrap-controller ok\\n[+]poststarthook/start-kubernetes-service-cidr-controller ok\\n[+]poststarthook/aggregator-reload-proxy-client-cert ok\\n[+]poststarthook/start-kube-aggregator-informers ok\\n[+]poststarthook/apiservice-status-local-available-controller ok\\n[+]poststarthook/apiservice-status-remote-available-controller ok\\n[+]poststarthook/apiservice-registration-controller ok\\n[+]poststarthook/apiservice-discovery-controller ok\\n[+]poststarthook/kube-apiserver-autoregistration ok\\n[+]autoregister-completion ok\\n[+]poststarthook/apiservice-openapi-controller ok\\n[+]poststarthook/apiservice-openapiv3-controller ok\\n[+]shutdown ok\\nreadyz check failed\") has prevented the request from succeeding"
Oct 25 01:13:03 k8s-m1 rke2[6685]: time="2025-10-25T01:13:03Z" level=warning msg="Failed to list nodes with etcd role: runtime core not ready"
Oct 25 01:13:04 k8s-m1 rke2[6685]: time="2025-10-25T01:13:04Z" level=info msg="Watching for delete of k8s-m1 Node object"
Oct 25 01:13:04 k8s-m1 rke2[6685]: time="2025-10-25T01:13:04Z" level=info msg="Creating rke2-supervisor event broadcaster"
Oct 25 01:13:04 k8s-m1 rke2[6685]: time="2025-10-25T01:13:04Z" level=info msg="Setting runtimes"
Oct 25 01:13:04 k8s-m1 rke2[6685]: time="2025-10-25T01:13:04Z" level=info msg="Kube API server is now running"
Oct 25 01:13:04 k8s-m1 rke2[6685]: time="2025-10-25T01:13:04Z" level=info msg="Applying Cluster Role Bindings"
Oct 25 01:13:04 k8s-m1 rke2[6685]: time="2025-10-25T01:13:04Z" level=info msg="Creating embedded CRD addons.k3s.cattle.io"
Oct 25 01:13:04 k8s-m1 rke2[6685]: time="2025-10-25T01:13:04Z" level=info msg="Creating embedded CRD etcdsnapshotfiles.k3s.cattle.io"
Oct 25 01:13:04 k8s-m1 rke2[6685]: time="2025-10-25T01:13:04Z" level=info msg="Creating embedded CRD helmchartconfigs.helm.cattle.io"
Oct 25 01:13:04 k8s-m1 rke2[6685]: time="2025-10-25T01:13:04Z" level=info msg="Annotations and labels have been set successfully on node: k8s-m1"
Oct 25 01:13:04 k8s-m1 rke2[6685]: time="2025-10-25T01:13:04Z" level=info msg="Creating embedded CRD helmcharts.helm.cattle.io"
Oct 25 01:13:04 k8s-m1 rke2[6685]: time="2025-10-25T01:13:04Z" level=info msg="Waiting for CRD helmcharts.helm.cattle.io to become available"
Oct 25 01:13:05 k8s-m1 rke2[6685]: time="2025-10-25T01:13:05Z" level=info msg="Done waiting for CRD helmcharts.helm.cattle.io to become available"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Cluster Role Bindings applied successfully"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="rke2 is up and running"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Tunnel server egress proxy mode: agent"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Reconciling ETCDSnapshotFile resources"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Starting managed etcd node metadata controller"
Oct 25 01:13:07 k8s-m1 systemd[1]: Started rke2-server.service - Rancher Kubernetes Engine v2 (server).
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Starting dynamiclistener CN filter node controller with SANs: [cluster.local 10.0.6.100 127.0.0.1 ::1 localhost k8s-m1 10.0.6.12 10.43.0.1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local]"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Starting k3s.cattle.io/v1, Kind=Addon controller"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Creating deploy event broadcaster"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Starting /v1, Kind=Node controller"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Cluster dns configmap has been set successfully"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Starting managed etcd apiserver addresses controller"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Starting managed etcd member removal controller"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Starting managed etcd snapshot ConfigMap controller"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Creating helm-controller event broadcaster"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Starting k3s.cattle.io/v1, Kind=ETCDSnapshotFile controller"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Starting /v1, Kind=ConfigMap controller"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Starting /v1, Kind=ServiceAccount controller"
Oct 25 01:13:07 k8s-m1 rke2[6685]: time="2025-10-25T01:13:07Z" level=info msg="Starting /v1, Kind=Secret controller"
Oct 25 01:13:08 k8s-m1 rke2[6685]: time="2025-10-25T01:13:08Z" level=info msg="Starting rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding controller"
Oct 25 01:13:08 k8s-m1 rke2[6685]: time="2025-10-25T01:13:08Z" level=info msg="Starting batch/v1, Kind=Job controller"
Oct 25 01:13:08 k8s-m1 rke2[6685]: time="2025-10-25T01:13:08Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChart controller"
Oct 25 01:13:08 k8s-m1 rke2[6685]: time="2025-10-25T01:13:08Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChartConfig controller"
Oct 25 01:13:08 k8s-m1 rke2[6685]: time="2025-10-25T01:13:08Z" level=info msg="Active TLS secret kube-system/rke2-serving (ver=294) (count 12): map[listener.cattle.io/cn-10.0.6.100:10.0.6.100 listener.cattle.io/cn-10.0.6.12:10.0.6.12 listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-cluster.local:cluster.local listener.cattle.io/cn-k8s-m1:k8s-m1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/fingerprint:SHA1=0EE3D8B365EEE088F9DBB7AFE83734D79EEC7E72]"
Oct 25 01:13:08 k8s-m1 rke2[6685]: time="2025-10-25T01:13:08Z" level=info msg="Updating TLS secret for kube-system/rke2-serving (count: 12): map[listener.cattle.io/cn-10.0.6.100:10.0.6.100 listener.cattle.io/cn-10.0.6.12:10.0.6.12 listener.cattle.io/cn-10.43.0.1:10.43.0.1 listener.cattle.io/cn-127.0.0.1:127.0.0.1 listener.cattle.io/cn-__1-f16284:::1 listener.cattle.io/cn-cluster.local:cluster.local listener.cattle.io/cn-k8s-m1:k8s-m1 listener.cattle.io/cn-kubernetes:kubernetes listener.cattle.io/cn-kubernetes.default:kubernetes.default listener.cattle.io/cn-kubernetes.default.svc:kubernetes.default.svc listener.cattle.io/cn-kubernetes.default.svc.cluster.local:kubernetes.default.svc.cluster.local listener.cattle.io/cn-localhost:localhost listener.cattle.io/fingerprint:SHA1=0EE3D8B365EEE088F9DBB7AFE83734D79EEC7E72]"
Oct 25 01:13:08 k8s-m1 rke2[6685]: time="2025-10-25T01:13:08Z" level=info msg="Reconciliation of ETCDSnapshotFile resources complete"
Oct 25 01:13:08 k8s-m1 rke2[6685]: time="2025-10-25T01:13:08Z" level=info msg="Reconciling ETCDSnapshotFile resources"
Oct 25 01:13:08 k8s-m1 rke2[6685]: time="2025-10-25T01:13:08Z" level=info msg="Reconciliation of ETCDSnapshotFile resources complete"
Oct 25 01:13:08 k8s-m1 rke2[6685]: time="2025-10-25T01:13:08Z" level=info msg="Labels and annotations have been set successfully on node: k8s-m1"
Oct 25 01:13:09 k8s-m1 rke2[6685]: time="2025-10-25T01:13:09Z" level=info msg="Reconciling snapshot ConfigMap data"
Oct 25 01:13:10 k8s-m1 rke2[6685]: time="2025-10-25T01:13:10Z" level=info msg="Started tunnel to 10.0.6.12:9345"
Oct 25 01:13:10 k8s-m1 rke2[6685]: time="2025-10-25T01:13:10Z" level=info msg="Stopped tunnel to 127.0.0.1:9345"
Oct 25 01:13:10 k8s-m1 rke2[6685]: time="2025-10-25T01:13:10Z" level=info msg="Connecting to proxy" url="wss://10.0.6.12:9345/v1-rke2/connect"
Oct 25 01:13:10 k8s-m1 rke2[6685]: time="2025-10-25T01:13:10Z" level=info msg="Proxy done" err="context canceled" url="wss://127.0.0.1:9345/v1-rke2/connect"
Oct 25 01:13:10 k8s-m1 rke2[6685]: time="2025-10-25T01:13:10Z" level=info msg="error in remotedialer server [400]: websocket: close 1006 (abnormal closure): unexpected EOF"
Oct 25 01:13:10 k8s-m1 rke2[6685]: time="2025-10-25T01:13:10Z" level=info msg="Handling backend connection request [k8s-m1]"
Oct 25 01:13:10 k8s-m1 rke2[6685]: time="2025-10-25T01:13:10Z" level=info msg="Connected to proxy" url="wss://10.0.6.12:9345/v1-rke2/connect"
Oct 25 01:13:10 k8s-m1 rke2[6685]: time="2025-10-25T01:13:10Z" level=info msg="Remotedialer connected to proxy" url="wss://10.0.6.12:9345/v1-rke2/connect"
Oct 25 01:13:18 k8s-m1 rke2[6685]: time="2025-10-25T01:13:18Z" level=info msg="Adding node k8s-m1-cae019fe etcd status condition"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/rke2-runtime:v1.34.1-rke2r1"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/hardened-dns-node-cache:1.26.0-build20250909"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/klipper-lb:v0.4.13"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/hardened-flannel:v0.27.3-build20250901"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/hardened-kubernetes:v1.34.1-rke2r1-build20250910"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/hardened-cluster-autoscaler:v1.10.2-build20250909"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/hardened-addon-resizer:1.8.23-build20250909"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/nginx-ingress-controller:v1.12.6-hardened1"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/hardened-calico:v3.30.3-build20250909"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/hardened-coredns:v1.12.3-build20250909"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/hardened-etcd:v3.6.4-k3s3-build20250908"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/hardened-k8s-metrics-server:v0.8.0-build20250909"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/klipper-helm:v0.9.8-build20250709"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/mirrored-sig-storage-snapshot-controller:v8.2.0"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/mirrored-pause:3.6"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/mirrored-ingress-nginx-kube-webhook-certgen:v1.6.2"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported docker.io/rancher/rke2-cloud-provider:v1.33.0-rc1.0.20250905195603-857412ae5891-build20250908"
Oct 25 01:13:21 k8s-m1 rke2[6685]: time="2025-10-25T01:13:21Z" level=info msg="Imported images from /var/lib/rancher/rke2/agent/images/rke2-images.linux-amd64.tar in 48.805855655s"

2025-10-25 08:21:52,685 p=3135610 u=root n=ansible | k8s-m1 | FAILED | rc=2 >>
[Errno 2] No such file or directory: b'KUBECONFIG=/etc/rancher/rke2/rke2.yaml'

2025-10-25 08:22:09,906 p=3136087 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
NAME     STATUS     ROLES                AGE     VERSION          INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-m1   NotReady   control-plane,etcd   9m10s   v1.34.1+rke2r1   10.0.6.12     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2

2025-10-25 08:22:34,834 p=3136647 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
Name:               k8s-m1
Roles:              control-plane,etcd
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=k8s-m1
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/control-plane=true
                    node-role.kubernetes.io/etcd=true
Annotations:        etcd.rke2.cattle.io/local-snapshots-timestamp: 2025-10-25T01:13:09Z
                    etcd.rke2.cattle.io/node-address: 10.0.6.12
                    etcd.rke2.cattle.io/node-name: k8s-m1-cae019fe
                    node.alpha.kubernetes.io/ttl: 0
                    rke2.io/encryption-config-hash: start-32158d4975f12f19a56a43b59cd6a22fd94d0a152ff4ec29584bd2145aaff9f5
                    rke2.io/node-args:
                      ["server","--agent-token","********","--token","********","--data-dir","/var/lib/rancher/rke2","--cni","cilium","--tls-san","cluster.local...
                    rke2.io/node-config-hash: KLPS7ZUZNJLXDVR3MSMQFION5RS77AOUUDKL4DK46N7OA577FUOA====
                    rke2.io/node-env: {}
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 25 Oct 2025 01:13:01 +0000
Taints:             CriticalAddonsOnly=true:NoExecute
                    node-role.kubernetes.io/etcd=true:NoExecute
                    node-role.kubernetes.io/control-plane=true:NoSchedule
                    node.kubernetes.io/not-ready:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  k8s-m1
  AcquireTime:     <unset>
  RenewTime:       Sat, 25 Oct 2025 01:22:34 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  EtcdIsVoter      True    Sat, 25 Oct 2025 01:18:18 +0000   Sat, 25 Oct 2025 01:13:18 +0000   MemberNotLearner             Node is a voting member of the etcd cluster
  MemoryPressure   False   Sat, 25 Oct 2025 01:17:43 +0000   Sat, 25 Oct 2025 01:13:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 25 Oct 2025 01:17:43 +0000   Sat, 25 Oct 2025 01:13:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 25 Oct 2025 01:17:43 +0000   Sat, 25 Oct 2025 01:13:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            False   Sat, 25 Oct 2025 01:17:43 +0000   Sat, 25 Oct 2025 01:13:00 +0000   KubeletNotReady              container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized
Addresses:
  InternalIP:  10.0.6.12
  Hostname:    k8s-m1
Capacity:
  cpu:                4
  ephemeral-storage:  59324Mi
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3960896Ki
  pods:               100
Allocatable:
  cpu:                4
  ephemeral-storage:  59095436447
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3960896Ki
  pods:               100
System Info:
  Machine ID:                 b79012181a3f45208e1e5ef6d1d0cddb
  System UUID:                07684d56-0488-2a85-e401-28c7c87c5c24
  Boot ID:                    27a25b19-18b1-41d1-a68f-f0bc7d016c72
  Kernel Version:             6.8.0-86-generic
  OS Image:                   Ubuntu 24.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://2.1.4-k3s2
  Kubelet Version:            v1.34.1+rke2r1
  Kube-Proxy Version:         
PodCIDR:                      10.42.0.0/24
PodCIDRs:                     10.42.0.0/24
Non-terminated Pods:          (6 in total)
  Namespace                   Name                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                              ------------  ----------  ---------------  -------------  ---
  kube-system                 cilium-98bkv                      100m (2%)     0 (0%)      10Mi (0%)        0 (0%)         9m11s
  kube-system                 etcd-k8s-m1                       200m (5%)     0 (0%)      512Mi (13%)      0 (0%)         9m29s
  kube-system                 kube-apiserver-k8s-m1             250m (6%)     0 (0%)      1Gi (26%)        0 (0%)         9m29s
  kube-system                 kube-controller-manager-k8s-m1    200m (5%)     0 (0%)      256Mi (6%)       0 (0%)         9m29s
  kube-system                 kube-scheduler-k8s-m1             100m (2%)     0 (0%)      128Mi (3%)       0 (0%)         9m29s
  kube-system                 kube-vip-ds-pmnqp                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m25s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                850m (21%)    0 (0%)
  memory             1930Mi (49%)  0 (0%)
  ephemeral-storage  0 (0%)        0 (0%)
  hugepages-1Gi      0 (0%)        0 (0%)
  hugepages-2Mi      0 (0%)        0 (0%)
Events:
  Type     Reason                          Age                From               Message
  ----     ------                          ----               ----               -------
  Normal   Starting                        10m                kubelet            Starting kubelet.
  Warning  InvalidDiskCapacity             10m                kubelet            invalid capacity 0 on image filesystem
  Normal   NodeAllocatableEnforced         10m                kubelet            Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory         10m (x8 over 10m)  kubelet            Node k8s-m1 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure           10m (x8 over 10m)  kubelet            Node k8s-m1 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID            10m (x7 over 10m)  kubelet            Node k8s-m1 status is now: NodeHasSufficientPID
  Normal   CertificateExpirationOK         9m32s              rke2-cert-monitor  Node and Certificate Authority certificates managed by rke2 are OK
  Normal   NodePasswordValidationComplete  9m26s              rke2-supervisor    Deferred node password secret validation complete
  Normal   RegisteredNode                  9m25s              node-controller    Node k8s-m1 event: Registered Node k8s-m1 in Controller

2025-10-25 08:22:48,776 p=3137004 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
NAME                                                    READY   STATUS      RESTARTS        AGE     IP          NODE     NOMINATED NODE   READINESS GATES
cilium-98bkv                                            0/1     Init:1/7    5 (2m15s ago)   9m25s   10.0.6.12   k8s-m1   <none>           <none>
cilium-operator-69c7b95789-89t66                        0/1     Pending     0               9m25s   <none>      <none>   <none>           <none>
cilium-operator-69c7b95789-z9dnz                        0/1     Pending     0               9m25s   <none>      <none>   <none>           <none>
etcd-k8s-m1                                             1/1     Running     0               9m43s   10.0.6.12   k8s-m1   <none>           <none>
helm-install-rke2-cilium-dt4kt                          0/1     Completed   0               9m39s   10.0.6.12   k8s-m1   <none>           <none>
helm-install-rke2-coredns-7sb7t                         0/1     Completed   0               9m39s   10.0.6.12   k8s-m1   <none>           <none>
helm-install-rke2-ingress-nginx-hbvng                   0/1     Pending     0               9m39s   <none>      <none>   <none>           <none>
helm-install-rke2-runtimeclasses-z58c6                  0/1     Pending     0               9m39s   <none>      <none>   <none>           <none>
helm-install-rke2-snapshot-controller-crd-7k6h4         0/1     Pending     0               9m39s   <none>      <none>   <none>           <none>
helm-install-rke2-snapshot-controller-tqtbm             0/1     Pending     0               9m39s   <none>      <none>   <none>           <none>
kube-apiserver-k8s-m1                                   1/1     Running     0               9m43s   10.0.6.12   k8s-m1   <none>           <none>
kube-controller-manager-k8s-m1                          1/1     Running     0               9m43s   10.0.6.12   k8s-m1   <none>           <none>
kube-scheduler-k8s-m1                                   1/1     Running     1 (3m20s ago)   9m43s   10.0.6.12   k8s-m1   <none>           <none>
kube-vip-ds-pmnqp                                       1/1     Running     3 (58s ago)     9m39s   10.0.6.12   k8s-m1   <none>           <none>
rke2-coredns-rke2-coredns-autoscaler-5fcf54974d-xwqrl   0/1     Pending     0               9m26s   <none>      <none>   <none>           <none>
rke2-coredns-rke2-coredns-bd94c5787-q7h4v               0/1     Pending     0               9m26s   <none>      <none>   <none>           <none>

2025-10-25 08:23:14,968 p=3137533 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
cilium        1         1         0       1            0           kubernetes.io/os=linux   9m51s
kube-vip-ds   1         1         1       1            1           <none>                   10m

2025-10-25 08:23:58,328 p=3138064 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
Name:           cilium
Namespace:      kube-system
Selector:       k8s-app=cilium
Node-Selector:  kubernetes.io/os=linux
Labels:         app.kubernetes.io/managed-by=Helm
                app.kubernetes.io/name=cilium-agent
                app.kubernetes.io/part-of=cilium
                k8s-app=cilium
Annotations:    deprecated.daemonset.template.generation: 1
                meta.helm.sh/release-name: rke2-cilium
                meta.helm.sh/release-namespace: kube-system
Desired Number of Nodes Scheduled: 1
Current Number of Nodes Scheduled: 1
Number of Nodes Scheduled with Up-to-date Pods: 1
Number of Nodes Scheduled with Available Pods: 0
Number of Nodes Misscheduled: 0
Pods Status:  0 Running / 1 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app.kubernetes.io/name=cilium-agent
                    app.kubernetes.io/part-of=cilium
                    k8s-app=cilium
  Annotations:      kubectl.kubernetes.io/default-container: cilium-agent
  Service Account:  cilium
  Init Containers:
   install-portmap-cni-plugin:
    Image:      rancher/hardened-cni-plugins:v1.8.0-build20250909
    Port:       <none>
    Host Port:  <none>
    Environment:
      SKIP_CNI_BINARIES:  bandwidth,bridge,dhcp,firewall,flannel,host-device,host-local,ipvlan,loopback,macvlan,ptp,sbr,static,tuning,vlan,vrf
    Mounts:
      /host/opt/cni/bin from cni-path (rw)
   config:
    Image:      rancher/mirrored-cilium-cilium:v1.18.1
    Port:       <none>
    Host Port:  <none>
    Command:
      cilium-dbg
      build-config
    Environment:
      K8S_NODE_NAME:          (v1:spec.nodeName)
      CILIUM_K8S_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /tmp from tmp (rw)
   mount-cgroup:
    Image:      rancher/mirrored-cilium-cilium:v1.18.1
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -ec
      cp /usr/bin/cilium-mount /hostbin/cilium-mount;
      nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT;
      rm /hostbin/cilium-mount
      
    Environment:
      CGROUP_ROOT:  /run/cilium/cgroupv2
      BIN_PATH:     /opt/cni/bin
    Mounts:
      /hostbin from cni-path (rw)
      /hostproc from hostproc (rw)
   apply-sysctl-overwrites:
    Image:      rancher/mirrored-cilium-cilium:v1.18.1
    Port:       <none>
    Host Port:  <none>
    Command:
      sh
      -ec
      cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;
      nsenter --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-sysctlfix";
      rm /hostbin/cilium-sysctlfix
      
    Environment:
      BIN_PATH:  /opt/cni/bin
    Mounts:
      /hostbin from cni-path (rw)
      /hostproc from hostproc (rw)
   mount-bpf-fs:
    Image:      rancher/mirrored-cilium-cilium:v1.18.1
    Port:       <none>
    Host Port:  <none>
    Command:
      /bin/bash
      -c
      --
    Args:
      mount | grep "/sys/fs/bpf type bpf" || mount -t bpf bpf /sys/fs/bpf
    Environment:  <none>
    Mounts:
      /sys/fs/bpf from bpf-maps (rw)
   clean-cilium-state:
    Image:      rancher/mirrored-cilium-cilium:v1.18.1
    Port:       <none>
    Host Port:  <none>
    Command:
      /init-container.sh
    Environment:
      CILIUM_ALL_STATE:           <set to the key 'clean-cilium-state' of config map 'cilium-config'>         Optional: true
      CILIUM_BPF_STATE:           <set to the key 'clean-cilium-bpf-state' of config map 'cilium-config'>     Optional: true
      WRITE_CNI_CONF_WHEN_READY:  <set to the key 'write-cni-conf-when-ready' of config map 'cilium-config'>  Optional: true
    Mounts:
      /run/cilium/cgroupv2 from cilium-cgroup (rw)
      /sys/fs/bpf from bpf-maps (rw)
      /var/run/cilium from cilium-run (rw)
   install-cni-binaries:
    Image:      rancher/mirrored-cilium-cilium:v1.18.1
    Port:       <none>
    Host Port:  <none>
    Command:
      /install-plugin.sh
    Requests:
      cpu:        100m
      memory:     10Mi
    Environment:  <none>
    Mounts:
      /host/opt/cni/bin from cni-path (rw)
  Containers:
   cilium-agent:
    Image:      rancher/mirrored-cilium-cilium:v1.18.1
    Port:       <none>
    Host Port:  <none>
    Command:
      cilium-agent
    Args:
      --config-dir=/tmp/cilium/config-map
    Liveness:   http-get http://127.0.0.1:9879/healthz delay=0s timeout=5s period=30s #success=1 #failure=10
    Readiness:  http-get http://127.0.0.1:9879/healthz delay=0s timeout=5s period=30s #success=1 #failure=3
    Startup:    http-get http://127.0.0.1:9879/healthz delay=5s timeout=1s period=2s #success=1 #failure=300
    Environment:
      K8S_NODE_NAME:                  (v1:spec.nodeName)
      CILIUM_K8S_NAMESPACE:           (v1:metadata.namespace)
      CILIUM_CLUSTERMESH_CONFIG:     /var/lib/cilium/clustermesh/
      GOMEMLIMIT:                    node allocatable (limits.memory)
      KUBE_CLIENT_BACKOFF_BASE:      1
      KUBE_CLIENT_BACKOFF_DURATION:  120
    Mounts:
      /host/etc/cni/net.d from etc-cni-netd (rw)
      /host/proc/sys/kernel from host-proc-sys-kernel (rw)
      /host/proc/sys/net from host-proc-sys-net (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /sys/fs/bpf from bpf-maps (rw)
      /tmp from tmp (rw)
      /var/lib/cilium/clustermesh from clustermesh-secrets (ro)
      /var/run/cilium from cilium-run (rw)
      /var/run/cilium/netns from cilium-netns (rw)
  Volumes:
   tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
   cilium-run:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/cilium
    HostPathType:  DirectoryOrCreate
   cilium-netns:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/netns
    HostPathType:  DirectoryOrCreate
   bpf-maps:
    Type:          HostPath (bare host directory volume)
    Path:          /sys/fs/bpf
    HostPathType:  DirectoryOrCreate
   hostproc:
    Type:          HostPath (bare host directory volume)
    Path:          /proc
    HostPathType:  Directory
   cilium-cgroup:
    Type:          HostPath (bare host directory volume)
    Path:          /run/cilium/cgroupv2
    HostPathType:  DirectoryOrCreate
   cni-path:
    Type:          HostPath (bare host directory volume)
    Path:          /opt/cni/bin
    HostPathType:  DirectoryOrCreate
   etc-cni-netd:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  DirectoryOrCreate
   lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   clustermesh-secrets:
    Type:        Projected (a volume that contains injected data from multiple sources)
    SecretName:  cilium-clustermesh
    Optional:    true
    SecretName:  clustermesh-apiserver-remote-cert
    Optional:    true
    SecretName:  clustermesh-apiserver-local-cert
    Optional:    true
   host-proc-sys-net:
    Type:          HostPath (bare host directory volume)
    Path:          /proc/sys/net
    HostPathType:  Directory
   host-proc-sys-kernel:
    Type:               HostPath (bare host directory volume)
    Path:               /proc/sys/kernel
    HostPathType:       Directory
  Priority Class Name:  system-node-critical
  Node-Selectors:       kubernetes.io/os=linux
  Tolerations:          op=Exists
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  10m   daemonset-controller  Created pod: cilium-98bkv

2025-10-25 08:24:08,466 p=3138302 u=root n=ansible | k8s-m1 | FAILED | rc=1 >>
error: unknown command "neat" for "kubectl"

Did you mean this?
	set
	getnon-zero return code

2025-10-25 08:24:16,661 p=3138574 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
pod/cilium-98bkv

2025-10-25 08:24:23,816 p=3138802 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
Name:                 cilium-98bkv
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Service Account:      cilium
Node:                 k8s-m1/10.0.6.12
Start Time:           Sat, 25 Oct 2025 01:13:25 +0000
Labels:               app.kubernetes.io/name=cilium-agent
                      app.kubernetes.io/part-of=cilium
                      controller-revision-hash=5f84779549
                      k8s-app=cilium
                      pod-template-generation=1
Annotations:          kubectl.kubernetes.io/default-container: cilium-agent
Status:               Pending
SeccompProfile:       Unconfined
IP:                   10.0.6.12
IPs:
  IP:           10.0.6.12
Controlled By:  DaemonSet/cilium
Init Containers:
  install-portmap-cni-plugin:
    Container ID:   containerd://ff2b3728f12ce2b13a37d83d2eac1ba1ddc653f0f86d9f2196802b80f8adb848
    Image:          rancher/hardened-cni-plugins:v1.8.0-build20250909
    Image ID:       sha256:86ebf489f678d0611e61fdc3ff13c05d7c74ee6b40e8a5d439bd115f163f212e
    Port:           <none>
    Host Port:      <none>
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 25 Oct 2025 01:13:27 +0000
      Finished:     Sat, 25 Oct 2025 01:13:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      SKIP_CNI_BINARIES:  bandwidth,bridge,dhcp,firewall,flannel,host-device,host-local,ipvlan,loopback,macvlan,ptp,sbr,static,tuning,vlan,vrf
    Mounts:
      /host/opt/cni/bin from cni-path (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g558h (ro)
  config:
    Container ID:  containerd://7d3149b2c2c99a190d213582a20e3259b674bb9d514a278a09b5969c6349fc26
    Image:         rancher/mirrored-cilium-cilium:v1.18.1
    Image ID:      sha256:d17ba2d17aae429d83ff8b699e75ce1f6a50ba0de791f2c685fb689176c5f69f
    Port:          <none>
    Host Port:     <none>
    Command:
      cilium-dbg
      build-config
    State:       Waiting
      Reason:    CrashLoopBackOff
    Last State:  Terminated
      Reason:    Error
      Message:   time=2025-10-25T01:21:57.927052905Z level=info msg=Running subsys=cilium-dbg
time=2025-10-25T01:21:57.929107844Z level=info msg="Starting hive" subsys=cilium-dbg
time=2025-10-25T01:21:57.929226373Z level=info msg="Establishing connection to apiserver" subsys=cilium-dbg module=k8s-client ipAddr=https://10.43.0.1:443
time=2025-10-25T01:22:34.704557389Z level=info msg="Establishing connection to apiserver" subsys=cilium-dbg module=k8s-client ipAddr=https://10.43.0.1:443
time=2025-10-25T01:23:04.733319759Z level=error msg="Unable to contact k8s api-server" subsys=cilium-dbg module=k8s-client ipAddr=https://10.43.0.1:443 error="Get \"https://10.43.0.1:443/api/v1/namespaces/kube-system\": dial tcp 10.43.0.1:443: i/o timeout"
time=2025-10-25T01:23:04.733428796Z level=error msg="Start hook failed" subsys=cilium-dbg function="client.(*compositeClientset).onStart (k8s-client)" error="Get \"https://10.43.0.1:443/api/v1/namespaces/kube-system\": dial tcp 10.43.0.1:443: i/o timeout"
time=2025-10-25T01:23:04.733455322Z level=error msg="Failed to start hive" subsys=cilium-dbg error="Get \"https://10.43.0.1:443/api/v1/namespaces/kube-system\": dial tcp 10.43.0.1:443: i/o timeout" duration=1m6.80429708s
time=2025-10-25T01:23:04.733523013Z level=info msg="Stopping hive" subsys=cilium-dbg
Error: Build config failed: failed to start: Get "https://10.43.0.1:443/api/v1/namespaces/kube-system": dial tcp 10.43.0.1:443: i/o timeout

time=2025-10-25T01:23:04.733624686Z level=info msg="Stopped hive" subsys=cilium-dbg duration=84.598s

      Exit Code:    1
      Started:      Sat, 25 Oct 2025 01:21:57 +0000
      Finished:     Sat, 25 Oct 2025 01:23:04 +0000
    Ready:          False
    Restart Count:  5
    Environment:
      K8S_NODE_NAME:          (v1:spec.nodeName)
      CILIUM_K8S_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g558h (ro)
  mount-cgroup:
    Container ID:  
    Image:         rancher/mirrored-cilium-cilium:v1.18.1
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -ec
      cp /usr/bin/cilium-mount /hostbin/cilium-mount;
      nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT;
      rm /hostbin/cilium-mount
      
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:
      CGROUP_ROOT:  /run/cilium/cgroupv2
      BIN_PATH:     /opt/cni/bin
    Mounts:
      /hostbin from cni-path (rw)
      /hostproc from hostproc (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g558h (ro)
  apply-sysctl-overwrites:
    Container ID:  
    Image:         rancher/mirrored-cilium-cilium:v1.18.1
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -ec
      cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;
      nsenter --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-sysctlfix";
      rm /hostbin/cilium-sysctlfix
      
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:
      BIN_PATH:  /opt/cni/bin
    Mounts:
      /hostbin from cni-path (rw)
      /hostproc from hostproc (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g558h (ro)
  mount-bpf-fs:
    Container ID:  
    Image:         rancher/mirrored-cilium-cilium:v1.18.1
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -c
      --
    Args:
      mount | grep "/sys/fs/bpf type bpf" || mount -t bpf bpf /sys/fs/bpf
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /sys/fs/bpf from bpf-maps (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g558h (ro)
  clean-cilium-state:
    Container ID:  
    Image:         rancher/mirrored-cilium-cilium:v1.18.1
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      /init-container.sh
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:
      CILIUM_ALL_STATE:           <set to the key 'clean-cilium-state' of config map 'cilium-config'>         Optional: true
      CILIUM_BPF_STATE:           <set to the key 'clean-cilium-bpf-state' of config map 'cilium-config'>     Optional: true
      WRITE_CNI_CONF_WHEN_READY:  <set to the key 'write-cni-conf-when-ready' of config map 'cilium-config'>  Optional: true
    Mounts:
      /run/cilium/cgroupv2 from cilium-cgroup (rw)
      /sys/fs/bpf from bpf-maps (rw)
      /var/run/cilium from cilium-run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g558h (ro)
  install-cni-binaries:
    Container ID:  
    Image:         rancher/mirrored-cilium-cilium:v1.18.1
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      /install-plugin.sh
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     10Mi
    Environment:  <none>
    Mounts:
      /host/opt/cni/bin from cni-path (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g558h (ro)
Containers:
  cilium-agent:
    Container ID:  
    Image:         rancher/mirrored-cilium-cilium:v1.18.1
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      cilium-agent
    Args:
      --config-dir=/tmp/cilium/config-map
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Liveness:       http-get http://127.0.0.1:9879/healthz delay=0s timeout=5s period=30s #success=1 #failure=10
    Readiness:      http-get http://127.0.0.1:9879/healthz delay=0s timeout=5s period=30s #success=1 #failure=3
    Startup:        http-get http://127.0.0.1:9879/healthz delay=5s timeout=1s period=2s #success=1 #failure=300
    Environment:
      K8S_NODE_NAME:                  (v1:spec.nodeName)
      CILIUM_K8S_NAMESPACE:          kube-system (v1:metadata.namespace)
      CILIUM_CLUSTERMESH_CONFIG:     /var/lib/cilium/clustermesh/
      GOMEMLIMIT:                    node allocatable (limits.memory)
      KUBE_CLIENT_BACKOFF_BASE:      1
      KUBE_CLIENT_BACKOFF_DURATION:  120
    Mounts:
      /host/etc/cni/net.d from etc-cni-netd (rw)
      /host/proc/sys/kernel from host-proc-sys-kernel (rw)
      /host/proc/sys/net from host-proc-sys-net (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /sys/fs/bpf from bpf-maps (rw)
      /tmp from tmp (rw)
      /var/lib/cilium/clustermesh from clustermesh-secrets (ro)
      /var/run/cilium from cilium-run (rw)
      /var/run/cilium/netns from cilium-netns (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g558h (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 False 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  cilium-run:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/cilium
    HostPathType:  DirectoryOrCreate
  cilium-netns:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/netns
    HostPathType:  DirectoryOrCreate
  bpf-maps:
    Type:          HostPath (bare host directory volume)
    Path:          /sys/fs/bpf
    HostPathType:  DirectoryOrCreate
  hostproc:
    Type:          HostPath (bare host directory volume)
    Path:          /proc
    HostPathType:  Directory
  cilium-cgroup:
    Type:          HostPath (bare host directory volume)
    Path:          /run/cilium/cgroupv2
    HostPathType:  DirectoryOrCreate
  cni-path:
    Type:          HostPath (bare host directory volume)
    Path:          /opt/cni/bin
    HostPathType:  DirectoryOrCreate
  etc-cni-netd:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  DirectoryOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  clustermesh-secrets:
    Type:        Projected (a volume that contains injected data from multiple sources)
    SecretName:  cilium-clustermesh
    Optional:    true
    SecretName:  clustermesh-apiserver-remote-cert
    Optional:    true
    SecretName:  clustermesh-apiserver-local-cert
    Optional:    true
  host-proc-sys-net:
    Type:          HostPath (bare host directory volume)
    Path:          /proc/sys/net
    HostPathType:  Directory
  host-proc-sys-kernel:
    Type:          HostPath (bare host directory volume)
    Path:          /proc/sys/kernel
    HostPathType:  Directory
  kube-api-access-g558h:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                  From               Message
  ----     ------     ----                 ----               -------
  Normal   Scheduled  10m                  default-scheduler  Successfully assigned kube-system/cilium-98bkv to k8s-m1
  Normal   Pulled     11m                  kubelet            Container image "rancher/hardened-cni-plugins:v1.8.0-build20250909" already present on machine
  Normal   Created    10m                  kubelet            Created container: install-portmap-cni-plugin
  Normal   Started    10m                  kubelet            Started container install-portmap-cni-plugin
  Normal   Pulled     2m28s (x6 over 10m)  kubelet            Container image "rancher/mirrored-cilium-cilium:v1.18.1" already present on machine
  Normal   Created    2m28s (x6 over 10m)  kubelet            Created container: config
  Normal   Started    2m28s (x6 over 10m)  kubelet            Started container config
  Warning  BackOff    1s (x19 over 8m38s)  kubelet            Back-off restarting failed container config in pod cilium-98bkv_kube-system(a691cc3a-d10b-456c-95f3-76f56b960e16)

2025-10-25 08:24:48,608 p=3139322 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
time=2025-10-25T01:21:57.927052905Z level=info msg=Running subsys=cilium-dbg
time=2025-10-25T01:21:57.929107844Z level=info msg="Starting hive" subsys=cilium-dbg
time=2025-10-25T01:21:57.929226373Z level=info msg="Establishing connection to apiserver" subsys=cilium-dbg module=k8s-client ipAddr=https://10.43.0.1:443
time=2025-10-25T01:22:34.704557389Z level=info msg="Establishing connection to apiserver" subsys=cilium-dbg module=k8s-client ipAddr=https://10.43.0.1:443
time=2025-10-25T01:23:04.733319759Z level=error msg="Unable to contact k8s api-server" subsys=cilium-dbg module=k8s-client ipAddr=https://10.43.0.1:443 error="Get \"https://10.43.0.1:443/api/v1/namespaces/kube-system\": dial tcp 10.43.0.1:443: i/o timeout"
time=2025-10-25T01:23:04.733428796Z level=error msg="Start hook failed" subsys=cilium-dbg function="client.(*compositeClientset).onStart (k8s-client)" error="Get \"https://10.43.0.1:443/api/v1/namespaces/kube-system\": dial tcp 10.43.0.1:443: i/o timeout"
time=2025-10-25T01:23:04.733455322Z level=error msg="Failed to start hive" subsys=cilium-dbg error="Get \"https://10.43.0.1:443/api/v1/namespaces/kube-system\": dial tcp 10.43.0.1:443: i/o timeout" duration=1m6.80429708s
time=2025-10-25T01:23:04.733523013Z level=info msg="Stopping hive" subsys=cilium-dbg
Error: Build config failed: failed to start: Get "https://10.43.0.1:443/api/v1/namespaces/kube-system": dial tcp 10.43.0.1:443: i/o timeout

time=2025-10-25T01:23:04.733624686Z level=info msg="Stopped hive" subsys=cilium-dbg duration=84.598s

2025-10-25 08:25:38,389 p=3140231 u=root n=ansible | PLAY [Set rp_filter to 0 across cluster] *******************************************************
2025-10-25 08:25:40,104 p=3140231 u=root n=ansible | TASK [Gathering Facts] *************************************************************************
2025-10-25 08:25:40,104 p=3140231 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:25:41,226 p=3140231 u=root n=ansible | TASK [Create persistent sysctl config for rp_filter] *******************************************
2025-10-25 08:25:41,227 p=3140231 u=root n=ansible | changed: [k8s-m1]
2025-10-25 08:25:41,748 p=3140231 u=root n=ansible | TASK [Apply sysctl from rp-filter config] ******************************************************
2025-10-25 08:25:41,748 p=3140231 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:25:42,314 p=3140231 u=root n=ansible | TASK [List interfaces from /proc to set rp_filter] *********************************************
2025-10-25 08:25:42,314 p=3140231 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:25:43,015 p=3140231 u=root n=ansible | TASK [Set rp_filter=0 for each interface] ******************************************************
2025-10-25 08:25:43,016 p=3140231 u=root n=ansible | changed: [k8s-m1] => (item={'path': '/proc/sys/net/ipv4/conf/all', 'mode': '0555', 'isdir': True, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': False, 'uid': 0, 'gid': 0, 'size': 0, 'inode': 3373, 'dev': 23, 'nlink': 1, 'atime': 1761300441.47, 'mtime': 1761300441.47, 'ctime': 1761300441.47, 'gr_name': 'root', 'pw_name': 'root', 'wusr': False, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})
2025-10-25 08:25:43,393 p=3140231 u=root n=ansible | changed: [k8s-m1] => (item={'path': '/proc/sys/net/ipv4/conf/default', 'mode': '0555', 'isdir': True, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': False, 'uid': 0, 'gid': 0, 'size': 0, 'inode': 3371, 'dev': 23, 'nlink': 1, 'atime': 1761300441.47, 'mtime': 1761300441.47, 'ctime': 1761300441.47, 'gr_name': 'root', 'pw_name': 'root', 'wusr': False, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})
2025-10-25 08:25:43,803 p=3140231 u=root n=ansible | changed: [k8s-m1] => (item={'path': '/proc/sys/net/ipv4/conf/ens34', 'mode': '0555', 'isdir': True, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': False, 'uid': 0, 'gid': 0, 'size': 0, 'inode': 217399, 'dev': 23, 'nlink': 1, 'atime': 1761355543.791983, 'mtime': 1761355543.791983, 'ctime': 1761355543.791983, 'gr_name': 'root', 'pw_name': 'root', 'wusr': False, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})
2025-10-25 08:25:44,164 p=3140231 u=root n=ansible | changed: [k8s-m1] => (item={'path': '/proc/sys/net/ipv4/conf/lo', 'mode': '0555', 'isdir': True, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': False, 'uid': 0, 'gid': 0, 'size': 0, 'inode': 217400, 'dev': 23, 'nlink': 1, 'atime': 1761355543.791983, 'mtime': 1761355543.791983, 'ctime': 1761355543.791983, 'gr_name': 'root', 'pw_name': 'root', 'wusr': False, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})
2025-10-25 08:25:44,587 p=3140231 u=root n=ansible | TASK [Verify rp_filter values on node] *********************************************************
2025-10-25 08:25:44,587 p=3140231 u=root n=ansible | ok: [k8s-m1]
2025-10-25 08:25:44,637 p=3140231 u=root n=ansible | TASK [Print rp_filter verification] ************************************************************
2025-10-25 08:25:44,638 p=3140231 u=root n=ansible | ok: [k8s-m1] => 
  rp_verify.stdout_lines:
  - all=0
  - default=0
  - all=0
  - default=0
  - ens34=2
  - lo=0
2025-10-25 08:25:44,692 p=3140231 u=root n=ansible | PLAY RECAP *************************************************************************************
2025-10-25 08:25:44,692 p=3140231 u=root n=ansible | k8s-m1                     : ok=7    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 08:27:00,909 p=3140551 u=root n=ansible | k8s-m1 | FAILED | rc=1 >>
daemonset.apps/cilium restartedE1025 01:27:02.389379   16178 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"http://localhost:8080/api?timeout=32s\": dial tcp 127.0.0.1:8080: connect: connection refused"
E1025 01:27:02.389804   16178 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"http://localhost:8080/api?timeout=32s\": dial tcp 127.0.0.1:8080: connect: connection refused"
E1025 01:27:02.391544   16178 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"http://localhost:8080/api?timeout=32s\": dial tcp 127.0.0.1:8080: connect: connection refused"
E1025 01:27:02.391853   16178 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"http://localhost:8080/api?timeout=32s\": dial tcp 127.0.0.1:8080: connect: connection refused"
E1025 01:27:02.393117   16178 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"http://localhost:8080/api?timeout=32s\": dial tcp 127.0.0.1:8080: connect: connection refused"
The connection to the server localhost:8080 was refused - did you specify the right host or port?non-zero return code

2025-10-25 08:27:12,648 p=3140782 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
NAME           READY   STATUS     RESTARTS   AGE
cilium-br9mc   0/1     Init:1/7   0          7s

2025-10-25 08:27:21,445 p=3141420 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
Name:                 cilium-br9mc
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Service Account:      cilium
Node:                 k8s-m1/10.0.6.12
Start Time:           Sat, 25 Oct 2025 01:26:57 +0000
Labels:               app.kubernetes.io/name=cilium-agent
                      app.kubernetes.io/part-of=cilium
                      controller-revision-hash=7cb76bbdc7
                      k8s-app=cilium
                      pod-template-generation=2
Annotations:          kubectl.kubernetes.io/default-container: cilium-agent
                      kubectl.kubernetes.io/restartedAt: 2025-10-25T01:26:57Z
Status:               Pending
SeccompProfile:       Unconfined
IP:                   10.0.6.12
IPs:
  IP:           10.0.6.12
Controlled By:  DaemonSet/cilium
Init Containers:
  install-portmap-cni-plugin:
    Container ID:   containerd://f7f7b99d1b950d392076330169191d9190a2c997184c721285f78e803dc86971
    Image:          rancher/hardened-cni-plugins:v1.8.0-build20250909
    Image ID:       sha256:86ebf489f678d0611e61fdc3ff13c05d7c74ee6b40e8a5d439bd115f163f212e
    Port:           <none>
    Host Port:      <none>
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 25 Oct 2025 01:26:58 +0000
      Finished:     Sat, 25 Oct 2025 01:26:58 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      SKIP_CNI_BINARIES:  bandwidth,bridge,dhcp,firewall,flannel,host-device,host-local,ipvlan,loopback,macvlan,ptp,sbr,static,tuning,vlan,vrf
    Mounts:
      /host/opt/cni/bin from cni-path (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vjhw7 (ro)
  config:
    Container ID:  containerd://1b4070c39d0d1ef712ee8e840b20c5c62a34d37aeb3d531e4f221d5aea00e866
    Image:         rancher/mirrored-cilium-cilium:v1.18.1
    Image ID:      sha256:d17ba2d17aae429d83ff8b699e75ce1f6a50ba0de791f2c685fb689176c5f69f
    Port:          <none>
    Host Port:     <none>
    Command:
      cilium-dbg
      build-config
    State:          Running
      Started:      Sat, 25 Oct 2025 01:26:59 +0000
    Ready:          False
    Restart Count:  0
    Environment:
      K8S_NODE_NAME:          (v1:spec.nodeName)
      CILIUM_K8S_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vjhw7 (ro)
  mount-cgroup:
    Container ID:  
    Image:         rancher/mirrored-cilium-cilium:v1.18.1
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -ec
      cp /usr/bin/cilium-mount /hostbin/cilium-mount;
      nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT;
      rm /hostbin/cilium-mount
      
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:
      CGROUP_ROOT:  /run/cilium/cgroupv2
      BIN_PATH:     /opt/cni/bin
    Mounts:
      /hostbin from cni-path (rw)
      /hostproc from hostproc (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vjhw7 (ro)
  apply-sysctl-overwrites:
    Container ID:  
    Image:         rancher/mirrored-cilium-cilium:v1.18.1
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -ec
      cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;
      nsenter --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-sysctlfix";
      rm /hostbin/cilium-sysctlfix
      
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:
      BIN_PATH:  /opt/cni/bin
    Mounts:
      /hostbin from cni-path (rw)
      /hostproc from hostproc (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vjhw7 (ro)
  mount-bpf-fs:
    Container ID:  
    Image:         rancher/mirrored-cilium-cilium:v1.18.1
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      /bin/bash
      -c
      --
    Args:
      mount | grep "/sys/fs/bpf type bpf" || mount -t bpf bpf /sys/fs/bpf
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /sys/fs/bpf from bpf-maps (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vjhw7 (ro)
  clean-cilium-state:
    Container ID:  
    Image:         rancher/mirrored-cilium-cilium:v1.18.1
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      /init-container.sh
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Environment:
      CILIUM_ALL_STATE:           <set to the key 'clean-cilium-state' of config map 'cilium-config'>         Optional: true
      CILIUM_BPF_STATE:           <set to the key 'clean-cilium-bpf-state' of config map 'cilium-config'>     Optional: true
      WRITE_CNI_CONF_WHEN_READY:  <set to the key 'write-cni-conf-when-ready' of config map 'cilium-config'>  Optional: true
    Mounts:
      /run/cilium/cgroupv2 from cilium-cgroup (rw)
      /sys/fs/bpf from bpf-maps (rw)
      /var/run/cilium from cilium-run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vjhw7 (ro)
  install-cni-binaries:
    Container ID:  
    Image:         rancher/mirrored-cilium-cilium:v1.18.1
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      /install-plugin.sh
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     10Mi
    Environment:  <none>
    Mounts:
      /host/opt/cni/bin from cni-path (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vjhw7 (ro)
Containers:
  cilium-agent:
    Container ID:  
    Image:         rancher/mirrored-cilium-cilium:v1.18.1
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      cilium-agent
    Args:
      --config-dir=/tmp/cilium/config-map
    State:          Waiting
      Reason:       PodInitializing
    Ready:          False
    Restart Count:  0
    Liveness:       http-get http://127.0.0.1:9879/healthz delay=0s timeout=5s period=30s #success=1 #failure=10
    Readiness:      http-get http://127.0.0.1:9879/healthz delay=0s timeout=5s period=30s #success=1 #failure=3
    Startup:        http-get http://127.0.0.1:9879/healthz delay=5s timeout=1s period=2s #success=1 #failure=300
    Environment:
      K8S_NODE_NAME:                  (v1:spec.nodeName)
      CILIUM_K8S_NAMESPACE:          kube-system (v1:metadata.namespace)
      CILIUM_CLUSTERMESH_CONFIG:     /var/lib/cilium/clustermesh/
      GOMEMLIMIT:                    node allocatable (limits.memory)
      KUBE_CLIENT_BACKOFF_BASE:      1
      KUBE_CLIENT_BACKOFF_DURATION:  120
    Mounts:
      /host/etc/cni/net.d from etc-cni-netd (rw)
      /host/proc/sys/kernel from host-proc-sys-kernel (rw)
      /host/proc/sys/net from host-proc-sys-net (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /sys/fs/bpf from bpf-maps (rw)
      /tmp from tmp (rw)
      /var/lib/cilium/clustermesh from clustermesh-secrets (ro)
      /var/run/cilium from cilium-run (rw)
      /var/run/cilium/netns from cilium-netns (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vjhw7 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 False 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  cilium-run:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/cilium
    HostPathType:  DirectoryOrCreate
  cilium-netns:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/netns
    HostPathType:  DirectoryOrCreate
  bpf-maps:
    Type:          HostPath (bare host directory volume)
    Path:          /sys/fs/bpf
    HostPathType:  DirectoryOrCreate
  hostproc:
    Type:          HostPath (bare host directory volume)
    Path:          /proc
    HostPathType:  Directory
  cilium-cgroup:
    Type:          HostPath (bare host directory volume)
    Path:          /run/cilium/cgroupv2
    HostPathType:  DirectoryOrCreate
  cni-path:
    Type:          HostPath (bare host directory volume)
    Path:          /opt/cni/bin
    HostPathType:  DirectoryOrCreate
  etc-cni-netd:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  DirectoryOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  clustermesh-secrets:
    Type:        Projected (a volume that contains injected data from multiple sources)
    SecretName:  cilium-clustermesh
    Optional:    true
    SecretName:  clustermesh-apiserver-remote-cert
    Optional:    true
    SecretName:  clustermesh-apiserver-local-cert
    Optional:    true
  host-proc-sys-net:
    Type:          HostPath (bare host directory volume)
    Path:          /proc/sys/net
    HostPathType:  Directory
  host-proc-sys-kernel:
    Type:          HostPath (bare host directory volume)
    Path:          /proc/sys/kernel
    HostPathType:  Directory
  kube-api-access-vjhw7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  24s   default-scheduler  Successfully assigned kube-system/cilium-br9mc to k8s-m1
  Normal  Pulled     24s   kubelet            Container image "rancher/hardened-cni-plugins:v1.8.0-build20250909" already present on machine
  Normal  Created    24s   kubelet            Created container: install-portmap-cni-plugin
  Normal  Started    24s   kubelet            Started container install-portmap-cni-plugin
  Normal  Pulled     24s   kubelet            Container image "rancher/mirrored-cilium-cilium:v1.18.1" already present on machine
  Normal  Created    24s   kubelet            Created container: config
  Normal  Started    23s   kubelet            Started container config

2025-10-25 08:27:46,808 p=3142096 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
2
all=0
default=0/bin/sh: 1: cannot create : Directory nonexistent
/bin/sh: 1: cannot create : Directory nonexistent
/bin/sh: 1: cannot create : Directory nonexistent
/bin/sh: 1: cannot create : Directory nonexistent

2025-10-25 08:28:11,091 p=3142785 u=root n=ansible | k8s-m1 | FAILED | rc=1 >>
0sysctl: command line(0): invalid syntax, continuing...
sysctl: command line(0): invalid syntax, continuing...
sysctl: cannot stat /proc/sys/ens34: No such file or directory
sysctl: command line(0): invalid syntax, continuing...
sysctl: command line(0): invalid syntax, continuing...non-zero return code

2025-10-25 08:31:51,323 p=3147568 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
ens34

2025-10-25 08:32:18,450 p=3148300 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
net.ipv4.conf.all.forwarding = 1
net.ipv4.conf.default.forwarding = 1
=== rp_filter values ===
all=0
default=0
ens34=0
=== IP forwarding ===
ip_forward=1

2025-10-25 08:32:46,884 p=3148943 u=root n=ansible | k8s-m1 | FAILED | rc=127 >>
/bin/sh: 3: kubectl: not foundnon-zero return code

2025-10-25 08:32:54,956 p=3149219 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
daemonset.apps/cilium restarted

2025-10-25 08:33:20,893 p=3150084 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
cilium-2vnkx                                            0/1     Init:1/7    0               25s
cilium-operator-69c7b95789-89t66                        0/1     Pending     0               19m
cilium-operator-69c7b95789-z9dnz                        0/1     Pending     0               19m
helm-install-rke2-cilium-dt4kt                          0/1     Completed   0               20m

2025-10-25 08:33:33,328 p=3150423 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  37s   default-scheduler  Successfully assigned kube-system/cilium-2vnkx to k8s-m1
  Normal  Pulled     36s   kubelet            Container image "rancher/hardened-cni-plugins:v1.8.0-build20250909" already present on machine
  Normal  Created    36s   kubelet            Created container: install-portmap-cni-plugin
  Normal  Started    36s   kubelet            Started container install-portmap-cni-plugin
  Normal  Pulled     36s   kubelet            Container image "rancher/mirrored-cilium-cilium:v1.18.1" already present on machine
  Normal  Created    36s   kubelet            Created container: config
  Normal  Started    36s   kubelet            Started container config

2025-10-25 08:34:25,126 p=3151863 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Cilium pods status ===
cilium-2vnkx                                            0/1     Init:1/7    1 (23s ago)     89s
cilium-operator-69c7b95789-89t66                        0/1     Pending     0               21m
cilium-operator-69c7b95789-z9dnz                        0/1     Pending     0               21m
helm-install-rke2-cilium-dt4kt                          0/1     Completed   0               21m
=== Node status ===
NAME     STATUS     ROLES                AGE   VERSION          INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-m1   NotReady   control-plane,etcd   21m   v1.34.1+rke2r1   10.0.6.12     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2

2025-10-25 08:34:50,952 p=3152618 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
time=2025-10-25T01:34:04.997018696Z level=info msg=Running subsys=cilium-dbg
time=2025-10-25T01:34:04.998843356Z level=info msg="Starting hive" subsys=cilium-dbg
time=2025-10-25T01:34:04.998951464Z level=info msg="Establishing connection to apiserver" subsys=cilium-dbg module=k8s-client ipAddr=https://10.43.0.1:443
time=2025-10-25T01:34:40.033222048Z level=info msg="Establishing connection to apiserver" subsys=cilium-dbg module=k8s-client ipAddr=https://10.43.0.1:443

2025-10-25 08:35:12,135 p=3153096 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== API server pods ===
kube-apiserver-k8s-m1                                   1/1     Running     0               22m
=== Test API connectivity ===
API server not reachable
=== Check service CIDR ===
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR
kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   22m   <none>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0  0     0    0     0    0     0      0      0 --:--:--  0:00:04 --:--:--     0  0     0    0     0    0     0      0      0 --:--:--  0:00:05 --:--:--     0
curl: (28) Failed to connect to 10.43.0.1 port 443 after 5001 ms: Timeout was reached

2025-10-25 08:35:23,281 p=3153644 u=root n=ansible | k8s-m1 | FAILED | rc=1 >>
=== Current routes ===
default via 10.0.6.1 dev ens34 proto static 
10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.12 
=== Check iptables rules ===
=== Check if kube-proxy is running ===iptables: No chain/target/match by that name.non-zero return code

2025-10-25 08:35:35,971 p=3154017 u=root n=ansible | k8s-m1 | FAILED | rc=1 >>
=== All kube-system pods ===
NAME                                                    READY   STATUS      RESTARTS        AGE     IP          NODE     NOMINATED NODE   READINESS GATES
cilium-2vnkx                                            0/1     Init:1/7    2 (27s ago)     2m40s   10.0.6.12   k8s-m1   <none>           <none>
cilium-operator-69c7b95789-89t66                        0/1     Pending     0               22m     <none>      <none>   <none>           <none>
cilium-operator-69c7b95789-z9dnz                        0/1     Pending     0               22m     <none>      <none>   <none>           <none>
etcd-k8s-m1                                             1/1     Running     0               22m     10.0.6.12   k8s-m1   <none>           <none>
helm-install-rke2-cilium-dt4kt                          0/1     Completed   0               22m     10.0.6.12   k8s-m1   <none>           <none>
helm-install-rke2-coredns-7sb7t                         0/1     Completed   0               22m     10.0.6.12   k8s-m1   <none>           <none>
helm-install-rke2-ingress-nginx-hbvng                   0/1     Pending     0               22m     <none>      <none>   <none>           <none>
helm-install-rke2-runtimeclasses-z58c6                  0/1     Pending     0               22m     <none>      <none>   <none>           <none>
helm-install-rke2-snapshot-controller-crd-7k6h4         0/1     Pending     0               22m     <none>      <none>   <none>           <none>
helm-install-rke2-snapshot-controller-tqtbm             0/1     Pending     0               22m     <none>      <none>   <none>           <none>
kube-apiserver-k8s-m1                                   1/1     Running     0               22m     10.0.6.12   k8s-m1   <none>           <none>
kube-controller-manager-k8s-m1                          1/1     Running     0               22m     10.0.6.12   k8s-m1   <none>           <none>
kube-scheduler-k8s-m1                                   1/1     Running     1 (16m ago)     22m     10.0.6.12   k8s-m1   <none>           <none>
kube-vip-ds-pmnqp                                       1/1     Running     6 (6m10s ago)   22m     10.0.6.12   k8s-m1   <none>           <none>
rke2-coredns-rke2-coredns-autoscaler-5fcf54974d-xwqrl   0/1     Pending     0               22m     <none>      <none>   <none>           <none>
rke2-coredns-rke2-coredns-bd94c5787-q7h4v               0/1     Pending     0               22m     <none>      <none>   <none>           <none>
=== Check for kube-proxy ===non-zero return code

2025-10-25 08:35:49,516 p=3154433 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Test direct API server connection ===
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "Unauthorized",
  "reason": "Unauthorized",
  "code": 401
}=== Check API server binding ===
tcp6       0      0 :::6443                 :::*                    LISTEN      6973/kube-apiserver 
=== Check if API server is accessible locally ===
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "Unauthorized",
  "reason": "Unauthorized",
  "code": 401
}  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   157  100   157    0     0  16597      0 --:--:-- --:--:-- --:--:-- 17444
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   157  100   157    0     0  14035      0 --:--:-- --:--:-- --:--:-- 14272

2025-10-25 08:38:38,476 p=3156234 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Delete current Cilium pod ===
pod "cilium-2vnkx" force deleted from kube-system namespace
=== Set environment variable for API server ===
daemonset.apps/cilium env updatedWarning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.

2025-10-25 08:39:12,248 p=3157257 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== New Cilium pod status ===
cilium-operator-69c7b95789-89t66                        0/1     Pending     0               25m
cilium-operator-69c7b95789-z9dnz                        0/1     Pending     0               25m
cilium-ttl74                                            0/1     Running     0               32s
helm-install-rke2-cilium-dt4kt                          0/1     Completed   0               26m
=== Node status ===
NAME     STATUS     ROLES                AGE   VERSION          INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-m1   NotReady   control-plane,etcd   26m   v1.34.1+rke2r1   10.0.6.12     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2

2025-10-25 08:39:23,219 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Wait for the first server be ready - with CNI] *************
2025-10-25 08:39:23,221 p=3100301 u=root n=ansible | fatal: [k8s-m1]: FAILED! => changed=false 
  attempts: 100
  cmd: |-
    set -o pipefail
    /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep "k8s-m1"
  delta: '0:00:00.130732'
  end: '2025-10-25 01:39:24.714855'
  msg: ''
  rc: 0
  start: '2025-10-25 01:39:24.584123'
  stderr: ''
  stderr_lines: <omitted>
  stdout: k8s-m1   NotReady   control-plane,etcd   26m   v1.34.1+rke2r1
  stdout_lines: <omitted>
2025-10-25 08:39:24,553 p=3100301 u=root n=ansible | TASK [rke2-1.49.0 : Final steps] ***********************************************
2025-10-25 08:39:24,695 p=3100301 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/summary.yml for k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 08:39:25,353 p=3100301 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 08:39:25,353 p=3100301 u=root n=ansible | k8s-m1                     : ok=27   changed=9    unreachable=0    failed=1    skipped=30   rescued=0    ignored=0   
2025-10-25 08:39:25,353 p=3100301 u=root n=ansible | k8s-m2                     : ok=18   changed=2    unreachable=0    failed=0    skipped=31   rescued=0    ignored=0   
2025-10-25 08:39:25,353 p=3100301 u=root n=ansible | k8s-m3                     : ok=18   changed=2    unreachable=0    failed=0    skipped=31   rescued=0    ignored=0   
2025-10-25 08:39:25,353 p=3100301 u=root n=ansible | k8s-w01                    : ok=18   changed=2    unreachable=0    failed=0    skipped=31   rescued=0    ignored=0   
2025-10-25 08:39:25,354 p=3100301 u=root n=ansible | k8s-w02                    : ok=18   changed=2    unreachable=0    failed=0    skipped=31   rescued=0    ignored=0   
2025-10-25 08:39:25,354 p=3100301 u=root n=ansible | k8s-w03                    : ok=18   changed=2    unreachable=0    failed=0    skipped=31   rescued=0    ignored=0   
2025-10-25 08:40:28,349 p=3159166 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Final Cilium status ===
cilium-operator-69c7b95789-89t66                        0/1     Pending     0             27m
cilium-operator-69c7b95789-z9dnz                        0/1     Pending     0             27m
cilium-ttl74                                            0/1     Running     0             108s
helm-install-rke2-cilium-dt4kt                          0/1     Completed   0             27m
=== Final node status ===
NAME     STATUS     ROLES                AGE   VERSION          INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-m1   NotReady   control-plane,etcd   27m   v1.34.1+rke2r1   10.0.6.12     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2
=== All kube-system pods ===
NAME                                                    READY   STATUS      RESTARTS      AGE
cilium-operator-69c7b95789-89t66                        0/1     Pending     0             27m
cilium-operator-69c7b95789-z9dnz                        0/1     Pending     0             27m
cilium-ttl74                                            0/1     Running     0             108s
etcd-k8s-m1                                             1/1     Running     0             27m
helm-install-rke2-cilium-dt4kt                          0/1     Completed   0             27m
helm-install-rke2-coredns-7sb7t                         0/1     Completed   0             27m
helm-install-rke2-ingress-nginx-hbvng                   0/1     Pending     0             27m
helm-install-rke2-runtimeclasses-z58c6                  0/1     Pending     0             27m
helm-install-rke2-snapshot-controller-crd-7k6h4         0/1     Pending     0             27m
helm-install-rke2-snapshot-controller-tqtbm             0/1     Pending     0             27m
kube-apiserver-k8s-m1                                   1/1     Running     0             27m
kube-controller-manager-k8s-m1                          1/1     Running     0             27m
kube-scheduler-k8s-m1                                   1/1     Running     1 (20m ago)   27m
kube-vip-ds-pmnqp                                       1/1     Running     6 (11m ago)   27m
rke2-coredns-rke2-coredns-autoscaler-5fcf54974d-xwqrl   0/1     Pending     0             27m
rke2-coredns-rke2-coredns-bd94c5787-q7h4v               0/1     Pending     0             27m

2025-10-25 08:41:26,183 p=3159597 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Cilium pod logs ===
time=2025-10-25T01:41:07.843170539Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:08.842737419Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:09.843948148Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:10.844231752Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:11.842795381Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:12.843242882Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T01:41:13.844631426Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T01:41:14.84444786Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:41:15.842920543Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:16.842918816Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:41:17.842987255Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:18.843724107Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T01:41:19.843312723Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:20.842943312Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:21.843990484Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:22.842777063Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:23.843055672Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:41:24.843151987Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:41:25.844288365Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:41:26.844616951Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
=== Describe Cilium pod ===
  Normal   Created    2m43s                  kubelet            Created container: apply-sysctl-overwrites
  Normal   Started    2m43s                  kubelet            Started container apply-sysctl-overwrites
  Normal   Pulled     2m42s                  kubelet            Container image "rancher/mirrored-cilium-cilium:v1.18.1" already present on machine
  Normal   Created    2m42s                  kubelet            Created container: mount-bpf-fs
  Normal   Started    2m42s                  kubelet            Started container mount-bpf-fs
  Normal   Pulled     2m41s                  kubelet            Container image "rancher/mirrored-cilium-cilium:v1.18.1" already present on machine
  Normal   Created    2m41s                  kubelet            Created container: clean-cilium-state
  Normal   Started    2m41s                  kubelet            Started container clean-cilium-state
  Normal   Pulled     2m40s                  kubelet            Container image "rancher/mirrored-cilium-cilium:v1.18.1" already present on machine
  Normal   Created    2m40s                  kubelet            Created container: install-cni-binaries
  Normal   Started    2m40s                  kubelet            Started container install-cni-binaries
  Normal   Pulled     2m39s                  kubelet            Container image "rancher/mirrored-cilium-cilium:v1.18.1" already present on machine
  Normal   Created    2m39s                  kubelet            Created container: cilium-agent
  Normal   Started    2m39s                  kubelet            Started container cilium-agent
  Warning  Unhealthy  106s (x25 over 2m34s)  kubelet            Startup probe failed: Get "http://127.0.0.1:9879/healthz": dial tcp 127.0.0.1:9879: connect: connection refused

2025-10-25 08:41:53,559 p=3160268 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
time=2025-10-25T01:38:48.713746983Z level=info msg="Memory available for map entries (0.250% of 4055957504B): 10139893B"
time=2025-10-25T01:38:48.713960402Z level=info msg="option bpf-ct-global-tcp-max set by dynamic sizing to 131072"
time=2025-10-25T01:38:48.713973667Z level=info msg="option bpf-ct-global-any-max set by dynamic sizing to 65536"
time=2025-10-25T01:38:48.71398126Z level=info msg="option bpf-nat-global-max set by dynamic sizing to 131072"
time=2025-10-25T01:38:48.713988695Z level=info msg="option bpf-neigh-global-max set by dynamic sizing to 131072"
time=2025-10-25T01:38:48.722063616Z level=info msg="  --agent-health-port='9879'"
time=2025-10-25T01:38:48.722433136Z level=info msg="  --agent-health-require-k8s-connectivity='true'"
time=2025-10-25T01:38:48.722542198Z level=info msg="  --agent-labels=''"
time=2025-10-25T01:38:48.722961256Z level=info msg="  --agent-liveness-update-interval='1s'"
time=2025-10-25T01:38:48.722987771Z level=info msg="  --agent-not-ready-taint-key='node.cilium.io/agent-not-ready'"
time=2025-10-25T01:38:48.723237655Z level=info msg="  --allocator-list-timeout='3m0s'"
time=2025-10-25T01:38:48.723314773Z level=info msg="  --allow-icmp-frag-needed='true'"
time=2025-10-25T01:38:48.723336035Z level=info msg="  --allow-localhost='auto'"
time=2025-10-25T01:38:48.723622033Z level=info msg="  --allow-unsafe-policy-skb-usage='false'"
time=2025-10-25T01:38:48.723690668Z level=info msg="  --annotate-k8s-node='false'"
time=2025-10-25T01:38:48.723758488Z level=info msg="  --api-rate-limit=''"
time=2025-10-25T01:38:48.723781555Z level=info msg="  --auto-create-cilium-node-resource='true'"
time=2025-10-25T01:38:48.723892117Z level=info msg="  --auto-direct-node-routes='false'"
time=2025-10-25T01:38:48.723923957Z level=info msg="  --bgp-router-id-allocation-ip-pool=''"
time=2025-10-25T01:38:48.723940201Z level=info msg="  --bgp-router-id-allocation-mode='default'"
time=2025-10-25T01:38:48.723997604Z level=info msg="  --boot-id-file='/proc/sys/kernel/random/boot_id'"
time=2025-10-25T01:38:48.724021693Z level=info msg="  --bpf-auth-map-max='524288'"
time=2025-10-25T01:38:48.724078761Z level=info msg="  --bpf-conntrack-accounting='false'"
time=2025-10-25T01:38:48.724152355Z level=info msg="  --bpf-ct-global-any-max='262144'"
time=2025-10-25T01:38:48.724173781Z level=info msg="  --bpf-ct-global-tcp-max='524288'"
time=2025-10-25T01:38:48.724231879Z level=info msg="  --bpf-ct-timeout-regular-any='1m0s'"
time=2025-10-25T01:38:48.724252155Z level=info msg="  --bpf-ct-timeout-regular-tcp='2h13m20s'"
time=2025-10-25T01:38:48.724267892Z level=info msg="  --bpf-ct-timeout-regular-tcp-fin='10s'"
time=2025-10-25T01:38:48.724282355Z level=info msg="  --bpf-ct-timeout-regular-tcp-syn='1m0s'"
time=2025-10-25T01:38:48.724296585Z level=info msg="  --bpf-ct-timeout-service-any='1m0s'"
time=2025-10-25T01:38:48.724311305Z level=info msg="  --bpf-ct-timeout-service-tcp='2h13m20s'"
time=2025-10-25T01:38:48.724325662Z level=info msg="  --bpf-ct-timeout-service-tcp-grace='1m0s'"
time=2025-10-25T01:38:48.724340647Z level=info msg="  --bpf-distributed-lru='false'"
time=2025-10-25T01:38:48.72435671Z level=info msg="  --bpf-events-default-burst-limit='0'"
time=2025-10-25T01:38:48.724371722Z level=info msg="  --bpf-events-default-rate-limit='0'"
time=2025-10-25T01:38:48.724386349Z level=info msg="  --bpf-events-drop-enabled='true'"
time=2025-10-25T01:38:48.724401464Z level=info msg="  --bpf-events-policy-verdict-enabled='true'"
time=2025-10-25T01:38:48.724457727Z level=info msg="  --bpf-events-trace-enabled='true'"
time=2025-10-25T01:38:48.724478966Z level=info msg="  --bpf-filter-priority='1'"
time=2025-10-25T01:38:48.724494672Z level=info msg="  --bpf-fragments-map-max='8192'"
time=2025-10-25T01:38:48.724509096Z level=info msg="  --bpf-lb-acceleration='disabled'"
time=2025-10-25T01:38:48.724524615Z level=info msg="  --bpf-lb-affinity-map-max='0'"
time=2025-10-25T01:38:48.724539977Z level=info msg="  --bpf-lb-algorithm='random'"
time=2025-10-25T01:38:48.724597313Z level=info msg="  --bpf-lb-algorithm-annotation='false'"
time=2025-10-25T01:38:48.724617887Z level=info msg="  --bpf-lb-dsr-dispatch='opt'"
time=2025-10-25T01:38:48.724632457Z level=info msg="  --bpf-lb-external-clusterip='false'"
time=2025-10-25T01:38:48.724688667Z level=info msg="  --bpf-lb-ipip-sock-mark='false'"
time=2025-10-25T01:38:48.724708921Z level=info msg="  --bpf-lb-maglev-hash-seed='JLfvgnHc2kaSUFaI'"
time=2025-10-25T01:38:48.724725299Z level=info msg="  --bpf-lb-maglev-map-max='0'"
time=2025-10-25T01:38:48.724740444Z level=info msg="  --bpf-lb-maglev-table-size='16381'"
time=2025-10-25T01:38:48.724755148Z level=info msg="  --bpf-lb-map-max='65536'"
time=2025-10-25T01:38:48.724769066Z level=info msg="  --bpf-lb-mode='snat'"
time=2025-10-25T01:38:48.724783125Z level=info msg="  --bpf-lb-mode-annotation='false'"
time=2025-10-25T01:38:48.724797696Z level=info msg="  --bpf-lb-nat46x64='false'"
time=2025-10-25T01:38:48.724811534Z level=info msg="  --bpf-lb-proto-diff='true'"
time=2025-10-25T01:38:48.724825854Z level=info msg="  --bpf-lb-rev-nat-map-max='0'"
time=2025-10-25T01:38:48.724844591Z level=info msg="  --bpf-lb-rss-ipv4-src-cidr=''"
time=2025-10-25T01:38:48.724862131Z level=info msg="  --bpf-lb-rss-ipv6-src-cidr=''"
time=2025-10-25T01:38:48.724877667Z level=info msg="  --bpf-lb-service-backend-map-max='0'"
time=2025-10-25T01:38:48.72489181Z level=info msg="  --bpf-lb-service-map-max='0'"
time=2025-10-25T01:38:48.724905575Z level=info msg="  --bpf-lb-sock='false'"
time=2025-10-25T01:38:48.724919524Z level=info msg="  --bpf-lb-sock-hostns-only='false'"
time=2025-10-25T01:38:48.724935143Z level=info msg="  --bpf-lb-sock-terminate-pod-connections='true'"
time=2025-10-25T01:38:48.725015994Z level=info msg="  --bpf-lb-source-range-all-types='false'"
time=2025-10-25T01:38:48.72507946Z level=info msg="  --bpf-lb-source-range-map-max='0'"
time=2025-10-25T01:38:48.725099821Z level=info msg="  --bpf-map-dynamic-size-ratio='0.0025'"
time=2025-10-25T01:38:48.725119432Z level=info msg="  --bpf-map-event-buffers=''"
time=2025-10-25T01:38:48.725178593Z level=info msg="  --bpf-nat-global-max='524288'"
time=2025-10-25T01:38:48.725198869Z level=info msg="  --bpf-neigh-global-max='524288'"
time=2025-10-25T01:38:48.725255647Z level=info msg="  --bpf-node-map-max='16384'"
time=2025-10-25T01:38:48.725276829Z level=info msg="  --bpf-policy-map-full-reconciliation-interval='15m0s'"
time=2025-10-25T01:38:48.725292389Z level=info msg="  --bpf-policy-map-max='16384'"
time=2025-10-25T01:38:48.725354135Z level=info msg="  --bpf-policy-stats-map-max='65536'"
time=2025-10-25T01:38:48.725374208Z level=info msg="  --bpf-root='/sys/fs/bpf'"
time=2025-10-25T01:38:48.725430675Z level=info msg="  --bpf-sock-rev-map-max='0'"
time=2025-10-25T01:38:48.725451804Z level=info msg="  --bypass-ip-availability-upon-restore='false'"
time=2025-10-25T01:38:48.725467781Z level=info msg="  --certificates-directory='/var/run/cilium/certs'"
time=2025-10-25T01:38:48.725504385Z level=info msg="  --cflags=''"
time=2025-10-25T01:38:48.725524141Z level=info msg="  --cgroup-root='/run/cilium/cgroupv2'"
time=2025-10-25T01:38:48.725539125Z level=info msg="  --cilium-endpoint-gc-interval='5m0s'"
time=2025-10-25T01:38:48.725555153Z level=info msg="  --cluster-health-port='4240'"
time=2025-10-25T01:38:48.725614279Z level=info msg="  --cluster-id='0'"
time=2025-10-25T01:38:48.725634021Z level=info msg="  --cluster-name='default'"
time=2025-10-25T01:38:48.725649217Z level=info msg="  --clustermesh-config='/var/lib/cilium/clustermesh/'"
time=2025-10-25T01:38:48.725704344Z level=info msg="  --clustermesh-enable-endpoint-sync='false'"
time=2025-10-25T01:38:48.725723344Z level=info msg="  --clustermesh-enable-mcs-api='false'"
time=2025-10-25T01:38:48.725739863Z level=info msg="  --clustermesh-sync-timeout='1m0s'"
time=2025-10-25T01:38:48.725758456Z level=info msg="  --cmdref=''"
time=2025-10-25T01:38:48.72577317Z level=info msg="  --cni-chaining-mode='portmap'"
time=2025-10-25T01:38:48.72579051Z level=info msg="  --cni-chaining-target=''"
time=2025-10-25T01:38:48.725805481Z level=info msg="  --cni-exclusive='true'"
time=2025-10-25T01:38:48.725820592Z level=info msg="  --cni-external-routing='false'"
time=2025-10-25T01:38:48.725834705Z level=info msg="  --cni-log-file='/var/run/cilium/cilium-cni.log'"
time=2025-10-25T01:38:48.725851327Z level=info msg="  --config=''"
time=2025-10-25T01:38:48.725934258Z level=info msg="  --config-dir='/tmp/cilium/config-map'"
time=2025-10-25T01:38:48.725956834Z level=info msg="  --config-sources='[{\"kind\":\"config-map\",\"namespace\":\"kube-system\",\"name\":\"cilium-config\"}]'"
time=2025-10-25T01:38:48.725976235Z level=info msg="  --config-sources-overrides='{\"allowConfigKeys\":[],\"denyConfigKeys\":[]}'"
time=2025-10-25T01:38:48.725998363Z level=info msg="  --connectivity-probe-frequency-ratio='0.5'"
time=2025-10-25T01:38:48.72601441Z level=info msg="  --conntrack-gc-interval='0s'"
time=2025-10-25T01:38:48.726028857Z level=info msg="  --conntrack-gc-max-interval='0s'"
time=2025-10-25T01:38:48.726044246Z level=info msg="  --container-ip-local-reserved-ports='auto'"
time=2025-10-25T01:38:48.726078047Z level=info msg="  --controller-group-metrics=''"
time=2025-10-25T01:38:48.726139907Z level=info msg="  --crd-wait-timeout='5m0s'"
time=2025-10-25T01:38:48.726159903Z level=info msg="  --custom-cni-conf='false'"
time=2025-10-25T01:38:48.726221449Z level=info msg="  --datapath-mode='veth'"
time=2025-10-25T01:38:48.726240356Z level=info msg="  --debug='false'"
time=2025-10-25T01:38:48.72625831Z level=info msg="  --debug-verbose=''"
time=2025-10-25T01:38:48.726272319Z level=info msg="  --default-lb-service-ipam='lbipam'"
time=2025-10-25T01:38:48.726290271Z level=info msg="  --derive-masq-ip-addr-from-device=''"
time=2025-10-25T01:38:48.726399126Z level=info msg="  --devices=''"
time=2025-10-25T01:38:48.72647314Z level=info msg="  --direct-routing-device=''"
time=2025-10-25T01:38:48.726496173Z level=info msg="  --direct-routing-skip-unreachable='false'"
time=2025-10-25T01:38:48.726520319Z level=info msg="  --disable-endpoint-crd='false'"
time=2025-10-25T01:38:48.726535494Z level=info msg="  --disable-envoy-version-check='false'"
time=2025-10-25T01:38:48.726592686Z level=info msg="  --disable-external-ip-mitigation='false'"
time=2025-10-25T01:38:48.72667226Z level=info msg="  --disable-iptables-feeder-rules=''"
time=2025-10-25T01:38:48.726696349Z level=info msg="  --dns-max-ips-per-restored-rule='1000'"
time=2025-10-25T01:38:48.726711812Z level=info msg="  --dns-policy-unload-on-shutdown='false'"
time=2025-10-25T01:38:48.726728807Z level=info msg="  --dnsproxy-concurrency-limit='0'"
time=2025-10-25T01:38:48.726788966Z level=info msg="  --dnsproxy-concurrency-processing-grace-period='0s'"
time=2025-10-25T01:38:48.726810586Z level=info msg="  --dnsproxy-enable-transparent-mode='false'"
time=2025-10-25T01:38:48.72689272Z level=info msg="  --dnsproxy-insecure-skip-transparent-mode-check='false'"
time=2025-10-25T01:38:48.726956948Z level=info msg="  --dnsproxy-lock-count='131'"
time=2025-10-25T01:38:48.726984104Z level=info msg="  --dnsproxy-lock-timeout='500ms'"
time=2025-10-25T01:38:48.726999343Z level=info msg="  --dnsproxy-socket-linger-timeout='10'"
time=2025-10-25T01:38:48.727020181Z level=info msg="  --dynamic-lifecycle-config='[]'"
time=2025-10-25T01:38:48.727036361Z level=info msg="  --egress-gateway-policy-map-max='16384'"
time=2025-10-25T01:38:48.727051881Z level=info msg="  --egress-gateway-reconciliation-trigger-interval='1s'"
time=2025-10-25T01:38:48.727122855Z level=info msg="  --egress-masquerade-interfaces=''"
time=2025-10-25T01:38:48.72714399Z level=info msg="  --egress-multi-home-ip-rule-compat='false'"
time=2025-10-25T01:38:48.727159827Z level=info msg="  --enable-active-connection-tracking='false'"
time=2025-10-25T01:38:48.727174985Z level=info msg="  --enable-auto-protect-node-port-range='true'"
time=2025-10-25T01:38:48.72718998Z level=info msg="  --enable-bandwidth-manager='false'"
time=2025-10-25T01:38:48.727204707Z level=info msg="  --enable-bbr='false'"
time=2025-10-25T01:38:48.727261522Z level=info msg="  --enable-bbr-hostns-only='false'"
time=2025-10-25T01:38:48.727322182Z level=info msg="  --enable-bgp-control-plane='false'"
time=2025-10-25T01:38:48.727382762Z level=info msg="  --enable-bgp-control-plane-status-report='true'"
time=2025-10-25T01:38:48.727402948Z level=info msg="  --enable-bpf-clock-probe='false'"
time=2025-10-25T01:38:48.727417806Z level=info msg="  --enable-bpf-masquerade='false'"
time=2025-10-25T01:38:48.72747309Z level=info msg="  --enable-bpf-tproxy='false'"
time=2025-10-25T01:38:48.7275074Z level=info msg="  --enable-cilium-api-server-access='*'"
time=2025-10-25T01:38:48.72752773Z level=info msg="  --enable-cilium-clusterwide-network-policy='true'"
time=2025-10-25T01:38:48.727543029Z level=info msg="  --enable-cilium-endpoint-slice='false'"
time=2025-10-25T01:38:48.72761127Z level=info msg="  --enable-cilium-health-api-server-access='*'"
time=2025-10-25T01:38:48.727675191Z level=info msg="  --enable-cilium-network-policy='true'"
time=2025-10-25T01:38:48.727700923Z level=info msg="  --enable-custom-calls='false'"
time=2025-10-25T01:38:48.727716636Z level=info msg="  --enable-drift-checker='true'"
time=2025-10-25T01:38:48.727731117Z level=info msg="  --enable-dynamic-config='true'"
time=2025-10-25T01:38:48.727746499Z level=info msg="  --enable-dynamic-lifecycle-manager='false'"
time=2025-10-25T01:38:48.727761714Z level=info msg="  --enable-egress-gateway='false'"
time=2025-10-25T01:38:48.727894263Z level=info msg="  --enable-encryption-strict-mode='false'"
time=2025-10-25T01:38:48.727981011Z level=info msg="  --enable-endpoint-health-checking='true'"
time=2025-10-25T01:38:48.728002587Z level=info msg="  --enable-endpoint-lockdown-on-policy-overflow='false'"
time=2025-10-25T01:38:48.728018354Z level=info msg="  --enable-endpoint-routes='false'"
time=2025-10-25T01:38:48.728033331Z level=info msg="  --enable-envoy-config='false'"
time=2025-10-25T01:38:48.728047711Z level=info msg="  --enable-external-ips='false'"
time=2025-10-25T01:38:48.728062091Z level=info msg="  --enable-gateway-api='false'"
time=2025-10-25T01:38:48.728118605Z level=info msg="  --enable-gops='true'"
time=2025-10-25T01:38:48.728183769Z level=info msg="  --enable-health-check-loadbalancer-ip='false'"
time=2025-10-25T01:38:48.728203702Z level=info msg="  --enable-health-check-nodeport='true'"
time=2025-10-25T01:38:48.728218463Z level=info msg="  --enable-health-checking='true'"
time=2025-10-25T01:38:48.72827358Z level=info msg="  --enable-host-firewall='false'"
time=2025-10-25T01:38:48.728336355Z level=info msg="  --enable-host-legacy-routing='true'"
time=2025-10-25T01:38:48.72839754Z level=info msg="  --enable-host-port='false'"
time=2025-10-25T01:38:48.72842373Z level=info msg="  --enable-hubble='false'"
time=2025-10-25T01:38:48.728439343Z level=info msg="  --enable-hubble-open-metrics='false'"
time=2025-10-25T01:38:48.728456218Z level=info msg="  --enable-hubble-recorder-api='true'"
time=2025-10-25T01:38:48.728470842Z level=info msg="  --enable-icmp-rules='true'"
time=2025-10-25T01:38:48.728485252Z level=info msg="  --enable-identity-mark='true'"
time=2025-10-25T01:38:48.728499605Z level=info msg="  --enable-ingress-controller='false'"
time=2025-10-25T01:38:48.728554195Z level=info msg="  --enable-internal-traffic-policy='true'"
time=2025-10-25T01:38:48.728574445Z level=info msg="  --enable-ip-masq-agent='false'"
time=2025-10-25T01:38:48.728589436Z level=info msg="  --enable-ipip-termination='false'"
time=2025-10-25T01:38:48.728644457Z level=info msg="  --enable-ipsec='false'"
time=2025-10-25T01:38:48.728664279Z level=info msg="  --enable-ipsec-encrypted-overlay='false'"
time=2025-10-25T01:38:48.728681519Z level=info msg="  --enable-ipsec-key-watcher='true'"
time=2025-10-25T01:38:48.728696193Z level=info msg="  --enable-ipsec-xfrm-state-caching='true'"
time=2025-10-25T01:38:48.728751554Z level=info msg="  --enable-ipv4='true'"
time=2025-10-25T01:38:48.728770922Z level=info msg="  --enable-ipv4-big-tcp='false'"
time=2025-10-25T01:38:48.72888383Z level=info msg="  --enable-ipv4-egress-gateway='false'"
time=2025-10-25T01:38:48.728950009Z level=info msg="  --enable-ipv4-fragment-tracking='true'"
time=2025-10-25T01:38:48.728970487Z level=info msg="  --enable-ipv4-masquerade='true'"
time=2025-10-25T01:38:48.72898476Z level=info msg="  --enable-ipv6='false'"
time=2025-10-25T01:38:48.728998909Z level=info msg="  --enable-ipv6-big-tcp='false'"
time=2025-10-25T01:38:48.729013209Z level=info msg="  --enable-ipv6-fragment-tracking='true'"
time=2025-10-25T01:38:48.729069288Z level=info msg="  --enable-ipv6-masquerade='true'"
time=2025-10-25T01:38:48.729096464Z level=info msg="  --enable-ipv6-ndp='false'"
time=2025-10-25T01:38:48.729111937Z level=info msg="  --enable-k8s='true'"
time=2025-10-25T01:38:48.729131295Z level=info msg="  --enable-k8s-api-discovery='false'"
time=2025-10-25T01:38:48.72914656Z level=info msg="  --enable-k8s-endpoint-slice='true'"
time=2025-10-25T01:38:48.729160559Z level=info msg="  --enable-k8s-networkpolicy='true'"
time=2025-10-25T01:38:48.729175263Z level=info msg="  --enable-l2-announcements='false'"
time=2025-10-25T01:38:48.729190308Z level=info msg="  --enable-l2-neigh-discovery='false'"
time=2025-10-25T01:38:48.729205429Z level=info msg="  --enable-l2-pod-announcements='false'"
time=2025-10-25T01:38:48.729219689Z level=info msg="  --enable-l7-proxy='true'"
time=2025-10-25T01:38:48.729276253Z level=info msg="  --enable-lb-ipam='true'"
time=2025-10-25T01:38:48.729296984Z level=info msg="  --enable-local-node-route='true'"
time=2025-10-25T01:38:48.729353064Z level=info msg="  --enable-local-redirect-policy='false'"
time=2025-10-25T01:38:48.729413453Z level=info msg="  --enable-masquerade-to-route-source='false'"
time=2025-10-25T01:38:48.72947263Z level=info msg="  --enable-metrics='true'"
time=2025-10-25T01:38:48.729493532Z level=info msg="  --enable-mke='false'"
time=2025-10-25T01:38:48.729508386Z level=info msg="  --enable-monitor='true'"
time=2025-10-25T01:38:48.729563904Z level=info msg="  --enable-nat46x64-gateway='false'"
time=2025-10-25T01:38:48.729630906Z level=info msg="  --enable-node-port='false'"
time=2025-10-25T01:38:48.729657063Z level=info msg="  --enable-node-selector-labels='false'"
time=2025-10-25T01:38:48.729672722Z level=info msg="  --enable-non-default-deny-policies='true'"
time=2025-10-25T01:38:48.729687443Z level=info msg="  --enable-pmtu-discovery='false'"
time=2025-10-25T01:38:48.729701649Z level=info msg="  --enable-policy='default'"
time=2025-10-25T01:38:48.729715972Z level=info msg="  --enable-policy-secrets-sync='true'"
time=2025-10-25T01:38:48.729770943Z level=info msg="  --enable-recorder='false'"
time=2025-10-25T01:38:48.729793518Z level=info msg="  --enable-route-mtu-for-cni-chaining='false'"
time=2025-10-25T01:38:48.72980872Z level=info msg="  --enable-sctp='false'"
time=2025-10-25T01:38:48.729891508Z level=info msg="  --enable-service-topology='false'"
time=2025-10-25T01:38:48.72999358Z level=info msg="  --enable-session-affinity='true'"
time=2025-10-25T01:38:48.730018839Z level=info msg="  --enable-source-ip-verification='true'"
time=2025-10-25T01:38:48.730035557Z level=info msg="  --enable-srv6='false'"
time=2025-10-25T01:38:48.730095175Z level=info msg="  --enable-stale-cilium-endpoint-cleanup='true'"
time=2025-10-25T01:38:48.730121947Z level=info msg="  --enable-standalone-dns-proxy='false'"
time=2025-10-25T01:38:48.730137453Z level=info msg="  --enable-svc-source-range-check='true'"
time=2025-10-25T01:38:48.730152113Z level=info msg="  --enable-tcx='true'"
time=2025-10-25T01:38:48.730166233Z level=info msg="  --enable-tracing='false'"
time=2025-10-25T01:38:48.730180746Z level=info msg="  --enable-unreachable-routes='false'"
time=2025-10-25T01:38:48.730235666Z level=info msg="  --enable-vtep='false'"
time=2025-10-25T01:38:48.730295548Z level=info msg="  --enable-well-known-identities='false'"
time=2025-10-25T01:38:48.730356509Z level=info msg="  --enable-wireguard='false'"
time=2025-10-25T01:38:48.730377822Z level=info msg="  --enable-xdp-prefilter='false'"
time=2025-10-25T01:38:48.73039302Z level=info msg="  --enable-xt-socket-fallback='true'"
time=2025-10-25T01:38:48.730445177Z level=info msg="  --encrypt-interface=''"
time=2025-10-25T01:38:48.73045587Z level=info msg="  --encrypt-node='false'"
time=2025-10-25T01:38:48.730463628Z level=info msg="  --encryption-strict-mode-allow-remote-node-identities='false'"
time=2025-10-25T01:38:48.730473168Z level=info msg="  --encryption-strict-mode-cidr=''"
time=2025-10-25T01:38:48.730481376Z level=info msg="  --endpoint-bpf-prog-watchdog-interval='30s'"
time=2025-10-25T01:38:48.730489297Z level=info msg="  --endpoint-gc-interval='5m0s'"
time=2025-10-25T01:38:48.730498918Z level=info msg="  --endpoint-queue-size='25'"
time=2025-10-25T01:38:48.730506429Z level=info msg="  --endpoint-regen-interval='2m0s'"
time=2025-10-25T01:38:48.730514192Z level=info msg="  --envoy-access-log-buffer-size='4096'"
time=2025-10-25T01:38:48.730521415Z level=info msg="  --envoy-base-id='0'"
time=2025-10-25T01:38:48.730528927Z level=info msg="  --envoy-config-retry-interval='15s'"
time=2025-10-25T01:38:48.73053649Z level=info msg="  --envoy-config-timeout='2m0s'"
time=2025-10-25T01:38:48.730545392Z level=info msg="  --envoy-default-log-level=''"
time=2025-10-25T01:38:48.730553903Z level=info msg="  --envoy-http-upstream-linger-timeout='-1'"
time=2025-10-25T01:38:48.730561163Z level=info msg="  --envoy-keep-cap-netbindservice='false'"
time=2025-10-25T01:38:48.730569959Z level=info msg="  --envoy-log=''"
time=2025-10-25T01:38:48.730577449Z level=info msg="  --envoy-policy-restore-timeout='3m0s'"
time=2025-10-25T01:38:48.730586094Z level=info msg="  --envoy-secrets-namespace=''"
time=2025-10-25T01:38:48.730604231Z level=info msg="  --exclude-local-address=''"
time=2025-10-25T01:38:48.730645014Z level=info msg="  --exclude-node-label-patterns=''"
time=2025-10-25T01:38:48.73065691Z level=info msg="  --external-envoy-proxy='false'"
time=2025-10-25T01:38:48.730666958Z level=info msg="  --fixed-identity-mapping=''"
time=2025-10-25T01:38:48.73067489Z level=info msg="  --force-device-detection='false'"
time=2025-10-25T01:38:48.730683565Z level=info msg="  --fqdn-regex-compile-lru-size='1024'"
time=2025-10-25T01:38:48.730693638Z level=info msg="  --gateway-api-secrets-namespace=''"
time=2025-10-25T01:38:48.730701802Z level=info msg="  --gops-port='9890'"
time=2025-10-25T01:38:48.730709187Z level=info msg="  --health-check-icmp-failure-threshold='3'"
time=2025-10-25T01:38:48.730717024Z level=info msg="  --http-idle-timeout='0'"
time=2025-10-25T01:38:48.730724153Z level=info msg="  --http-max-grpc-timeout='0'"
time=2025-10-25T01:38:48.730731646Z level=info msg="  --http-normalize-path='true'"
time=2025-10-25T01:38:48.730738902Z level=info msg="  --http-request-timeout='3600'"
time=2025-10-25T01:38:48.730746089Z level=info msg="  --http-retry-count='3'"
time=2025-10-25T01:38:48.730753177Z level=info msg="  --http-retry-timeout='0'"
time=2025-10-25T01:38:48.730760367Z level=info msg="  --http-stream-idle-timeout='300'"
time=2025-10-25T01:38:48.730768085Z level=info msg="  --hubble-disable-tls='true'"
time=2025-10-25T01:38:48.730775614Z level=info msg="  --hubble-drop-events='false'"
time=2025-10-25T01:38:48.730783058Z level=info msg="  --hubble-drop-events-interval='2m0s'"
time=2025-10-25T01:38:48.730796983Z level=info msg="  --hubble-drop-events-reasons='auth_required,policy_denied'"
time=2025-10-25T01:38:48.730865167Z level=info msg="  --hubble-dynamic-metrics-config-path=''"
time=2025-10-25T01:38:48.730902071Z level=info msg="  --hubble-event-buffer-capacity='4095'"
time=2025-10-25T01:38:48.730913193Z level=info msg="  --hubble-event-queue-size='0'"
time=2025-10-25T01:38:48.730923582Z level=info msg="  --hubble-export-allowlist=''"
time=2025-10-25T01:38:48.730931963Z level=info msg="  --hubble-export-denylist=''"
time=2025-10-25T01:38:48.730949457Z level=info msg="  --hubble-export-fieldmask=''"
time=2025-10-25T01:38:48.730962359Z level=info msg="  --hubble-export-file-compress='false'"
time=2025-10-25T01:38:48.730971091Z level=info msg="  --hubble-export-file-max-backups='5'"
time=2025-10-25T01:38:48.730978596Z level=info msg="  --hubble-export-file-max-size-mb='10'"
time=2025-10-25T01:38:48.730987113Z level=info msg="  --hubble-export-file-path=''"
time=2025-10-25T01:38:48.730995674Z level=info msg="  --hubble-flowlogs-config-path=''"
time=2025-10-25T01:38:48.731004139Z level=info msg="  --hubble-listen-address=''"
time=2025-10-25T01:38:48.731013408Z level=info msg="  --hubble-metrics=''"
time=2025-10-25T01:38:48.73102218Z level=info msg="  --hubble-metrics-server=''"
time=2025-10-25T01:38:48.731030099Z level=info msg="  --hubble-metrics-server-enable-tls='false'"
time=2025-10-25T01:38:48.731039119Z level=info msg="  --hubble-metrics-server-tls-cert-file=''"
time=2025-10-25T01:38:48.731063537Z level=info msg="  --hubble-metrics-server-tls-client-ca-files=''"
time=2025-10-25T01:38:48.731100738Z level=info msg="  --hubble-metrics-server-tls-key-file=''"
time=2025-10-25T01:38:48.731121926Z level=info msg="  --hubble-monitor-events=''"
time=2025-10-25T01:38:48.731131287Z level=info msg="  --hubble-network-policy-correlation-enabled='true'"
time=2025-10-25T01:38:48.731139129Z level=info msg="  --hubble-prefer-ipv6='false'"
time=2025-10-25T01:38:48.731147734Z level=info msg="  --hubble-recorder-sink-queue-size='1024'"
time=2025-10-25T01:38:48.731155804Z level=info msg="  --hubble-recorder-storage-path='/var/run/cilium/pcaps'"
time=2025-10-25T01:38:48.731164185Z level=info msg="  --hubble-redact-enabled='false'"
time=2025-10-25T01:38:48.731177698Z level=info msg="  --hubble-redact-http-headers-allow=''"
time=2025-10-25T01:38:48.731199585Z level=info msg="  --hubble-redact-http-headers-deny=''"
time=2025-10-25T01:38:48.731208384Z level=info msg="  --hubble-redact-http-urlquery='false'"
time=2025-10-25T01:38:48.731217045Z level=info msg="  --hubble-redact-http-userinfo='true'"
time=2025-10-25T01:38:48.731224732Z level=info msg="  --hubble-redact-kafka-apikey='false'"
time=2025-10-25T01:38:48.731232089Z level=info msg="  --hubble-skip-unknown-cgroup-ids='true'"
time=2025-10-25T01:38:48.731239694Z level=info msg="  --hubble-socket-path='/var/run/cilium/hubble.sock'"
time=2025-10-25T01:38:48.731248608Z level=info msg="  --hubble-tls-cert-file=''"
time=2025-10-25T01:38:48.731261737Z level=info msg="  --hubble-tls-client-ca-files=''"
time=2025-10-25T01:38:48.731295789Z level=info msg="  --hubble-tls-key-file=''"
time=2025-10-25T01:38:48.731307119Z level=info msg="  --identity-allocation-mode='crd'"
time=2025-10-25T01:38:48.731315396Z level=info msg="  --identity-allocation-sync-interval='5m0s'"
time=2025-10-25T01:38:48.731323109Z level=info msg="  --identity-allocation-timeout='2m0s'"
time=2025-10-25T01:38:48.731330653Z level=info msg="  --identity-change-grace-period='5s'"
time=2025-10-25T01:38:48.731339062Z level=info msg="  --identity-gc-interval='15m0s'"
time=2025-10-25T01:38:48.731346827Z level=info msg="  --identity-heartbeat-timeout='30m0s'"
time=2025-10-25T01:38:48.731354045Z level=info msg="  --identity-management-mode='agent'"
time=2025-10-25T01:38:48.731361726Z level=info msg="  --identity-max-jitter='30s'"
time=2025-10-25T01:38:48.731371415Z level=info msg="  --identity-restore-grace-period='30s'"
time=2025-10-25T01:38:48.731387319Z level=info msg="  --ignore-flags-drift-checker=''"
time=2025-10-25T01:38:48.73142155Z level=info msg="  --ingress-secrets-namespace=''"
time=2025-10-25T01:38:48.731432913Z level=info msg="  --install-iptables-rules='true'"
time=2025-10-25T01:38:48.731440821Z level=info msg="  --install-no-conntrack-iptables-rules='false'"
time=2025-10-25T01:38:48.731450264Z level=info msg="  --install-uplink-routes-for-delegated-ipam='false'"
time=2025-10-25T01:38:48.73145781Z level=info msg="  --ip-masq-agent-config-path='/etc/config/ip-masq-agent'"
time=2025-10-25T01:38:48.731465548Z level=info msg="  --ipam='kubernetes'"
time=2025-10-25T01:38:48.731472658Z level=info msg="  --ipam-cilium-node-update-rate='15s'"
time=2025-10-25T01:38:48.731480242Z level=info msg="  --ipam-default-ip-pool='default'"
time=2025-10-25T01:38:48.731489963Z level=info msg="  --ipam-multi-pool-pre-allocation=''"
time=2025-10-25T01:38:48.731499082Z level=info msg="  --ipsec-key-file=''"
time=2025-10-25T01:38:48.731506671Z level=info msg="  --ipsec-key-rotation-duration='5m0s'"
time=2025-10-25T01:38:48.731514463Z level=info msg="  --iptables-lock-timeout='5s'"
time=2025-10-25T01:38:48.731521776Z level=info msg="  --iptables-random-fully='false'"
time=2025-10-25T01:38:48.731530782Z level=info msg="  --ipv4-native-routing-cidr=''"
time=2025-10-25T01:38:48.731545117Z level=info msg="  --ipv4-node='auto'"
time=2025-10-25T01:38:48.731559642Z level=info msg="  --ipv4-pod-subnets=''"
time=2025-10-25T01:38:48.731567721Z level=info msg="  --ipv4-range='auto'"
time=2025-10-25T01:38:48.731575063Z level=info msg="  --ipv4-service-loopback-address='169.254.42.1'"
time=2025-10-25T01:38:48.731582553Z level=info msg="  --ipv4-service-range='auto'"
time=2025-10-25T01:38:48.731590012Z level=info msg="  --ipv6-cluster-alloc-cidr='f00d::/64'"
time=2025-10-25T01:38:48.731598448Z level=info msg="  --ipv6-mcast-device=''"
time=2025-10-25T01:38:48.731606856Z level=info msg="  --ipv6-native-routing-cidr=''"
time=2025-10-25T01:38:48.731613909Z level=info msg="  --ipv6-node='auto'"
time=2025-10-25T01:38:48.731626734Z level=info msg="  --ipv6-pod-subnets=''"
time=2025-10-25T01:38:48.731635863Z level=info msg="  --ipv6-range='auto'"
time=2025-10-25T01:38:48.731642911Z level=info msg="  --ipv6-service-range='auto'"
time=2025-10-25T01:38:48.731651755Z level=info msg="  --k8s-api-server=''"
time=2025-10-25T01:38:48.731664389Z level=info msg="  --k8s-api-server-urls=''"
time=2025-10-25T01:38:48.731697163Z level=info msg="  --k8s-client-burst='20'"
time=2025-10-25T01:38:48.731708676Z level=info msg="  --k8s-client-connection-keep-alive='30s'"
time=2025-10-25T01:38:48.731716302Z level=info msg="  --k8s-client-connection-timeout='30s'"
time=2025-10-25T01:38:48.731724949Z level=info msg="  --k8s-client-qps='10'"
time=2025-10-25T01:38:48.731732211Z level=info msg="  --k8s-heartbeat-timeout='30s'"
time=2025-10-25T01:38:48.731741622Z level=info msg="  --k8s-kubeconfig-path=''"
time=2025-10-25T01:38:48.731751251Z level=info msg="  --k8s-namespace='kube-system'"
time=2025-10-25T01:38:48.731759098Z level=info msg="  --k8s-require-ipv4-pod-cidr='false'"
time=2025-10-25T01:38:48.731766426Z level=info msg="  --k8s-require-ipv6-pod-cidr='false'"
time=2025-10-25T01:38:48.731775138Z level=info msg="  --k8s-service-proxy-name=''"
time=2025-10-25T01:38:48.731782671Z level=info msg="  --k8s-sync-timeout='3m0s'"
time=2025-10-25T01:38:48.731789974Z level=info msg="  --keep-config='false'"
time=2025-10-25T01:38:48.731797461Z level=info msg="  --kube-proxy-replacement='false'"
time=2025-10-25T01:38:48.731807435Z level=info msg="  --kube-proxy-replacement-healthz-bind-address=''"
time=2025-10-25T01:38:48.731816322Z level=info msg="  --kvstore=''"
time=2025-10-25T01:38:48.731825119Z level=info msg="  --kvstore-lease-ttl='15m0s'"
time=2025-10-25T01:38:48.731832788Z level=info msg="  --kvstore-max-consecutive-quorum-errors='2'"
time=2025-10-25T01:38:48.731854779Z level=info msg="  --kvstore-opt=''"
time=2025-10-25T01:38:48.731902297Z level=info msg="  --l2-announcements-lease-duration='15s'"
time=2025-10-25T01:38:48.731914144Z level=info msg="  --l2-announcements-renew-deadline='5s'"
time=2025-10-25T01:38:48.731921545Z level=info msg="  --l2-announcements-retry-period='2s'"
time=2025-10-25T01:38:48.731931981Z level=info msg="  --l2-pod-announcements-interface=''"
time=2025-10-25T01:38:48.731941414Z level=info msg="  --l2-pod-announcements-interface-pattern=''"
time=2025-10-25T01:38:48.73195034Z level=info msg="  --label-prefix-file=''"
time=2025-10-25T01:38:48.731982011Z level=info msg="  --labels=''"
time=2025-10-25T01:38:48.731992013Z level=info msg="  --lb-init-wait-timeout='1m0s'"
time=2025-10-25T01:38:48.732000083Z level=info msg="  --lb-pressure-metrics-interval='5m0s'"
time=2025-10-25T01:38:48.73200753Z level=info msg="  --lb-retry-backoff-max='1s'"
time=2025-10-25T01:38:48.732014733Z level=info msg="  --lb-retry-backoff-min='1s'"
time=2025-10-25T01:38:48.732029573Z level=info msg="  --lb-state-file=''"
time=2025-10-25T01:38:48.732037142Z level=info msg="  --lb-state-file-interval='1s'"
time=2025-10-25T01:38:48.732045042Z level=info msg="  --lib-dir='/var/lib/cilium'"
time=2025-10-25T01:38:48.732052202Z level=info msg="  --local-max-addr-scope='252'"
time=2025-10-25T01:38:48.732060877Z level=info msg="  --local-router-ipv4=''"
time=2025-10-25T01:38:48.732069184Z level=info msg="  --local-router-ipv6=''"
time=2025-10-25T01:38:48.732092866Z level=info msg="  --log-driver=''"
time=2025-10-25T01:38:48.73210696Z level=info msg="  --log-opt=''"
time=2025-10-25T01:38:48.732115364Z level=info msg="  --log-system-load='false'"
time=2025-10-25T01:38:48.732129196Z level=info msg="  --lrp-address-matcher-cidrs=''"
time=2025-10-25T01:38:48.732174743Z level=info msg="  --max-connected-clusters='255'"
time=2025-10-25T01:38:48.732194355Z level=info msg="  --max-controller-interval='0'"
time=2025-10-25T01:38:48.732203127Z level=info msg="  --max-internal-timer-delay='0s'"
time=2025-10-25T01:38:48.732210641Z level=info msg="  --mesh-auth-enabled='true'"
time=2025-10-25T01:38:48.73221812Z level=info msg="  --mesh-auth-gc-interval='5m0s'"
time=2025-10-25T01:38:48.732226235Z level=info msg="  --mesh-auth-mutual-connect-timeout='5s'"
time=2025-10-25T01:38:48.732234217Z level=info msg="  --mesh-auth-mutual-listener-port='0'"
time=2025-10-25T01:38:48.732241571Z level=info msg="  --mesh-auth-queue-size='1024'"
time=2025-10-25T01:38:48.732249232Z level=info msg="  --mesh-auth-rotated-identities-queue-size='1024'"
time=2025-10-25T01:38:48.732258325Z level=info msg="  --mesh-auth-signal-backoff-duration='1s'"
time=2025-10-25T01:38:48.732266495Z level=info msg="  --mesh-auth-spiffe-trust-domain='spiffe.cilium'"
time=2025-10-25T01:38:48.732276605Z level=info msg="  --mesh-auth-spire-admin-socket=''"
time=2025-10-25T01:38:48.732293407Z level=info msg="  --metrics=''"
time=2025-10-25T01:38:48.732327424Z level=info msg="  --metrics-sampling-interval='5m'"
time=2025-10-25T01:38:48.7323416Z level=info msg="  --mke-cgroup-mount=''"
time=2025-10-25T01:38:48.732349113Z level=info msg="  --monitor-aggregation='medium'"
time=2025-10-25T01:38:48.732356229Z level=info msg="  --monitor-aggregation-flags='all'"
time=2025-10-25T01:38:48.732363454Z level=info msg="  --monitor-aggregation-interval='5s'"
time=2025-10-25T01:38:48.732371489Z level=info msg="  --monitor-queue-size='0'"
time=2025-10-25T01:38:48.732378758Z level=info msg="  --mtu='0'"
time=2025-10-25T01:38:48.732386641Z level=info msg="  --multicast-enabled='false'"
time=2025-10-25T01:38:48.73239398Z level=info msg="  --nat-map-stats-entries='32'"
time=2025-10-25T01:38:48.732400856Z level=info msg="  --nat-map-stats-interval='30s'"
time=2025-10-25T01:38:48.732408634Z level=info msg="  --node-encryption-opt-out-labels='node-role.kubernetes.io/control-plane'"
time=2025-10-25T01:38:48.732424998Z level=info msg="  --node-labels=''"
time=2025-10-25T01:38:48.732432898Z level=info msg="  --node-port-acceleration='disabled'"
time=2025-10-25T01:38:48.732442518Z level=info msg="  --node-port-algorithm=''"
time=2025-10-25T01:38:48.732449683Z level=info msg="  --node-port-bind-protection='true'"
time=2025-10-25T01:38:48.732458153Z level=info msg="  --node-port-mode=''"
time=2025-10-25T01:38:48.732471405Z level=info msg="  --node-port-range='30000,32767'"
time=2025-10-25T01:38:48.732480287Z level=info msg="  --nodeport-addresses=''"
time=2025-10-25T01:38:48.732487182Z level=info msg="  --nodes-gc-interval='5m0s'"
time=2025-10-25T01:38:48.732494191Z level=info msg="  --operator-api-serve-addr='127.0.0.1:9234'"
time=2025-10-25T01:38:48.732501408Z level=info msg="  --operator-prometheus-serve-addr=':9963'"
time=2025-10-25T01:38:48.732509042Z level=info msg="  --policy-accounting='true'"
time=2025-10-25T01:38:48.732516257Z level=info msg="  --policy-audit-mode='false'"
time=2025-10-25T01:38:48.732524257Z level=info msg="  --policy-cidr-match-mode=''"
time=2025-10-25T01:38:48.732531608Z level=info msg="  --policy-default-local-cluster='false'"
time=2025-10-25T01:38:48.732539364Z level=info msg="  --policy-queue-size='100'"
time=2025-10-25T01:38:48.732547164Z level=info msg="  --policy-secrets-namespace='cilium-secrets'"
time=2025-10-25T01:38:48.732555242Z level=info msg="  --policy-secrets-only-from-secrets-namespace='true'"
time=2025-10-25T01:38:48.732569909Z level=info msg="  --policy-trigger-interval='1s'"
time=2025-10-25T01:38:48.732577261Z level=info msg="  --pprof='false'"
time=2025-10-25T01:38:48.732584846Z level=info msg="  --pprof-address='localhost'"
time=2025-10-25T01:38:48.732592376Z level=info msg="  --pprof-port='6060'"
time=2025-10-25T01:38:48.732599075Z level=info msg="  --preallocate-bpf-maps='false'"
time=2025-10-25T01:38:48.73260636Z level=info msg="  --prepend-iptables-chains='true'"
time=2025-10-25T01:38:48.732613582Z level=info msg="  --procfs='/host/proc'"
time=2025-10-25T01:38:48.732622202Z level=info msg="  --prometheus-serve-addr=''"
time=2025-10-25T01:38:48.732629805Z level=info msg="  --proxy-admin-port='0'"
time=2025-10-25T01:38:48.732636692Z level=info msg="  --proxy-connect-timeout='2'"
time=2025-10-25T01:38:48.732643897Z level=info msg="  --proxy-gid='1337'"
time=2025-10-25T01:38:48.732650918Z level=info msg="  --proxy-idle-timeout-seconds='60'"
time=2025-10-25T01:38:48.732658016Z level=info msg="  --proxy-initial-fetch-timeout='30'"
time=2025-10-25T01:38:48.732665064Z level=info msg="  --proxy-max-concurrent-retries='128'"
time=2025-10-25T01:38:48.732672339Z level=info msg="  --proxy-max-connection-duration-seconds='0'"
time=2025-10-25T01:38:48.732679479Z level=info msg="  --proxy-max-requests-per-connection='0'"
time=2025-10-25T01:38:48.732688325Z level=info msg="  --proxy-portrange-max='20000'"
time=2025-10-25T01:38:48.732695617Z level=info msg="  --proxy-portrange-min='10000'"
time=2025-10-25T01:38:48.7327028Z level=info msg="  --proxy-prometheus-port='9964'"
time=2025-10-25T01:38:48.732709956Z level=info msg="  --proxy-xff-num-trusted-hops-egress='0'"
time=2025-10-25T01:38:48.732718651Z level=info msg="  --proxy-xff-num-trusted-hops-ingress='0'"
time=2025-10-25T01:38:48.732727453Z level=info msg="  --read-cni-conf=''"
time=2025-10-25T01:38:48.732734486Z level=info msg="  --remove-cilium-node-taints='true'"
time=2025-10-25T01:38:48.732741666Z level=info msg="  --restore='true'"
time=2025-10-25T01:38:48.732749137Z level=info msg="  --restored-proxy-ports-age-limit='15'"
time=2025-10-25T01:38:48.732756479Z level=info msg="  --route-metric='0'"
time=2025-10-25T01:38:48.732763544Z level=info msg="  --routing-mode='tunnel'"
time=2025-10-25T01:38:48.732770493Z level=info msg="  --service-no-backend-response='reject'"
time=2025-10-25T01:38:48.732777604Z level=info msg="  --set-cilium-is-up-condition='true'"
time=2025-10-25T01:38:48.732784906Z level=info msg="  --socket-path='/var/run/cilium/cilium.sock'"
time=2025-10-25T01:38:48.73279242Z level=info msg="  --srv6-encap-mode='reduced'"
time=2025-10-25T01:38:48.732800937Z level=info msg="  --standalone-dns-proxy-server-port='40045'"
time=2025-10-25T01:38:48.732808157Z level=info msg="  --state-dir='/var/run/cilium'"
time=2025-10-25T01:38:48.732816558Z level=info msg="  --static-cnp-path=''"
time=2025-10-25T01:38:48.732824424Z level=info msg="  --status-collector-failure-threshold='1m0s'"
time=2025-10-25T01:38:48.732831992Z level=info msg="  --status-collector-interval='5s'"
time=2025-10-25T01:38:48.732839546Z level=info msg="  --status-collector-probe-check-timeout='5m0s'"
time=2025-10-25T01:38:48.732848228Z level=info msg="  --status-collector-stackdump-path='/run/cilium/state/agent.stack.gz'"
time=2025-10-25T01:38:48.732939133Z level=info msg="  --status-collector-warning-threshold='15s'"
time=2025-10-25T01:38:48.732951722Z level=info msg="  --synchronize-k8s-nodes='true'"
time=2025-10-25T01:38:48.732959733Z level=info msg="  --tofqdns-dns-reject-response-code='refused'"
time=2025-10-25T01:38:48.732967423Z level=info msg="  --tofqdns-enable-dns-compression='true'"
time=2025-10-25T01:38:48.732974763Z level=info msg="  --tofqdns-endpoint-max-ip-per-hostname='1000'"
time=2025-10-25T01:38:48.732981993Z level=info msg="  --tofqdns-idle-connection-grace-period='0s'"
time=2025-10-25T01:38:48.732989447Z level=info msg="  --tofqdns-max-deferred-connection-deletes='10000'"
time=2025-10-25T01:38:48.732998002Z level=info msg="  --tofqdns-min-ttl='0'"
time=2025-10-25T01:38:48.733008819Z level=info msg="  --tofqdns-pre-cache=''"
time=2025-10-25T01:38:48.733016236Z level=info msg="  --tofqdns-preallocate-identities='true'"
time=2025-10-25T01:38:48.733023818Z level=info msg="  --tofqdns-proxy-port='0'"
time=2025-10-25T01:38:48.733030709Z level=info msg="  --tofqdns-proxy-response-max-delay='100ms'"
time=2025-10-25T01:38:48.733038661Z level=info msg="  --trace-payloadlen='128'"
time=2025-10-25T01:38:48.733046409Z level=info msg="  --trace-payloadlen-overlay='192'"
time=2025-10-25T01:38:48.733055005Z level=info msg="  --trace-sock='true'"
time=2025-10-25T01:38:48.73306272Z level=info msg="  --tunnel-port='0'"
time=2025-10-25T01:38:48.733069716Z level=info msg="  --tunnel-protocol='vxlan'"
time=2025-10-25T01:38:48.733076877Z level=info msg="  --tunnel-source-port-range='0-0'"
time=2025-10-25T01:38:48.733084712Z level=info msg="  --underlay-protocol='ipv4'"
time=2025-10-25T01:38:48.73309154Z level=info msg="  --unmanaged-pod-watcher-interval='15'"
time=2025-10-25T01:38:48.733098948Z level=info msg="  --use-cilium-internal-ip-for-ipsec='false'"
time=2025-10-25T01:38:48.733106416Z level=info msg="  --use-full-tls-context='false'"
time=2025-10-25T01:38:48.733113457Z level=info msg="  --version='false'"
time=2025-10-25T01:38:48.733143567Z level=info msg="  --vlan-bpf-bypass=''"
time=2025-10-25T01:38:48.733154002Z level=info msg="  --vtep-cidr=''"
time=2025-10-25T01:38:48.733161994Z level=info msg="  --vtep-endpoint=''"
time=2025-10-25T01:38:48.733169869Z level=info msg="  --vtep-mac=''"
time=2025-10-25T01:38:48.733177426Z level=info msg="  --vtep-mask=''"
time=2025-10-25T01:38:48.733186018Z level=info msg="  --wireguard-persistent-keepalive='0s'"
time=2025-10-25T01:38:48.733193597Z level=info msg="  --wireguard-track-all-ips-fallback='false'"
time=2025-10-25T01:38:48.733201367Z level=info msg="  --write-cni-conf-when-ready='/host/etc/cni/net.d/05-cilium.conflist'"
time=2025-10-25T01:38:48.733221112Z level=info msg="     _ _ _"
time=2025-10-25T01:38:48.733228235Z level=info msg=" ___|_| |_|_ _ _____"
time=2025-10-25T01:38:48.733234612Z level=info msg="|  _| | | | | |     |"
time=2025-10-25T01:38:48.733241017Z level=info msg=|___|_|_|_|___|_|_|_|
time=2025-10-25T01:38:48.733248205Z level=info msg="Cilium 1.18.1 e8a7070f 2025-08-13T14:47:02+00:00 go version go1.24.6 linux/amd64"
time=2025-10-25T01:38:48.75095764Z level=info msg="Detected mounted BPF filesystem" bpffsRoot=/sys/fs/bpf
time=2025-10-25T01:38:48.751461768Z level=info msg="Mounted cgroupv2 filesystem" location=/run/cilium/cgroupv2
time=2025-10-25T01:38:48.751525589Z level=info msg="Parsing base label prefixes from default label list"
time=2025-10-25T01:38:48.751652303Z level=info msg="Parsing node label prefixes from user inputs" prefix=[]
time=2025-10-25T01:38:48.751676463Z level=info msg="Parsing additional label prefixes from user inputs" prefix=[]
time=2025-10-25T01:38:48.751694887Z level=info msg="Final label prefixes to be used for identity evaluation:"
time=2025-10-25T01:38:48.75171864Z level=info msg=" - reserved:.*"
time=2025-10-25T01:38:48.751725746Z level=info msg=" - :io\\.kubernetes\\.pod\\.namespace"
time=2025-10-25T01:38:48.751731924Z level=info msg=" - :io\\.cilium\\.k8s\\.namespace\\.labels"
time=2025-10-25T01:38:48.751737654Z level=info msg=" - :app\\.kubernetes\\.io"
time=2025-10-25T01:38:48.751743029Z level=info msg=" - :io\\.cilium\\.k8s\\.policy\\.cluster"
time=2025-10-25T01:38:48.751748597Z level=info msg=" - :io\\.cilium\\.k8s\\.policy\\.serviceaccount"
time=2025-10-25T01:38:48.751770083Z level=info msg=" - !:io\\.kubernetes"
time=2025-10-25T01:38:48.751776217Z level=info msg=" - !:kubernetes\\.io"
time=2025-10-25T01:38:48.75178171Z level=info msg=" - !:statefulset\\.kubernetes\\.io/pod-name"
time=2025-10-25T01:38:48.751788518Z level=info msg=" - !:apps\\.kubernetes\\.io/pod-index"
time=2025-10-25T01:38:48.751794338Z level=info msg=" - !:batch\\.kubernetes\\.io/job-completion-index"
time=2025-10-25T01:38:48.751800081Z level=info msg=" - !:batch\\.kubernetes\\.io/controller-uid"
time=2025-10-25T01:38:48.751805897Z level=info msg=" - !:.*beta\\.kubernetes\\.io"
time=2025-10-25T01:38:48.751863146Z level=info msg=" - !:k8s\\.io"
time=2025-10-25T01:38:48.751871848Z level=info msg=" - !:pod-template-generation"
time=2025-10-25T01:38:48.751877506Z level=info msg=" - !:pod-template-hash"
time=2025-10-25T01:38:48.751882978Z level=info msg=" - !:controller-revision-hash"
time=2025-10-25T01:38:48.751888504Z level=info msg=" - !:controller-uid"
time=2025-10-25T01:38:48.751893927Z level=info msg=" - !:annotation.*"
time=2025-10-25T01:38:48.751899353Z level=info msg=" - !:etcd_node"
time=2025-10-25T01:38:48.751905092Z level=info msg="Final node label prefixes to be used for identity evaluation:"
time=2025-10-25T01:38:48.860313228Z level=info msg="Memory available for map entries (0.250% of 4055957504B): 10139893B" module=agent.controlplane
time=2025-10-25T01:38:48.860361008Z level=info msg="option bpf-sock-rev-map-max set by dynamic sizing to 65536" module=agent.controlplane
time=2025-10-25T01:38:48.965196485Z level=info msg="Cgroup metadata manager is enabled" module=agent.controlplane.cgroup-manager
time=2025-10-25T01:38:48.996599619Z level=info msg="Spire Delegate API Client is disabled as no socket path is configured" module=agent.controlplane.auth.spire-delegate
time=2025-10-25T01:38:48.996790743Z level=info msg="Mutual authentication handler is disabled as no port is configured" module=agent.controlplane.auth
time=2025-10-25T01:38:48.997527604Z level=info msg="Certloader TLS watcher disabled" module=agent.controlplane.hubble config=certloader-server-tls
time=2025-10-25T01:38:48.997787022Z level=info msg="The Hubble static exporter is disabled" module=agent.controlplane.hubble.hubble-exporters
time=2025-10-25T01:38:48.997817174Z level=info msg="The Hubble dynamic exporter is disabled" module=agent.controlplane.hubble.hubble-exporters
time=2025-10-25T01:38:48.998393452Z level=info msg="Certloader TLS watcher disabled" module=agent.controlplane.hubble.hubble-metrics config=certloader-server-tls
time=2025-10-25T01:38:48.998982079Z level=info msg="The Hubble metrics server is disabled" module=agent.controlplane.hubble.hubble-metrics
time=2025-10-25T01:38:49.016325969Z level=info msg=Invoked duration=142.24841ms function="cmd.configureAPIServer (cmd/cells.go:334)"
time=2025-10-25T01:38:49.076916707Z level=info msg="Determined final XDP mode" module=agent.datapath.datapath-xdp-config accelarationMode=disabled mode=disabled
time=2025-10-25T01:38:49.080728647Z level=info msg="Starting hive"
time=2025-10-25T01:38:49.080860507Z level=info msg="Started gops server" module=agent.infra.gops address=127.0.0.1:9890
time=2025-10-25T01:38:49.082549354Z level=info msg="Establishing connection to apiserver" module=agent.infra.k8s-client ipAddr=https://10.0.6.12:6443
time=2025-10-25T01:38:49.090665339Z level=info msg="Connected to apiserver" module=agent.infra.k8s-client
time=2025-10-25T01:38:49.092430835Z level=info msg="Waiting until all Cilium CRDs are available" module=agent.infra.k8s-synced-crdsync
time=2025-10-25T01:38:50.093750433Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T01:38:51.0940334Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T01:38:52.092681168Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:38:53.092698799Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T01:38:54.09351288Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:38:55.093698429Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:38:56.09398853Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:38:57.093412137Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:38:58.096607759Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:38:59.092708224Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T01:39:00.093302143Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T01:39:01.093387649Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:02.093047851Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:03.09326479Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:39:04.094126487Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T01:39:05.092640595Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:06.093678603Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:07.09427334Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:39:08.094418471Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:39:09.093017419Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T01:39:10.094489908Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T01:39:11.093218951Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T01:39:12.093071763Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T01:39:13.092952377Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:39:14.094136649Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:15.09338728Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T01:39:16.093021927Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:39:17.093254405Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:18.097074976Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:19.144100308Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:20.143426751Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:39:21.143112876Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:39:22.292783937Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:23.294007694Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T01:39:24.294477067Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T01:39:25.293394218Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:26.29330743Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:39:27.294684882Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:39:28.294378288Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T01:39:29.296435956Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T01:39:30.292729429Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:31.292926853Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:39:32.292681721Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:33.29449026Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:39:34.293451063Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:39:35.294495796Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:36.293717649Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:37.294270057Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T01:39:38.29419833Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:39.294127175Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T01:39:40.293676269Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:39:41.293007881Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:39:42.292778938Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T01:39:43.293333622Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T01:39:44.293978228Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:45.292915094Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:39:46.292772175Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:39:47.293248356Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:48.292686133Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:39:49.293032046Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:39:50.293467602Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T01:39:51.293552068Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:52.293694106Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:39:53.294237694Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T01:39:54.293306177Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:39:55.293442564Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T01:39:56.294733437Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T01:39:57.293495447Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:58.294570747Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:39:59.294553767Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:40:00.294889722Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T01:40:01.294895805Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:02.293680552Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T01:40:03.294230424Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:04.29349478Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:05.293405433Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T01:40:06.294519696Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:40:07.29430127Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:40:08.292645631Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:09.294601844Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T01:40:10.294336995Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:11.292723754Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:12.293266585Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:40:13.294477683Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T01:40:14.294572964Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:40:15.292701971Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T01:40:16.294499424Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T01:40:17.294353879Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:40:18.294599679Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:19.293377243Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:20.293910025Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T01:40:21.29450451Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T01:40:22.293103145Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:40:23.293889322Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:40:24.294387447Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:40:25.294465994Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:26.294763385Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:27.393188Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:40:28.393271093Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T01:40:29.879406586Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:40:30.844413101Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T01:40:31.8433805Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:32.844175911Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:33.844418347Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:40:34.84328097Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:35.84293301Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:36.843454817Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:37.843353793Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T01:40:38.842904839Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:40:39.844024435Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T01:40:40.843028855Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:40:41.842661177Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:42.842733538Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:43.844221965Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:40:44.842890941Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:40:45.844051182Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T01:40:46.842666274Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:40:47.844619126Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:40:48.844111733Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:49.844408742Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:40:50.842889668Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:51.843524601Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:40:52.844125437Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T01:40:53.842950389Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:40:54.843026749Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:55.842992241Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:40:56.844374219Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:40:57.843096784Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:40:58.843372928Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T01:40:59.843464078Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:00.842748718Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:01.844386939Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:41:02.844424075Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:41:03.842672124Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:04.84427912Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:05.844251661Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:06.84363132Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:07.843170539Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:08.842737419Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:09.843948148Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:10.844231752Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:11.842795381Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:12.843242882Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T01:41:13.844631426Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T01:41:14.84444786Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:41:15.842920543Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:16.842918816Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:41:17.842987255Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:18.843724107Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T01:41:19.843312723Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:20.842943312Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:21.843990484Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:22.842777063Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:23.843055672Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:41:24.843151987Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:41:25.844288365Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:41:26.844616951Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T01:41:27.843295171Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T01:41:28.843184418Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:29.844301777Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T01:41:30.842658403Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:31.843212112Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:32.844377886Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T01:41:33.842907576Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:34.844410096Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:41:35.944648278Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:36.943969796Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:37.942959091Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:38.942758814Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T01:41:39.942898317Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:40.944103115Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:41:41.943019243Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:42.942749339Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:41:43.94297394Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T01:41:44.942768024Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:45.942759097Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T01:41:46.94352556Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:47.942722694Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T01:41:48.94280855Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:49.943764128Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:41:50.942906162Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T01:41:51.94362809Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:52.942872618Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T01:41:53.943653865Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:41:54.943573591Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io]"

2025-10-25 08:42:08,811 p=3160637 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Describe Cilium Operator pod ===
    Optional:  false
  kube-api-access-dh4rg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane op=Exists
                             node-role.kubernetes.io/master op=Exists
                             node.cilium.io/agent-not-ready op=Exists
                             node.kubernetes.io/not-ready op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 40s
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  28m                default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {CriticalAddonsOnly: true}. no new claims to deallocate, preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Warning  FailedScheduling  23m                default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {CriticalAddonsOnly: true}. no new claims to deallocate, preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Warning  FailedScheduling  17m (x2 over 22m)  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {CriticalAddonsOnly: true}. no new claims to deallocate, preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.

2025-10-25 08:42:26,926 p=3161050 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Node taints ===
Taints:             CriticalAddonsOnly=true:NoExecute
                    node-role.kubernetes.io/etcd=true:NoExecute
                    node-role.kubernetes.io/control-plane=true:NoSchedule
                    node.kubernetes.io/not-ready:NoSchedule
Unschedulable:      false
Lease:
=== Add toleration to Cilium Operator ===
deployment.apps/cilium-operator patched

2025-10-25 08:43:07,341 p=3162084 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Cilium pods status ===
cilium-operator-69c7b95789-z9dnz                        0/1     Pending     0             29m
cilium-operator-6f85dd987c-7s9c8                        0/1     Pending     0             40s
cilium-operator-6f85dd987c-82qxh                        0/1     Pending     0             40s
cilium-ttl74                                            0/1     Running     0             4m27s
helm-install-rke2-cilium-dt4kt                          0/1     Completed   0             29m
=== Node status ===
NAME     STATUS     ROLES                AGE   VERSION          INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-m1   NotReady   control-plane,etcd   30m   v1.34.1+rke2r1   10.0.6.12     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2

2025-10-25 08:43:21,107 p=3162451 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Describe new Cilium Operator pod ===
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane op=Exists
                             node-role.kubernetes.io/master op=Exists
                             node.cilium.io/agent-not-ready op=Exists
                             node.kubernetes.io/not-ready op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 40s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  54s   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/etcd: true}. no new claims to deallocate, preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.

2025-10-25 08:43:34,230 p=3162803 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Add etcd toleration to Cilium Operator ===
deployment.apps/cilium-operator patched

2025-10-25 08:44:24,331 p=3164127 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Final status check ===
cilium-operator-6f85dd987c-7s9c8                        0/1     Pending             0             117s
cilium-operator-f47b7f4f9-vlhhp                         0/1     Pending             0             50s
cilium-operator-f47b7f4f9-vmmhh                         0/1     Running             0             50s
cilium-ttl74                                            0/1     Running             1 (34s ago)   5m44s
helm-install-rke2-cilium-dt4kt                          0/1     Completed           0             31m
=== Node status ===
NAME     STATUS   ROLES                AGE   VERSION          INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-m1   Ready    control-plane,etcd   31m   v1.34.1+rke2r1   10.0.6.12     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2
=== All kube-system pods ===
cilium-operator-f47b7f4f9-vmmhh                         0/1     Running             0             50s
cilium-ttl74                                            0/1     Running             1 (34s ago)   5m44s
etcd-k8s-m1                                             1/1     Running             0             31m
kube-apiserver-k8s-m1                                   1/1     Running             0             31m
kube-controller-manager-k8s-m1                          1/1     Running             0             31m
kube-scheduler-k8s-m1                                   1/1     Running             1 (24m ago)   31m
kube-vip-ds-pmnqp                                       1/1     Running             6 (14m ago)   31m

2025-10-25 08:44:33,430 p=3164386 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Cilium agent logs ===
time=2025-10-25T01:44:25.598787829Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumnodes.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T01:44:26.597463494Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:44:27.697649868Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumnodes.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:44:28.697655848Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:44:29.698496528Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumnodes.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T01:44:30.697898215Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumnodes.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T01:44:31.697945411Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:44:32.69868075Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumnodes.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T01:44:33.697910747Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumnodes.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:44:34.697745824Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumnodes.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"

2025-10-25 08:45:57,497 p=3166438 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Final comprehensive status check ===
--- Cilium pods ---
cilium-operator-6f85dd987c-7s9c8                        0/1     Pending             0              3m30s
cilium-operator-f47b7f4f9-vlhhp                         0/1     Pending             0              2m23s
cilium-operator-f47b7f4f9-vmmhh                         0/1     Running             1 (52s ago)    2m23s
cilium-ttl74                                            0/1     Running             1 (2m7s ago)   7m17s
helm-install-rke2-cilium-dt4kt                          0/1     Completed           0              32m
--- Node status ---
NAME     STATUS   ROLES                AGE   VERSION          INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-m1   Ready    control-plane,etcd   32m   v1.34.1+rke2r1   10.0.6.12     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2
--- All kube-system pods ---
NAME                                                    READY   STATUS              RESTARTS       AGE
cilium-operator-6f85dd987c-7s9c8                        0/1     Pending             0              3m30s
cilium-operator-f47b7f4f9-vlhhp                         0/1     Pending             0              2m23s
cilium-operator-f47b7f4f9-vmmhh                         0/1     Running             1 (52s ago)    2m23s
cilium-ttl74                                            0/1     Running             1 (2m7s ago)   7m17s
etcd-k8s-m1                                             1/1     Running             0              32m
helm-install-rke2-cilium-dt4kt                          0/1     Completed           0              32m
helm-install-rke2-coredns-7sb7t                         0/1     Completed           0              32m
helm-install-rke2-ingress-nginx-hbvng                   0/1     Pending             0              32m
helm-install-rke2-runtimeclasses-z58c6                  0/1     Pending             0              32m
helm-install-rke2-snapshot-controller-crd-7k6h4         0/1     Pending             0              32m
helm-install-rke2-snapshot-controller-tqtbm             0/1     Pending             0              32m
kube-apiserver-k8s-m1                                   1/1     Running             0              32m
kube-controller-manager-k8s-m1                          1/1     Running             0              32m
kube-scheduler-k8s-m1                                   1/1     Running             1 (26m ago)    32m
kube-vip-ds-pmnqp                                       1/1     Running             6 (16m ago)    32m
rke2-coredns-rke2-coredns-autoscaler-5fcf54974d-xwqrl   0/1     Pending             0              32m
rke2-coredns-rke2-coredns-bd94c5787-q7h4v               0/1     ContainerCreating   0              32m
--- Cilium agent readiness ---
time=2025-10-25T01:45:54.048131547Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T01:45:55.047865449Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumnodes.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T01:45:56.048502146Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumnodes.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T01:45:57.048931526Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumnodes.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T01:45:58.047237126Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumloadbalancerippools.cilium.io]"

2025-10-25 08:46:13,585 p=3166846 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Cilium Operator logs ===
time=2025-10-25T01:45:06.537681011Z level=info msg="  --synchronize-k8s-services='true'" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537687531Z level=info msg="  --taint-sync-workers='10'" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537693594Z level=info msg="  --tofqdns-dns-reject-response-code='refused'" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537699607Z level=info msg="  --tofqdns-enable-dns-compression='true'" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537705719Z level=info msg="  --tofqdns-endpoint-max-ip-per-hostname='1000'" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537711832Z level=info msg="  --tofqdns-idle-connection-grace-period='0s'" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537717856Z level=info msg="  --tofqdns-max-deferred-connection-deletes='10000'" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537723942Z level=info msg="  --tofqdns-preallocate-identities='true'" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537729919Z level=info msg="  --tofqdns-proxy-response-max-delay='100ms'" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537735911Z level=info msg="  --tunnel-protocol='vxlan'" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537741822Z level=info msg="  --tunnel-source-port-range='0-0'" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537748117Z level=info msg="  --unmanaged-pod-watcher-interval='15'" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537754408Z level=info msg="  --validate-network-policy='true'" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537760768Z level=info msg="  --version='false'" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537767363Z level=info msg="  --vtep-cidr=''" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537773879Z level=info msg="  --vtep-endpoint=''" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537780278Z level=info msg="  --vtep-mac=''" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537786627Z level=info msg="  --vtep-mask=''" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537792728Z level=info msg="  --write-cni-conf-when-ready='/host/etc/cni/net.d/05-cilium.conflist'" subsys=cilium-operator-generic
time=2025-10-25T01:45:06.537799075Z level=info msg="Cilium Operator" subsys=cilium-operator-generic version="1.18.1 e8a7070f 2025-08-13T14:47:02+00:00 go version go1.24.6 linux/amd64"

2025-10-25 08:46:29,258 p=3167210 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Check Cilium CRDs ===
0
=== List some Cilium CRDs ===

2025-10-25 08:46:42,143 p=3167517 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Cilium Operator detailed logs ===
time=2025-10-25T01:46:26.567859108Z level=info msg="  --operator-pprof='false'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568101142Z level=info msg="  --operator-pprof-address='localhost'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568152226Z level=info msg="  --operator-pprof-port='6061'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568165747Z level=info msg="  --operator-prometheus-serve-addr=':9963'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568176292Z level=info msg="  --parallel-alloc-workers='50'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568184055Z level=info msg="  --pod-restart-selector='k8s-app=kube-dns'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568194925Z level=info msg="  --policy-cidr-match-mode=''" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568202878Z level=info msg="  --policy-default-local-cluster='false'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568217089Z level=info msg="  --policy-secrets-namespace='cilium-secrets'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568224679Z level=info msg="  --policy-secrets-only-from-secrets-namespace='true'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568231845Z level=info msg="  --preallocate-bpf-maps='false'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568238893Z level=info msg="  --procfs='/host/proc'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568245901Z level=info msg="  --proxy-connect-timeout='2'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568252561Z level=info msg="  --proxy-idle-timeout-seconds='60'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568259135Z level=info msg="  --proxy-initial-fetch-timeout='30'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568266084Z level=info msg="  --proxy-max-concurrent-retries='128'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568273049Z level=info msg="  --proxy-max-connection-duration-seconds='0'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568279864Z level=info msg="  --proxy-max-requests-per-connection='0'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568291871Z level=info msg="  --proxy-prometheus-port='9964'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568298852Z level=info msg="  --proxy-xff-num-trusted-hops-egress='0'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.56830542Z level=info msg="  --proxy-xff-num-trusted-hops-ingress='0'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.56831247Z level=info msg="  --remove-cilium-node-taints='true'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568319254Z level=info msg="  --routing-mode='tunnel'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568325836Z level=info msg="  --service-no-backend-response='reject'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568334148Z level=info msg="  --set-cilium-is-up-condition='true'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.56834197Z level=info msg="  --set-cilium-node-taints='false'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568349385Z level=info msg="  --skip-crd-creation='false'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568367024Z level=info msg="  --subnet-ids-filter=''" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568377268Z level=info msg="  --subnet-tags-filter=''" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568384591Z level=info msg="  --synchronize-k8s-nodes='true'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568398666Z level=info msg="  --synchronize-k8s-services='true'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568406398Z level=info msg="  --taint-sync-workers='10'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568413311Z level=info msg="  --tofqdns-dns-reject-response-code='refused'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568420103Z level=info msg="  --tofqdns-enable-dns-compression='true'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568426971Z level=info msg="  --tofqdns-endpoint-max-ip-per-hostname='1000'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568433864Z level=info msg="  --tofqdns-idle-connection-grace-period='0s'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568440631Z level=info msg="  --tofqdns-max-deferred-connection-deletes='10000'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568447365Z level=info msg="  --tofqdns-preallocate-identities='true'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.56845415Z level=info msg="  --tofqdns-proxy-response-max-delay='100ms'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568460899Z level=info msg="  --tunnel-protocol='vxlan'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568467541Z level=info msg="  --tunnel-source-port-range='0-0'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568497011Z level=info msg="  --unmanaged-pod-watcher-interval='15'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568504908Z level=info msg="  --validate-network-policy='true'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568512363Z level=info msg="  --version='false'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568520308Z level=info msg="  --vtep-cidr=''" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568527577Z level=info msg="  --vtep-endpoint=''" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568535361Z level=info msg="  --vtep-mac=''" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.56854285Z level=info msg="  --vtep-mask=''" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568549994Z level=info msg="  --write-cni-conf-when-ready='/host/etc/cni/net.d/05-cilium.conflist'" subsys=cilium-operator-generic
time=2025-10-25T01:46:26.568557576Z level=info msg="Cilium Operator" subsys=cilium-operator-generic version="1.18.1 e8a7070f 2025-08-13T14:47:02+00:00 go version go1.24.6 linux/amd64"

2025-10-25 08:46:56,916 p=3167871 u=root n=ansible | k8s-m1 | FAILED | rc=1 >>
=== Check Cilium Operator events ===
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane op=Exists
                             node-role.kubernetes.io/etcd op=Exists
                             node-role.kubernetes.io/master op=Exists
                             node.cilium.io/agent-not-ready op=Exists
                             node.kubernetes.io/not-ready op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 40s
Events:
  Type     Reason     Age                    From               Message
  ----     ------     ----                   ----               -------
  Normal   Scheduled  3m22s                  default-scheduler  Successfully assigned kube-system/cilium-operator-f47b7f4f9-vmmhh to k8s-m1
  Warning  Unhealthy  112s (x22 over 3m21s)  kubelet            Readiness probe failed: Get "http://127.0.0.1:9234/healthz": dial tcp 127.0.0.1:9234: connect: connection refused
  Warning  Unhealthy  112s (x3 over 2m12s)   kubelet            Liveness probe failed: Get "http://127.0.0.1:9234/healthz": dial tcp 127.0.0.1:9234: connect: connection refused
  Normal   Pulled     32s (x3 over 3m22s)    kubelet            Container image "rancher/mirrored-cilium-operator-generic:v1.18.1" already present on machine
  Normal   Created    32s (x3 over 3m21s)    kubelet            Created container: cilium-operator
  Normal   Started    32s (x3 over 3m21s)    kubelet            Started container cilium-operator
  Normal   Killing    32s (x2 over 112s)     kubelet            Container cilium-operator failed liveness probe, will be restarted
=== Try to manually create a simple CRD test ===non-zero return code

2025-10-25 08:48:00,785 p=3168467 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Delete all Cilium pods to force restart ===
pod "cilium-ttl74" deleted from kube-system namespace
=== Delete Cilium Operator pods ===
pod "cilium-operator-6f85dd987c-7s9c8" deleted from kube-system namespace
pod "cilium-operator-f47b7f4f9-vlhhp" deleted from kube-system namespace
pod "cilium-operator-f47b7f4f9-vmmhh" deleted from kube-system namespace

2025-10-25 08:49:06,652 p=3170198 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Check status after restart ===
cilium-2x4zg                                            0/1     Running             0             67s
cilium-operator-6f85dd987c-qth7b                        0/1     Pending             0             66s
cilium-operator-f47b7f4f9-5mf24                         0/1     Running             0             66s
cilium-operator-f47b7f4f9-7vfhs                         0/1     Pending             0             66s
helm-install-rke2-cilium-dt4kt                          0/1     Completed           0             35m
=== Node status ===
NAME     STATUS   ROLES                AGE   VERSION
k8s-m1   Ready    control-plane,etcd   36m   v1.34.1+rke2r1
=== Check CRDs ===
0

2025-10-25 08:50:36,483 p=3172396 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Final comprehensive check ===
--- Cilium pods ---
cilium-2x4zg                                            0/1     Running             0             2m37s
cilium-operator-6f85dd987c-qth7b                        0/1     Pending             0             2m36s
cilium-operator-f47b7f4f9-5mf24                         0/1     Running             1 (75s ago)   2m36s
cilium-operator-f47b7f4f9-7vfhs                         0/1     Pending             0             2m36s
helm-install-rke2-cilium-dt4kt                          0/1     Completed           0             37m
--- Node status ---
NAME     STATUS   ROLES                AGE   VERSION          INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-m1   Ready    control-plane,etcd   37m   v1.34.1+rke2r1   10.0.6.12     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2
--- CRDs count ---
0
--- Sample CRDs ---
--- Cilium agent status ---
time=2025-10-25T01:50:35.941978575Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T01:50:36.940986348Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T01:50:37.940934166Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"

2025-10-25 08:50:52,967 p=3172824 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Check running Cilium Operator logs ===
time=2025-10-25T01:50:42.583351833Z level=info msg="  --synchronize-k8s-services='true'" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.58335914Z level=info msg="  --taint-sync-workers='10'" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.583366009Z level=info msg="  --tofqdns-dns-reject-response-code='refused'" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.583372799Z level=info msg="  --tofqdns-enable-dns-compression='true'" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.583379689Z level=info msg="  --tofqdns-endpoint-max-ip-per-hostname='1000'" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.583386557Z level=info msg="  --tofqdns-idle-connection-grace-period='0s'" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.583393318Z level=info msg="  --tofqdns-max-deferred-connection-deletes='10000'" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.583399995Z level=info msg="  --tofqdns-preallocate-identities='true'" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.583407011Z level=info msg="  --tofqdns-proxy-response-max-delay='100ms'" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.5834138Z level=info msg="  --tunnel-protocol='vxlan'" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.583420405Z level=info msg="  --tunnel-source-port-range='0-0'" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.583427234Z level=info msg="  --unmanaged-pod-watcher-interval='15'" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.583437065Z level=info msg="  --validate-network-policy='true'" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.58344435Z level=info msg="  --version='false'" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.583451913Z level=info msg="  --vtep-cidr=''" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.583459148Z level=info msg="  --vtep-endpoint=''" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.583466295Z level=info msg="  --vtep-mac=''" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.583473418Z level=info msg="  --vtep-mask=''" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.583480297Z level=info msg="  --write-cni-conf-when-ready='/host/etc/cni/net.d/05-cilium.conflist'" subsys=cilium-operator-generic
time=2025-10-25T01:50:42.58348759Z level=info msg="Cilium Operator" subsys=cilium-operator-generic version="1.18.1 e8a7070f 2025-08-13T14:47:02+00:00 go version go1.24.6 linux/amd64"
=== Check Cilium Operator events ===
  Type     Reason            Age                   From               Message
  ----     ------            ----                  ----               -------
  Warning  FailedScheduling  2m53s                 default-scheduler  0/1 nodes are available: 1 node(s) didn't have free ports for the requested pod ports. no new claims to deallocate, preemption: 0/1 nodes are available: 1 node(s) didn't have free ports for the requested pod ports.
  Normal   Scheduled         2m52s                 default-scheduler  Successfully assigned kube-system/cilium-operator-f47b7f4f9-5mf24 to k8s-m1
  Warning  Unhealthy         92s (x3 over 112s)    kubelet            Liveness probe failed: Get "http://127.0.0.1:9234/healthz": dial tcp 127.0.0.1:9234: connect: connection refused
  Warning  Unhealthy         86s (x22 over 2m51s)  kubelet            Readiness probe failed: Get "http://127.0.0.1:9234/healthz": dial tcp 127.0.0.1:9234: connect: connection refused
  Normal   Pulled            12s (x3 over 2m52s)   kubelet            Container image "rancher/mirrored-cilium-operator-generic:v1.18.1" already present on machine
  Normal   Created           12s (x3 over 2m52s)   kubelet            Created container: cilium-operator
  Normal   Started           12s (x3 over 2m52s)   kubelet            Started container cilium-operator
  Normal   Killing           12s (x2 over 92s)     kubelet            Container cilium-operator failed liveness probe, will be restarted

2025-10-25 08:51:10,600 p=3173292 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Try to get Cilium CRDs from Helm chart ===/bin/sh: 4: /var/lib/rancher/rke2/bin/helm: not found

2025-10-25 08:51:22,424 p=3173598 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Check if CRDs exist in manifests directory ===
/var/lib/rancher/rke2/server/manifests/rke2-cilium.yaml
=== Check RKE2 addons ===
total 464
drwx------ 2 root root   4096 Oct 25 01:13 .
drwx------ 7 root root    117 Oct 25 01:12 ..
-rw-rw-r-- 1 root root    805 Oct 25 01:11 kube-vip-rbac.yml
-rw-rw-r-- 1 root root   2523 Oct 25 01:11 kube-vip.yml
-rw-r--r-- 1 root root 290487 Oct 25 01:11 rke2-cilium.yaml
-rw-r--r-- 1 root root  33926 Oct 25 01:11 rke2-coredns.yaml
-rw-r--r-- 1 root root  79245 Oct 25 01:11 rke2-ingress-nginx.yaml
-rw-r--r-- 1 root root   7459 Oct 25 01:11 rke2-runtimeclasses.yaml
-rw-r--r-- 1 root root  20186 Oct 25 01:11 rke2-snapshot-controller-crd.yaml
-rw-r--r-- 1 root root  20134 Oct 25 01:11 rke2-snapshot-controller.yaml
-rw-r--r-- 1 root root     52 Oct 25 01:11 rke2-snapshot-validation-webhook.yaml

2025-10-25 08:51:43,600 p=3174062 u=root n=ansible | k8s-m1 | FAILED | rc=1 >>
=== Check CRDs in Cilium manifest ===
0
=== Try to apply Cilium manifest again ===Warning: resource helmcharts/rke2-cilium is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
The HelmChart "rke2-cilium" is invalid: metadata.annotations: Too long: may not be more than 262144 bytesnon-zero return code

2025-10-25 08:51:56,909 p=3174467 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Disable health checks for Cilium Operator ===
deployment.apps/cilium-operator patched

2025-10-25 08:53:19,179 p=3175768 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Check status after disabling health checks ===
cilium-2x4zg                                            0/1     Running             0             4m50s
cilium-operator-6d8db94895-8lpmg                        0/1     Pending             0             52s
cilium-operator-6d8db94895-cdhqr                        0/1     Pending             0             52s
cilium-operator-f47b7f4f9-5mf24                         0/1     Running             3 (48s ago)   4m49s
helm-install-rke2-cilium-dt4kt                          0/1     Completed           0             39m
=== Check CRDs ===
0
=== Wait a bit more and check again ===
0

2025-10-25 08:53:32,726 p=3176851 u=root n=ansible | k8s-m1 | FAILED | rc=1 >>
=== Download and apply Cilium CRDs manually ===error: no objects passed to apply
error: no objects passed to apply
error: no objects passed to applynon-zero return code

2025-10-25 08:54:17,123 p=3177171 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Final comprehensive status check ===
--- Node status ---
NAME     STATUS   ROLES                AGE   VERSION          INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-m1   Ready    control-plane,etcd   41m   v1.34.1+rke2r1   10.0.6.12     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2
--- All kube-system pods ---
NAME                                                    READY   STATUS              RESTARTS      AGE
cilium-2x4zg                                            0/1     Running             1 (67s ago)   6m18s
cilium-operator-6d8db94895-8lpmg                        0/1     Pending             0             2m20s
cilium-operator-6d8db94895-cdhqr                        0/1     Pending             0             2m20s
cilium-operator-f47b7f4f9-5mf24                         0/1     Running             4 (56s ago)   6m17s
etcd-k8s-m1                                             1/1     Running             0             41m
helm-install-rke2-cilium-dt4kt                          0/1     Completed           0             41m
helm-install-rke2-coredns-7sb7t                         0/1     Completed           0             41m
helm-install-rke2-ingress-nginx-hbvng                   0/1     Pending             0             41m
helm-install-rke2-runtimeclasses-z58c6                  0/1     Pending             0             41m
helm-install-rke2-snapshot-controller-crd-7k6h4         0/1     Pending             0             41m
helm-install-rke2-snapshot-controller-tqtbm             0/1     Pending             0             41m
kube-apiserver-k8s-m1                                   1/1     Running             0             41m
kube-controller-manager-k8s-m1                          1/1     Running             0             41m
kube-scheduler-k8s-m1                                   1/1     Running             1 (34m ago)   41m
kube-vip-ds-pmnqp                                       1/1     Running             6 (24m ago)   41m
rke2-coredns-rke2-coredns-autoscaler-5fcf54974d-xwqrl   0/1     Pending             0             40m
rke2-coredns-rke2-coredns-bd94c5787-q7h4v               0/1     ContainerCreating   0             40m
--- Cilium agent logs (last 5 lines) ---
time=2025-10-25T01:54:13.859894743Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:54:14.859162521Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:54:15.85997526Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T01:54:16.859282101Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T01:54:17.859113935Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumendpoints.cilium.io]"
--- Test basic cluster functionality ---
NAME              STATUS   AGE
cilium-secrets    Active   40m
default           Active   41m
kube-node-lease   Active   41m
kube-public       Active   41m
kube-system       Active   41m

2025-10-25 08:54:45,486 p=3177639 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Test pod creation ===
pod/test-pod created
--- Wait for pod to start ---
NAME       READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
test-pod   0/1     Pending   0          10s   <none>   <none>   <none>           <none>

2025-10-25 08:59:49,287 p=3180744 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Check why pod is pending ===
Name:             test-pod
Namespace:        default
Priority:         0
Service Account:  default
Node:             <none>
Labels:           <none>
Annotations:      <none>
Status:           Pending
IP:               
IPs:              <none>
Containers:
  test:
    Image:        nginx:alpine
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-84xpt (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  kube-api-access-84xpt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 60s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 40s
Events:
  Type     Reason            Age    From               Message
  ----     ------            ----   ----               -------
  Warning  FailedScheduling  5m13s  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {CriticalAddonsOnly: true}. no new claims to deallocate, preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Warning  FailedScheduling  0s     default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {CriticalAddonsOnly: true}. no new claims to deallocate, preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
=== Clean up test pod ===
pod "test-pod" deleted from default namespace

2025-10-25 09:00:06,059 p=3181185 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Remove CriticalAddonsOnly taint to allow regular pods ===
node/k8s-m1 untainted
=== Verify node taints ===
Taints:             node-role.kubernetes.io/etcd=true:NoExecute
                    node-role.kubernetes.io/control-plane=true:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  k8s-m1
  AcquireTime:     <unset>
  RenewTime:       Sat, 25 Oct 2025 02:00:06 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  EtcdIsVoter      True    Sat, 25 Oct 2025 01:58:18 +0000   Sat, 25 Oct 2025 01:13:18 +0000   MemberNotLearner             Node is a voting member of the etcd cluster

2025-10-25 09:00:34,881 p=3181510 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Test pod creation again ===
pod/test-pod created
--- Wait for pod to start ---
NAME       READY   STATUS              RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
test-pod   0/1     ContainerCreating   0          15s   <none>   k8s-m1   <none>           <none>

2025-10-25 09:01:12,672 p=3182441 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
=== Check pod status after more time ===
NAME       READY   STATUS              RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
test-pod   0/1     ContainerCreating   0          48s   <none>   k8s-m1   <none>           <none>
=== Check pod events if still not running ===
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule op=Exists
                             node-role.kubernetes.io/etcd:NoExecute op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 60s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 40s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  48s   default-scheduler  Successfully assigned default/test-pod to k8s-m1
=== Clean up test pod ===
pod "test-pod" deleted from default namespace

2025-10-25 09:06:52,001 p=3186933 u=root n=ansible | k8s-m1 | FAILED | rc=1 >>
=== Nodes ===
NAME     STATUS   ROLES                AGE   VERSION          INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-m1   Ready    control-plane,etcd   53m   v1.34.1+rke2r1   10.0.6.12     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2
=== Cilium and Operator pods ===
cilium-2x4zg                                            0/1     Running             4 (3m15s ago)   18m
cilium-operator-6d8db94895-8lpmg                        0/1     Pending             0               14m
cilium-operator-6d8db94895-cdhqr                        0/1     Pending             0               14m
cilium-operator-f47b7f4f9-5mf24                         0/1     Running             8 (5m21s ago)   18m
helm-install-rke2-cilium-dt4kt                          0/1     Completed           0               53m
=== cilium-operator deployment ===
=== Cilium CRDs ===
=== cilium-operator logs (tail) ===
=== cilium agent logs (tail) ===error: error executing jsonpath "{.status.readyReplicas}/{.status.replicas}{n}": Error executing template: unrecognized identifier n. Printing more information for debugging the template:
	template was:
		{.status.readyReplicas}/{.status.replicas}{n}
	object given to jsonpath engine was:
		map[string]interface {}{"apiVersion":"apps/v1", "kind":"Deployment", "metadata":map[string]interface {}{"annotations":map[string]interface {}{"deployment.kubernetes.io/revision":"4", "meta.helm.sh/release-name":"rke2-cilium", "meta.helm.sh/release-namespace":"kube-system"}, "creationTimestamp":"2025-10-25T01:13:25Z", "generation":4, "labels":map[string]interface {}{"app.kubernetes.io/managed-by":"Helm", "app.kubernetes.io/name":"cilium-operator", "app.kubernetes.io/part-of":"cilium", "io.cilium/app":"operator", "name":"cilium-operator"}, "managedFields":[]interface {}{map[string]interface {}{"apiVersion":"apps/v1", "fieldsType":"FieldsV1", "fieldsV1":map[string]interface {}{"f:metadata":map[string]interface {}{"f:annotations":map[string]interface {}{".":map[string]interface {}{}, "f:meta.helm.sh/release-name":map[string]interface {}{}, "f:meta.helm.sh/release-namespace":map[string]interface {}{}}, "f:labels":map[string]interface {}{".":map[string]interface {}{}, "f:app.kubernetes.io/managed-by":map[string]interface {}{}, "f:app.kubernetes.io/name":map[string]interface {}{}, "f:app.kubernetes.io/part-of":map[string]interface {}{}, "f:io.cilium/app":map[string]interface {}{}, "f:name":map[string]interface {}{}}}, "f:spec":map[string]interface {}{"f:progressDeadlineSeconds":map[string]interface {}{}, "f:replicas":map[string]interface {}{}, "f:revisionHistoryLimit":map[string]interface {}{}, "f:selector":map[string]interface {}{}, "f:strategy":map[string]interface {}{"f:rollingUpdate":map[string]interface {}{".":map[string]interface {}{}, "f:maxSurge":map[string]interface {}{}, "f:maxUnavailable":map[string]interface {}{}}, "f:type":map[string]interface {}{}}, "f:template":map[string]interface {}{"f:metadata":map[string]interface {}{"f:annotations":map[string]interface {}{".":map[string]interface {}{}, "f:prometheus.io/port":map[string]interface {}{}, "f:prometheus.io/scrape":map[string]interface {}{}}, "f:labels":map[string]interface {}{".":map[string]interface {}{}, "f:app.kubernetes.io/name":map[string]interface {}{}, "f:app.kubernetes.io/part-of":map[string]interface {}{}, "f:io.cilium/app":map[string]interface {}{}, "f:name":map[string]interface {}{}}}, "f:spec":map[string]interface {}{"f:affinity":map[string]interface {}{".":map[string]interface {}{}, "f:podAntiAffinity":map[string]interface {}{".":map[string]interface {}{}, "f:requiredDuringSchedulingIgnoredDuringExecution":map[string]interface {}{}}}, "f:automountServiceAccountToken":map[string]interface {}{}, "f:containers":map[string]interface {}{"k:{\"name\":\"cilium-operator\"}":map[string]interface {}{".":map[string]interface {}{}, "f:args":map[string]interface {}{}, "f:command":map[string]interface {}{}, "f:env":map[string]interface {}{".":map[string]interface {}{}, "k:{\"name\":\"CILIUM_DEBUG\"}":map[string]interface {}{".":map[string]interface {}{}, "f:name":map[string]interface {}{}, "f:valueFrom":map[string]interface {}{".":map[string]interface {}{}, "f:configMapKeyRef":map[string]interface {}{}}}, "k:{\"name\":\"CILIUM_K8S_NAMESPACE\"}":map[string]interface {}{".":map[string]interface {}{}, "f:name":map[string]interface {}{}, "f:valueFrom":map[string]interface {}{".":map[string]interface {}{}, "f:fieldRef":map[string]interface {}{}}}, "k:{\"name\":\"K8S_NODE_NAME\"}":map[string]interface {}{".":map[string]interface {}{}, "f:name":map[string]interface {}{}, "f:valueFrom":map[string]interface {}{".":map[string]interface {}{}, "f:fieldRef":map[string]interface {}{}}}}, "f:image":map[string]interface {}{}, "f:imagePullPolicy":map[string]interface {}{}, "f:name":map[string]interface {}{}, "f:ports":map[string]interface {}{".":map[string]interface {}{}, "k:{\"containerPort\":9963,\"protocol\":\"TCP\"}":map[string]interface {}{".":map[string]interface {}{}, "f:containerPort":map[string]interface {}{}, "f:hostPort":map[string]interface {}{}, "f:name":map[string]interface {}{}, "f:protocol":map[string]interface {}{}}}, "f:resources":map[string]interface {}{}, "f:securityContext":map[string]interface {}{".":map[string]interface {}{}, "f:allowPrivilegeEscalation":map[string]interface {}{}, "f:capabilities":map[string]interface {}{".":map[string]interface {}{}, "f:drop":map[string]interface {}{}}}, "f:terminationMessagePath":map[string]interface {}{}, "f:terminationMessagePolicy":map[string]interface {}{}, "f:volumeMounts":map[string]interface {}{".":map[string]interface {}{}, "k:{\"mountPath\":\"/tmp/cilium/config-map\"}":map[string]interface {}{".":map[string]interface {}{}, "f:mountPath":map[string]interface {}{}, "f:name":map[string]interface {}{}, "f:readOnly":map[string]interface {}{}}}}}, "f:dnsPolicy":map[string]interface {}{}, "f:hostNetwork":map[string]interface {}{}, "f:nodeSelector":map[string]interface {}{}, "f:priorityClassName":map[string]interface {}{}, "f:restartPolicy":map[string]interface {}{}, "f:schedulerName":map[string]interface {}{}, "f:securityContext":map[string]interface {}{".":map[string]interface {}{}, "f:seccompProfile":map[string]interface {}{".":map[string]interface {}{}, "f:type":map[string]interface {}{}}}, "f:serviceAccount":map[string]interface {}{}, "f:serviceAccountName":map[string]interface {}{}, "f:terminationGracePeriodSeconds":map[string]interface {}{}, "f:volumes":map[string]interface {}{".":map[string]interface {}{}, "k:{\"name\":\"cilium-config-path\"}":map[string]interface {}{".":map[string]interface {}{}, "f:configMap":map[string]interface {}{".":map[string]interface {}{}, "f:defaultMode":map[string]interface {}{}, "f:name":map[string]interface {}{}}, "f:name":map[string]interface {}{}}}}}}}, "manager":"helm", "operation":"Update", "time":"2025-10-25T01:13:25Z"}, map[string]interface {}{"apiVersion":"apps/v1", "fieldsType":"FieldsV1", "fieldsV1":map[string]interface {}{"f:spec":map[string]interface {}{"f:template":map[string]interface {}{"f:spec":map[string]interface {}{"f:tolerations":map[string]interface {}{}}}}}, "manager":"kubectl-patch", "operation":"Update", "time":"2025-10-25T01:43:35Z"}, map[string]interface {}{"apiVersion":"apps/v1", "fieldsType":"FieldsV1", "fieldsV1":map[string]interface {}{"f:metadata":map[string]interface {}{"f:annotations":map[string]interface {}{"f:deployment.kubernetes.io/revision":map[string]interface {}{}}}, "f:status":map[string]interface {}{"f:conditions":map[string]interface {}{".":map[string]interface {}{}, "k:{\"type\":\"Available\"}":map[string]interface {}{".":map[string]interface {}{}, "f:lastTransitionTime":map[string]interface {}{}, "f:lastUpdateTime":map[string]interface {}{}, "f:message":map[string]interface {}{}, "f:reason":map[string]interface {}{}, "f:status":map[string]interface {}{}, "f:type":map[string]interface {}{}}, "k:{\"type\":\"Progressing\"}":map[string]interface {}{".":map[string]interface {}{}, "f:lastTransitionTime":map[string]interface {}{}, "f:lastUpdateTime":map[string]interface {}{}, "f:message":map[string]interface {}{}, "f:reason":map[string]interface {}{}, "f:status":map[string]interface {}{}, "f:type":map[string]interface {}{}}}, "f:observedGeneration":map[string]interface {}{}, "f:replicas":map[string]interface {}{}, "f:unavailableReplicas":map[string]interface {}{}, "f:updatedReplicas":map[string]interface {}{}}}, "manager":"kube-controller-manager", "operation":"Update", "subresource":"status", "time":"2025-10-25T02:01:59Z"}}, "name":"cilium-operator", "namespace":"kube-system", "resourceVersion":"10429", "uid":"b600e265-d1e7-49db-93c1-0b667c48d84c"}, "spec":map[string]interface {}{"progressDeadlineSeconds":600, "replicas":2, "revisionHistoryLimit":10, "selector":map[string]interface {}{"matchLabels":map[string]interface {}{"io.cilium/app":"operator", "name":"cilium-operator"}}, "strategy":map[string]interface {}{"rollingUpdate":map[string]interface {}{"maxSurge":"25%", "maxUnavailable":"50%"}, "type":"RollingUpdate"}, "template":map[string]interface {}{"metadata":map[string]interface {}{"annotations":map[string]interface {}{"prometheus.io/port":"9963", "prometheus.io/scrape":"true"}, "labels":map[string]interface {}{"app.kubernetes.io/name":"cilium-operator", "app.kubernetes.io/part-of":"cilium", "io.cilium/app":"operator", "name":"cilium-operator"}}, "spec":map[string]interface {}{"affinity":map[string]interface {}{"podAntiAffinity":map[string]interface {}{"requiredDuringSchedulingIgnoredDuringExecution":[]interface {}{map[string]interface {}{"labelSelector":map[string]interface {}{"matchLabels":map[string]interface {}{"io.cilium/app":"operator"}}, "topologyKey":"kubernetes.io/hostname"}}}}, "automountServiceAccountToken":true, "containers":[]interface {}{map[string]interface {}{"args":[]interface {}{"--config-dir=/tmp/cilium/config-map", "--debug=$(CILIUM_DEBUG)"}, "command":[]interface {}{"cilium-operator-generic"}, "env":[]interface {}{map[string]interface {}{"name":"K8S_NODE_NAME", "valueFrom":map[string]interface {}{"fieldRef":map[string]interface {}{"apiVersion":"v1", "fieldPath":"spec.nodeName"}}}, map[string]interface {}{"name":"CILIUM_K8S_NAMESPACE", "valueFrom":map[string]interface {}{"fieldRef":map[string]interface {}{"apiVersion":"v1", "fieldPath":"metadata.namespace"}}}, map[string]interface {}{"name":"CILIUM_DEBUG", "valueFrom":map[string]interface {}{"configMapKeyRef":map[string]interface {}{"key":"debug", "name":"cilium-config", "optional":true}}}}, "image":"rancher/mirrored-cilium-operator-generic:v1.18.1", "imagePullPolicy":"IfNotPresent", "name":"cilium-operator", "ports":[]interface {}{map[string]interface {}{"containerPort":9963, "hostPort":9963, "name":"prometheus", "protocol":"TCP"}}, "resources":map[string]interface {}{}, "securityContext":map[string]interface {}{"allowPrivilegeEscalation":false, "capabilities":map[string]interface {}{"drop":[]interface {}{"ALL"}}}, "terminationMessagePath":"/dev/termination-log", "terminationMessagePolicy":"FallbackToLogsOnError", "volumeMounts":[]interface {}{map[string]interface {}{"mountPath":"/tmp/cilium/config-map", "name":"cilium-config-path", "readOnly":true}}}}, "dnsPolicy":"ClusterFirst", "hostNetwork":true, "nodeSelector":map[string]interface {}{"kubernetes.io/os":"linux"}, "priorityClassName":"system-cluster-critical", "restartPolicy":"Always", "schedulerName":"default-scheduler", "securityContext":map[string]interface {}{"seccompProfile":map[string]interface {}{"type":"RuntimeDefault"}}, "serviceAccount":"cilium-operator", "serviceAccountName":"cilium-operator", "terminationGracePeriodSeconds":30, "tolerations":[]interface {}{map[string]interface {}{"key":"CriticalAddonsOnly", "operator":"Exists"}, map[string]interface {}{"key":"node-role.kubernetes.io/control-plane", "operator":"Exists"}, map[string]interface {}{"key":"node-role.kubernetes.io/master", "operator":"Exists"}, map[string]interface {}{"key":"node-role.kubernetes.io/etcd", "operator":"Exists"}, map[string]interface {}{"key":"node.cilium.io/agent-not-ready", "operator":"Exists"}, map[string]interface {}{"key":"node.kubernetes.io/not-ready", "operator":"Exists"}, map[string]interface {}{"effect":"NoExecute", "key":"node.kubernetes.io/unreachable", "operator":"Exists", "tolerationSeconds":40}}, "volumes":[]interface {}{map[string]interface {}{"configMap":map[string]interface {}{"defaultMode":420, "name":"cilium-config"}, "name":"cilium-config-path"}}}}}, "status":map[string]interface {}{"conditions":[]interface {}{map[string]interface {}{"lastTransitionTime":"2025-10-25T01:13:25Z", "lastUpdateTime":"2025-10-25T01:13:25Z", "message":"Deployment does not have minimum availability.", "reason":"MinimumReplicasUnavailable", "status":"False", "type":"Available"}, map[string]interface {}{"lastTransitionTime":"2025-10-25T02:01:59Z", "lastUpdateTime":"2025-10-25T02:01:59Z", "message":"ReplicaSet \"cilium-operator-6d8db94895\" has timed out progressing.", "reason":"ProgressDeadlineExceeded", "status":"False", "type":"Progressing"}}, "observedGeneration":4, "replicas":3, "unavailableReplicas":3, "updatedReplicas":2}}


error: expected 'logs [-f] [-p] (POD | TYPE/NAME) [-c CONTAINER]'.
POD or TYPE/NAME is a required argument for the logs command
See 'kubectl logs -h' for help and examples
error: expected 'logs [-f] [-p] (POD | TYPE/NAME) [-c CONTAINER]'.
POD or TYPE/NAME is a required argument for the logs command
See 'kubectl logs -h' for help and examplesnon-zero return code

2025-10-25 09:08:21,951 p=3188108 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
-e 
===  ===
total 16
drwxr-x--- 5 ubuntu ubuntu  176 Oct 25 00:35 .
drwxr-xr-x 3 root   root     20 Oct 24 09:58 ..
drwx------ 3 ubuntu ubuntu   17 Oct 25 00:35 .ansible
-rw------- 1 ubuntu ubuntu   87 Oct 24 10:04 .bash_history
-rw-r--r-- 1 ubuntu ubuntu  220 Mar 31  2024 .bash_logout
-rw-r--r-- 1 ubuntu ubuntu 3771 Mar 31  2024 .bashrc
drwx------ 2 ubuntu ubuntu   34 Oct 24 09:59 .cache
-rw-r--r-- 1 ubuntu ubuntu  807 Mar 31  2024 .profile
-rw------- 1 ubuntu ubuntu    0 Oct 24 10:00 .python_history
drwx------ 2 ubuntu ubuntu   29 Oct 24 09:58 .ssh
-rw-r--r-- 1 ubuntu ubuntu    0 Oct 24 09:59 .sudo_as_admin_successful
-e 
===  ===
-e 
===  ===
-e 
===  ===
No CRD YAMLs found in chart.
-e 
===  ===
0

2025-10-25 11:40:07,862 p=3230976 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
cilium-2x4zg                                            0/1     Running             32 (17s ago)    172m
cilium-operator-6d8db94895-8lpmg                        0/1     Pending             0               168m
cilium-operator-6d8db94895-cdhqr                        0/1     Pending             0               168m
cilium-operator-f47b7f4f9-5mf24                         0/1     CrashLoopBackOff    47 (97s ago)    172m
helm-install-rke2-cilium-dt4kt                          0/1     Completed           0               3h26m
kube-system   rke2-cilium                                                               true        False

2025-10-25 11:43:43,547 p=3235770 u=root n=ansible | k8s-m1 | FAILED | rc=2 >>
bash: -c: line 1: syntax error near unexpected token `('
bash: -c: line 1: `set -e; K="/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml"; echo ==== NODES ====;  get nodes -o wide; echo ==== KUBE-SYSTEM PODS ====;  -n kube-system get pods -o wide; echo ==== HELMCHARTS kube-system ====;  -n kube-system get helmcharts.helm.cattle.io || true; echo ==== CRDs Cilium ====;  get crd | grep -i cilium || true; echo ==== EVENTS kube-system (last 50) ====;  -n kube-system get events --sort-by=.lastTimestamp | tail -n 50 || true'non-zero return code

2025-10-25 11:44:04,304 p=3236302 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
NAME     STATUS   ROLES                AGE     VERSION          INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-m1   Ready    control-plane,etcd   3h31m   v1.34.1+rke2r1   10.0.6.12     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2
====
NAME                                                    READY   STATUS              RESTARTS         AGE     IP          NODE     NOMINATED NODE   READINESS GATES
cilium-2x4zg                                            0/1     Running             32 (4m13s ago)   176m    10.0.6.12   k8s-m1   <none>           <none>
cilium-operator-6d8db94895-8lpmg                        0/1     Pending             0                172m    <none>      <none>   <none>           <none>
cilium-operator-6d8db94895-cdhqr                        0/1     Pending             0                172m    <none>      <none>   <none>           <none>
cilium-operator-f47b7f4f9-5mf24                         0/1     Running             48 (5m33s ago)   176m    10.0.6.12   k8s-m1   <none>           <none>
etcd-k8s-m1                                             1/1     Running             0                3h30m   10.0.6.12   k8s-m1   <none>           <none>
helm-install-rke2-cilium-dt4kt                          0/1     Completed           0                3h30m   10.0.6.12   k8s-m1   <none>           <none>
helm-install-rke2-coredns-7sb7t                         0/1     Completed           0                3h30m   10.0.6.12   k8s-m1   <none>           <none>
helm-install-rke2-ingress-nginx-hbvng                   0/1     Pending             0                3h30m   <none>      <none>   <none>           <none>
helm-install-rke2-runtimeclasses-z58c6                  0/1     Pending             0                3h30m   <none>      <none>   <none>           <none>
helm-install-rke2-snapshot-controller-crd-7k6h4         0/1     Pending             0                3h30m   <none>      <none>   <none>           <none>
helm-install-rke2-snapshot-controller-tqtbm             0/1     Pending             0                3h30m   <none>      <none>   <none>           <none>
kube-apiserver-k8s-m1                                   1/1     Running             0                3h30m   10.0.6.12   k8s-m1   <none>           <none>
kube-controller-manager-k8s-m1                          1/1     Running             1 (158m ago)     3h30m   10.0.6.12   k8s-m1   <none>           <none>
kube-scheduler-k8s-m1                                   1/1     Running             2 (158m ago)     3h30m   10.0.6.12   k8s-m1   <none>           <none>
kube-vip-ds-pmnqp                                       1/1     Running             11 (154m ago)    3h30m   10.0.6.12   k8s-m1   <none>           <none>
rke2-coredns-rke2-coredns-autoscaler-5fcf54974d-xwqrl   0/1     ContainerCreating   0                3h30m   <none>      k8s-m1   <none>           <none>
rke2-coredns-rke2-coredns-bd94c5787-q7h4v               0/1     ContainerCreating   0                3h30m   <none>      k8s-m1   <none>           <none>
====
NAME                           REPO   CHART   VERSION   TARGETNAMESPACE   BOOTSTRAP   FAILED
rke2-cilium                                                               true        False
rke2-coredns                                                              true        False
rke2-ingress-nginx                                                                    False
rke2-runtimeclasses                                                                   False
rke2-snapshot-controller                                                              False
rke2-snapshot-controller-crd                                                          False
====

2025-10-25 11:44:31,566 p=3236944 u=root n=ansible | k8s-m1 | FAILED | rc=2 >>
bash: -c: line 1: syntax error near unexpected token `('
bash: -c: line 1: `K="/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml"; echo === Helm install pods ===;  -n kube-system get pods | grep helm-install || true; echo === Describe first 3 helm-install pods (Events) ===;  -n kube-system get pods -o name | grep helm-install | head -n 3 | xargs -I{} sh -c " -n kube-system describe {} | sed -n "/Events:/,"" || true; echo === Cilium resources ===;  -n kube-system get ds,deploy,po | grep -i cilium || true; echo === Describe coredns pods (Events) ===;  -n kube-system get pods -o name | grep rke2-coredns | head -n 2 | xargs -I{} sh -c " -n kube-system describe {} | sed -n "/Events:/,"" || true'non-zero return code

2025-10-25 11:45:00,357 p=3237522 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
helm-install-rke2-cilium-dt4kt                          0/1     Completed           0                3h31m   10.0.6.12   k8s-m1   <none>           <none>
helm-install-rke2-coredns-7sb7t                         0/1     Completed           0                3h31m   10.0.6.12   k8s-m1   <none>           <none>
helm-install-rke2-ingress-nginx-hbvng                   0/1     Pending             0                3h31m   <none>      <none>   <none>           <none>
helm-install-rke2-runtimeclasses-z58c6                  0/1     Pending             0                3h31m   <none>      <none>   <none>           <none>
helm-install-rke2-snapshot-controller-crd-7k6h4         0/1     Pending             0                3h31m   <none>      <none>   <none>           <none>
helm-install-rke2-snapshot-controller-tqtbm             0/1     Pending             0                3h31m   <none>      <none>   <none>           <none>
----
Name:             helm-install-rke2-snapshot-controller-crd-7k6h4
Namespace:        kube-system
Priority:         0
Service Account:  helm-rke2-snapshot-controller-crd
Node:             <none>
Labels:           batch.kubernetes.io/controller-uid=3ae4234e-d05a-4372-9b09-0f8e979f3312
                  batch.kubernetes.io/job-name=helm-install-rke2-snapshot-controller-crd
                  controller-uid=3ae4234e-d05a-4372-9b09-0f8e979f3312
                  helmcharts.helm.cattle.io/chart=rke2-snapshot-controller-crd
                  job-name=helm-install-rke2-snapshot-controller-crd
Annotations:      helmcharts.helm.cattle.io/configHash: SHA256=BBA1F24B6D98383A90386F8C5FA6CE6293DEF54D74A4EA6256A6DCBA25585956
Status:           Pending
SeccompProfile:   RuntimeDefault
IP:               
IPs:              <none>
Controlled By:    Job/helm-install-rke2-snapshot-controller-crd
Containers:
  helm:
    Image:      rancher/klipper-helm:v0.9.8-build20250709
    Port:       <none>
    Host Port:  <none>
    Args:
      install
      --set-string
      global.clusterCIDR=10.42.0.0/16
      --set-string
      global.clusterCIDRv4=10.42.0.0/16
      --set-string
      global.clusterDNS=10.43.0.10
      --set-string
      global.clusterDomain=cluster.local
      --set-string
      global.rke2DataDir=/var/lib/rancher/rke2
      --set-string
      global.serviceCIDR=10.43.0.0/16
      --set-string
      global.systemDefaultIngressClass=ingress-nginx
    Environment:
      NAME:                      rke2-snapshot-controller-crd
      VERSION:                   
      REPO:                      
      HELM_DRIVER:               secret
      CHART_NAMESPACE:           kube-system
      CHART:                     
      HELM_VERSION:              
      TARGET_NAMESPACE:          kube-system
      AUTH_PASS_CREDENTIALS:     false
      INSECURE_SKIP_TLS_VERIFY:  false
      PLAIN_HTTP:                false
      NO_PROXY:                  .svc,.cluster.local,10.42.0.0/16,10.43.0.0/16
      FAILURE_POLICY:            reinstall
    Mounts:
      /chart from content (rw)
      /config from values (rw)
      /home/klipper-helm/.cache from klipper-cache (rw)
      /home/klipper-helm/.config from klipper-config (rw)
      /home/klipper-helm/.helm from klipper-helm (rw)
      /tmp from tmp (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-n2v4h (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  klipper-helm:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  klipper-cache:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  klipper-config:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  tmp:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     Memory
    SizeLimit:  <unset>
  values:
    Type:        Projected (a volume that contains injected data from multiple sources)
    SecretName:  chart-values-rke2-snapshot-controller-crd
    Optional:    false
  content:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      chart-content-rke2-snapshot-controller-crd
    Optional:  false
  kube-api-access-n2v4h:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 60s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 40s
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  14m (x30 over 159m)  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: true}. no new claims to deallocate, preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
----
NAME                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE     CONTAINERS     IMAGES                                   SELECTOR
daemonset.apps/cilium   1         1         0       1            0           kubernetes.io/os=linux   3h31m   cilium-agent   rancher/mirrored-cilium-cilium:v1.18.1   k8s-app=cilium

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS        IMAGES                                             SELECTOR
deployment.apps/cilium-operator   0/2     2            0           3h31m   cilium-operator   rancher/mirrored-cilium-operator-generic:v1.18.1   io.cilium/app=operator,name=cilium-operator
----
NAME           READY   STATUS             RESTARTS      AGE    IP          NODE     NOMINATED NODE   READINESS GATES
cilium-2x4zg   0/1     CrashLoopBackOff   32 (5s ago)   177m   10.0.6.12   k8s-m1   <none>           <none>

2025-10-25 11:45:30,469 p=3238202 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
time=2025-10-25T04:45:02.531950024Z level=info msg="  --config=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53195631Z level=info msg="  --config-dir='/tmp/cilium/config-map'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.531983366Z level=info msg="  --controller-group-metrics=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.531990981Z level=info msg="  --custom-cni-conf='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.531997132Z level=info msg="  --datapath-mode='veth'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532003574Z level=info msg="  --debug='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532010658Z level=info msg="  --debug-verbose=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532017099Z level=info msg="  --default-lb-service-ipam='lbipam'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532023291Z level=info msg="  --direct-routing-skip-unreachable='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532061647Z level=info msg="  --disable-endpoint-crd='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532068649Z level=info msg="  --dnsproxy-socket-linger-timeout='10'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532076016Z level=info msg="  --double-write-metric-reporter-interval='1m0s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532082696Z level=info msg="  --egress-gateway-reconciliation-trigger-interval='1s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53208902Z level=info msg="  --enable-auto-protect-node-port-range='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532095263Z level=info msg="  --enable-bpf-clock-probe='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532101929Z level=info msg="  --enable-cilium-clusterwide-network-policy='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532123437Z level=info msg="  --enable-cilium-endpoint-slice='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532130919Z level=info msg="  --enable-cilium-network-policy='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532154513Z level=info msg="  --enable-cilium-operator-server-access='*'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532164738Z level=info msg="  --enable-egress-gateway='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532171199Z level=info msg="  --enable-endpoint-health-checking='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532177434Z level=info msg="  --enable-endpoint-lockdown-on-policy-overflow='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532184101Z level=info msg="  --enable-gateway-api-alpn='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532190492Z level=info msg="  --enable-gateway-api-app-protocol='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532197094Z level=info msg="  --enable-gateway-api-proxy-protocol='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53220348Z level=info msg="  --enable-gateway-api-secrets-sync='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532210208Z level=info msg="  --enable-gops='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532216358Z level=info msg="  --enable-health-check-loadbalancer-ip='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532222596Z level=info msg="  --enable-health-check-nodeport='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532228588Z level=info msg="  --enable-health-checking='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532234647Z level=info msg="  --enable-host-legacy-routing='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532240754Z level=info msg="  --enable-hubble='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532246897Z level=info msg="  --enable-icmp-rules='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532253231Z level=info msg="  --enable-ingress-controller='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532259552Z level=info msg="  --enable-ingress-proxy-protocol='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532265938Z level=info msg="  --enable-ingress-secrets-sync='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532272071Z level=info msg="  --enable-internal-traffic-policy='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532278381Z level=info msg="  --enable-ipv4='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532284369Z level=info msg="  --enable-ipv4-big-tcp='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532290592Z level=info msg="  --enable-ipv4-egress-gateway='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532296858Z level=info msg="  --enable-ipv4-masquerade='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532310578Z level=info msg="  --enable-ipv6='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53231674Z level=info msg="  --enable-ipv6-big-tcp='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532332732Z level=info msg="  --enable-ipv6-masquerade='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532340659Z level=info msg="  --enable-k8s='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53234653Z level=info msg="  --enable-k8s-api-discovery='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532353026Z level=info msg="  --enable-k8s-endpoint-slice='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532359389Z level=info msg="  --enable-k8s-networkpolicy='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532365528Z level=info msg="  --enable-l2-neigh-discovery='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532371721Z level=info msg="  --enable-l7-proxy='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532377894Z level=info msg="  --enable-lb-ipam='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.5323843Z level=info msg="  --enable-local-redirect-policy='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532390677Z level=info msg="  --enable-masquerade-to-route-source='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53239691Z level=info msg="  --enable-metrics='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532404887Z level=info msg="  --enable-node-ipam='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532411062Z level=info msg="  --enable-node-port='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532417177Z level=info msg="  --enable-node-selector-labels='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532423428Z level=info msg="  --enable-non-default-deny-policies='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532429536Z level=info msg="  --enable-policy='default'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532435694Z level=info msg="  --enable-policy-secrets-sync='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532441785Z level=info msg="  --enable-sctp='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532447718Z level=info msg="  --enable-source-ip-verification='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532454069Z level=info msg="  --enable-srv6='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532460303Z level=info msg="  --enable-svc-source-range-check='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532466366Z level=info msg="  --enable-tcx='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532472189Z level=info msg="  --enable-vtep='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532508769Z level=info msg="  --enable-well-known-identities='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532518723Z level=info msg="  --enable-xt-socket-fallback='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532526128Z level=info msg="  --enforce-ingress-https='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532532202Z level=info msg="  --envoy-access-log-buffer-size='4096'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532538413Z level=info msg="  --envoy-base-id='0'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53254437Z level=info msg="  --envoy-keep-cap-netbindservice='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53255044Z level=info msg="  --external-envoy-proxy='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532557016Z level=info msg="  --gateway-api-hostnetwork-enabled='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53256623Z level=info msg="  --gateway-api-hostnetwork-nodelabelselector=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532573384Z level=info msg="  --gateway-api-secrets-namespace='cilium-secrets'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532580136Z level=info msg="  --gateway-api-service-externaltrafficpolicy='Cluster'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532587013Z level=info msg="  --gateway-api-xff-num-trusted-hops='0'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532594006Z level=info msg="  --gops-port='9891'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53260014Z level=info msg="  --health-check-icmp-failure-threshold='3'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532606291Z level=info msg="  --http-retry-count='3'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53261255Z level=info msg="  --identity-allocation-mode='crd'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532618871Z level=info msg="  --identity-gc-interval='15m0s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53262559Z level=info msg="  --identity-gc-rate-interval='1m0s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532632872Z level=info msg="  --identity-gc-rate-limit='2500'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53263907Z level=info msg="  --identity-heartbeat-timeout='30m0s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532645339Z level=info msg="  --identity-management-mode='agent'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532652046Z level=info msg="  --ingress-default-lb-mode='dedicated'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532658611Z level=info msg="  --ingress-default-request-timeout='0s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532666533Z level=info msg="  --ingress-default-secret-name=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532673967Z level=info msg="  --ingress-default-secret-namespace=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532680593Z level=info msg="  --ingress-default-xff-num-trusted-hops='0'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532687111Z level=info msg="  --ingress-hostnetwork-enabled='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532694775Z level=info msg="  --ingress-hostnetwork-nodelabelselector=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53270179Z level=info msg="  --ingress-hostnetwork-shared-listener-port='0'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532729158Z level=info msg="  --ingress-lb-annotation-prefixes='lbipam.cilium.io,service.beta.kubernetes.io,service.kubernetes.io,cloud.google.com'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532738261Z level=info msg="  --ingress-secrets-namespace='cilium-secrets'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532744844Z level=info msg="  --ingress-shared-lb-service-name='cilium-ingress'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532753346Z level=info msg="  --install-no-conntrack-iptables-rules='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532761634Z level=info msg="  --instance-tags-filter=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532768213Z level=info msg="  --ipam='kubernetes'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53277425Z level=info msg="  --ipam-cilium-node-update-rate='15s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532780335Z level=info msg="  --iptables-random-fully='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532787962Z level=info msg="  --k8s-api-server=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532815371Z level=info msg="  --k8s-api-server-urls=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532823908Z level=info msg="  --k8s-client-connection-keep-alive='30s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532830537Z level=info msg="  --k8s-client-connection-timeout='30s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532847974Z level=info msg="  --k8s-heartbeat-timeout='30s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532856624Z level=info msg="  --k8s-kubeconfig-path=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532863231Z level=info msg="  --k8s-namespace='kube-system'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532869305Z level=info msg="  --k8s-require-ipv4-pod-cidr='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532875275Z level=info msg="  --k8s-require-ipv6-pod-cidr='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532883132Z level=info msg="  --k8s-service-proxy-name=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532889734Z level=info msg="  --kube-proxy-replacement='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532897166Z level=info msg="  --kvstore=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53290376Z level=info msg="  --kvstore-lease-ttl='15m0s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532910476Z level=info msg="  --kvstore-max-consecutive-quorum-errors='2'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532950924Z level=info msg="  --kvstore-opt=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.532959313Z level=info msg="  --leader-election-lease-duration='15s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533074618Z level=info msg="  --leader-election-renew-deadline='10s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533090583Z level=info msg="  --leader-election-retry-period='2s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533101111Z level=info msg="  --limit-ipam-api-burst='20'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53310842Z level=info msg="  --limit-ipam-api-qps='4'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533117708Z level=info msg="  --loadbalancer-l7=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533124489Z level=info msg="  --loadbalancer-l7-algorithm='round_robin'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533162568Z level=info msg="  --loadbalancer-l7-ports=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533220791Z level=info msg="  --log-driver=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533233729Z level=info msg="  --log-opt=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533240498Z level=info msg="  --max-connected-clusters='255'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533246836Z level=info msg="  --mesh-auth-enabled='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533252825Z level=info msg="  --mesh-auth-gc-interval='5m0s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533259918Z level=info msg="  --mesh-auth-mutual-enabled='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533266147Z level=info msg="  --mesh-auth-queue-size='1024'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533272372Z level=info msg="  --mesh-auth-rotated-identities-queue-size='1024'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533279389Z level=info msg="  --mesh-auth-spiffe-trust-domain='spiffe.cilium'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533286227Z level=info msg="  --mesh-auth-spire-agent-socket='/run/spire/sockets/agent/agent.sock'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533293168Z level=info msg="  --mesh-auth-spire-server-address='spire-server.spire.svc:8081'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533300722Z level=info msg="  --mesh-auth-spire-server-connection-timeout='10s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533307037Z level=info msg="  --metrics-sampling-interval='5m'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533313084Z level=info msg="  --monitor-aggregation='medium'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533319202Z level=info msg="  --monitor-aggregation-flags='all'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533325265Z level=info msg="  --monitor-aggregation-interval='5s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533337745Z level=info msg="  --nat-map-stats-entries='32'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533343755Z level=info msg="  --nat-map-stats-interval='30s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533349822Z level=info msg="  --node-port-bind-protection='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533357236Z level=info msg="  --nodeport-addresses=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533363389Z level=info msg="  --nodes-gc-interval='5m0s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533369777Z level=info msg="  --operator-api-serve-addr='127.0.0.1:9234'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533377521Z level=info msg="  --operator-k8s-client-burst='200'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533385086Z level=info msg="  --operator-k8s-client-qps='100'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533392013Z level=info msg="  --operator-pprof='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533398706Z level=info msg="  --operator-pprof-address='localhost'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533405346Z level=info msg="  --operator-pprof-port='6061'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533411667Z level=info msg="  --operator-prometheus-serve-addr=':9963'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53341845Z level=info msg="  --parallel-alloc-workers='50'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533425092Z level=info msg="  --pod-restart-selector='k8s-app=kube-dns'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533432403Z level=info msg="  --policy-cidr-match-mode=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533438696Z level=info msg="  --policy-default-local-cluster='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533445054Z level=info msg="  --policy-secrets-namespace='cilium-secrets'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533453795Z level=info msg="  --policy-secrets-only-from-secrets-namespace='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533460087Z level=info msg="  --preallocate-bpf-maps='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533466118Z level=info msg="  --procfs='/host/proc'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533472023Z level=info msg="  --proxy-connect-timeout='2'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533478017Z level=info msg="  --proxy-idle-timeout-seconds='60'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53348411Z level=info msg="  --proxy-initial-fetch-timeout='30'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533490142Z level=info msg="  --proxy-max-concurrent-retries='128'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.5334962Z level=info msg="  --proxy-max-connection-duration-seconds='0'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533502384Z level=info msg="  --proxy-max-requests-per-connection='0'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533508368Z level=info msg="  --proxy-prometheus-port='9964'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533514348Z level=info msg="  --proxy-xff-num-trusted-hops-egress='0'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533520297Z level=info msg="  --proxy-xff-num-trusted-hops-ingress='0'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533526692Z level=info msg="  --remove-cilium-node-taints='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533532874Z level=info msg="  --routing-mode='tunnel'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533538834Z level=info msg="  --service-no-backend-response='reject'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53354506Z level=info msg="  --set-cilium-is-up-condition='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533551336Z level=info msg="  --set-cilium-node-taints='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533557898Z level=info msg="  --skip-crd-creation='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533584252Z level=info msg="  --subnet-ids-filter=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53364999Z level=info msg="  --subnet-tags-filter=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533660483Z level=info msg="  --synchronize-k8s-nodes='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533667505Z level=info msg="  --synchronize-k8s-services='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533674711Z level=info msg="  --taint-sync-workers='10'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533681365Z level=info msg="  --tofqdns-dns-reject-response-code='refused'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533687582Z level=info msg="  --tofqdns-enable-dns-compression='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533693913Z level=info msg="  --tofqdns-endpoint-max-ip-per-hostname='1000'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533700171Z level=info msg="  --tofqdns-idle-connection-grace-period='0s'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533706559Z level=info msg="  --tofqdns-max-deferred-connection-deletes='10000'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533712722Z level=info msg="  --tofqdns-preallocate-identities='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533718898Z level=info msg="  --tofqdns-proxy-response-max-delay='100ms'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533725061Z level=info msg="  --tunnel-protocol='vxlan'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533731039Z level=info msg="  --tunnel-source-port-range='0-0'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533737343Z level=info msg="  --unmanaged-pod-watcher-interval='15'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533744069Z level=info msg="  --validate-network-policy='true'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53375054Z level=info msg="  --version='false'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533757457Z level=info msg="  --vtep-cidr=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533804184Z level=info msg="  --vtep-endpoint=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53387706Z level=info msg="  --vtep-mac=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533889095Z level=info msg="  --vtep-mask=''" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.53389603Z level=info msg="  --write-cni-conf-when-ready='/host/etc/cni/net.d/05-cilium.conflist'" subsys=cilium-operator-generic
time=2025-10-25T04:45:02.533903053Z level=info msg="Cilium Operator" subsys=cilium-operator-generic version="1.18.1 e8a7070f 2025-08-13T14:47:02+00:00 go version go1.24.6 linux/amd64"
----
time=2025-10-25T04:41:58.102874566Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:41:59.100919126Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:42:00.100842511Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T04:42:01.102038881Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T04:42:02.102511217Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T04:42:03.101761705Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:04.100886324Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:42:05.101221992Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T04:42:06.101127664Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:07.101133841Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T04:42:08.101367999Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:09.10097785Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:10.100940436Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T04:42:11.100966936Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:12.101406662Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:42:13.100777601Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:14.101251432Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:15.100697594Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T04:42:16.102234728Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:42:17.100649037Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:18.102470863Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T04:42:19.102162881Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:20.101695709Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:42:21.102350754Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T04:42:22.101958406Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:23.101012327Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:24.100923793Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:42:25.101034711Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:26.101685917Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T04:42:27.101007617Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T04:42:28.102306217Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:42:29.101052298Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T04:42:30.102219598Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:31.101389615Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T04:42:32.102008967Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:42:33.102017925Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T04:42:34.102476228Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T04:42:35.100774977Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T04:42:36.102394017Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:37.100637707Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:42:38.102289632Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:39.100691261Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:40.101991512Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T04:42:41.101456104Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T04:42:42.101178962Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T04:42:43.101887446Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:44.102026591Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T04:42:45.102469728Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:42:46.101863349Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:47.102491511Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T04:42:48.101814446Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T04:42:49.102628432Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:42:50.101556089Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:42:51.101337478Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:52.102597173Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T04:42:53.102438049Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:54.102123716Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:55.101562958Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T04:42:56.100819845Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:57.100942663Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T04:42:58.101517323Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:42:59.100745317Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:00.101337513Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:01.10206462Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:43:02.10117612Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T04:43:03.102100594Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:04.102483389Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:43:05.102530714Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T04:43:06.100709206Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:43:07.102236861Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:08.101403098Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T04:43:09.10135356Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T04:43:10.102294597Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:43:11.102569398Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T04:43:12.100803777Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:13.102214493Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:43:14.102674613Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:15.101050684Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:16.101408933Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:17.10076806Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T04:43:18.102022845Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:19.102075995Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:20.102150026Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:43:21.100652056Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:22.101248477Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:23.10111238Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:24.100698058Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:25.101086565Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:43:26.102266533Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:27.100943025Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T04:43:28.100716808Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T04:43:29.100978316Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T04:43:30.100869111Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T04:43:31.102590638Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T04:43:32.100994484Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T04:43:33.102528414Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:43:34.101182458Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:43:35.10125335Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:36.101550946Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:43:37.101278188Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:38.10168239Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T04:43:39.101962837Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T04:43:40.102179452Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:41.102281619Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:42.101581226Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T04:43:43.101778214Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:44.101864139Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:43:45.101994083Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T04:43:46.10232971Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:47.101093618Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T04:43:48.101045321Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:49.102097783Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T04:43:50.100929639Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:43:51.10246611Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T04:43:52.100832257Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T04:43:53.101438698Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T04:43:54.10085742Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T04:43:55.10084858Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:56.100725352Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:57.10203479Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T04:43:58.100731557Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:43:59.10255912Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:00.102063603Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:01.10216909Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:44:02.102378717Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T04:44:03.102707447Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:44:04.10129804Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:05.101712883Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:06.102183835Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:07.100731202Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T04:44:08.10063818Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:44:09.101719363Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:44:10.101756573Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T04:44:11.100863271Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:12.101314316Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T04:44:13.10181735Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T04:44:14.100632098Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T04:44:15.101663312Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:44:16.100649153Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:17.101013634Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io]"
time=2025-10-25T04:44:18.10225754Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T04:44:19.101092157Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:44:20.101965755Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:21.102192962Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:22.100834249Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:23.101116868Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:24.101412525Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T04:44:25.101686283Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:44:26.100901235Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:27.100802267Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:28.100653669Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:29.101114011Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:30.10154256Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:31.103364518Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T04:44:32.100677043Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T04:44:33.100958718Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:34.101064162Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:35.100727114Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T04:44:36.102558935Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io]"
time=2025-10-25T04:44:37.100933744Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:38.100636423Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:39.101316327Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T04:44:40.100651233Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io]"
time=2025-10-25T04:44:41.100695287Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:42.101854714Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:43.102153003Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:44:44.101122991Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io]"
time=2025-10-25T04:44:45.102215393Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io]"
time=2025-10-25T04:44:46.101411452Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:47.101174077Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io]"
time=2025-10-25T04:44:48.10301693Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T04:44:49.101261874Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:50.102432965Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:51.101193405Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io]"
time=2025-10-25T04:44:52.100966203Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io]"
time=2025-10-25T04:44:53.101078686Z level=info msg="Still waiting for Cilium Operator to register CRDs" module=agent.infra.k8s-synced-crdsync CRDs="[crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io]"
time=2025-10-25T04:44:53.854933897Z level=info msg="no local ciliumnode found, will not restore cilium internal and health ips from k8s" module=agent.controlplane
time=2025-10-25T04:44:53.855232092Z level=info msg="Local boot ID" module=agent.controlplane bootID=27a25b19-18b1-41d1-a68f-f0bc7d016c72
time=2025-10-25T04:44:53.855393876Z level=info msg="Start hook executed" function="node.NewLocalNodeStore.func1 (pkg/node/local_node_store.go:139) (agent.controlplane.local-node-store)" duration=4m59.755199343s
time=2025-10-25T04:44:53.856316237Z level=info msg="Serving cilium node monitor v1.2 API at unix:///var/run/cilium/monitor1_2.sock" module=agent.datapath.monitor-agent
time=2025-10-25T04:44:53.861305899Z level=info msg="Generating CNI configuration file with mode" module=agent.infra.cni-config mode=portmap
time=2025-10-25T04:44:53.861500615Z level=error msg="Command execution failed" module=agent.datapath.iptables error="context deadline exceeded" cmd="[iptables --version]"
time=2025-10-25T04:44:53.896290168Z level=info msg="Restoring proxy ports from file failed, falling back to restoring from iptables rules" module=agent.controlplane.l7-proxy file-path=/var/run/cilium/state/proxy_ports_state.json error="stat /var/run/cilium/state/proxy_ports_state.json: no such file or directory"
time=2025-10-25T04:44:53.896505753Z level=info msg="Envoy: Starting access log server listening" module=agent.controlplane.envoy-proxy address=/var/run/cilium/envoy/sockets/access_log.sock
time=2025-10-25T04:44:53.896675702Z level=info msg="Envoy: Waiting for endpoint restorer before serving xDS resources..." module=agent.controlplane.envoy-proxy
time=2025-10-25T04:44:53.909214798Z level=info msg="Datapath signal listener running" module=agent.controlplane.signal
time=2025-10-25T04:44:53.909298109Z level=info msg="Restored node IDs from the BPF map" module=agent.datapath count=0
time=2025-10-25T04:44:53.909384579Z level=error msg="Start hook failed" function="*hive.fence.Start (agent.controlplane.endpoint-regeneration)" error="context deadline exceeded"
time=2025-10-25T04:44:53.909401533Z level=error msg="Failed to start hive" error="context deadline exceeded" duration=4m59.81857033s
time=2025-10-25T04:44:53.909420005Z level=info msg="Stopping hive"
time=2025-10-25T04:44:53.909845114Z level=info msg="Using discoveryv1.EndpointSlice" module=agent.controlplane.k8s-resources
time=2025-10-25T04:44:53.91068965Z level=error msg="failed to delete reporter status tree" module=health error="reporting for \"agent.controlplane.bgp-control-plane.job-diffstore-events\" has been stopped"
time=2025-10-25T04:44:53.911126171Z level=error msg="deletionQueue: restorer promise failed" module=agent.controlplane.endpoint-api error="context canceled"
time=2025-10-25T04:44:53.911957413Z level=info msg="Datapath signal listener exiting" module=agent.controlplane.signal
time=2025-10-25T04:44:53.912007048Z level=info msg="Datapath signal listener done" module=agent.controlplane.signal
time=2025-10-25T04:44:54.878407686Z level=info msg="Stop hook executed" function="*job.queuedJob.Stop (agent.datapath.iptables)" duration=965.937994ms
time=2025-10-25T04:44:54.879235676Z level=error msg="Failed to get endpoint restorer" module=agent.controlplane.fqdn.namemanager error="context canceled"
time=2025-10-25T04:44:54.880613914Z level=error msg="Stop hook failed" function="cell.newIPCache.func1 (.../ipcache/cell/cell.go:71) (agent.controlplane.ipcache)" error="unable to find controller ipcache-inject-labels"
time=2025-10-25T04:44:54.880736796Z level=error msg="Close() called without calling InitIdentityAllocator() first" module=agent.controlplane.identity.identity
time=2025-10-25T04:44:56.101480504Z level=fatal msg="Unable to find all Cilium CRDs necessary within 5m0s timeout. Please ensure that Cilium Operator is running, as it's responsible for registering all the Cilium CRDs. The following CRDs were not found: [crd:ciliumnetworkpolicies.cilium.io crd:ciliumclusterwidenetworkpolicies.cilium.io crd:ciliumnodes.cilium.io crd:ciliumidentities.cilium.io crd:ciliumpodippools.cilium.io crd:ciliumcidrgroups.cilium.io crd:ciliumloadbalancerippools.cilium.io crd:ciliuml2announcementpolicies.cilium.io crd:ciliumendpoints.cilium.io]" module=agent.infra.k8s-synced-crdsync error="context deadline exceeded"Found 3 pods, using pod/cilium-operator-f47b7f4f9-5mf24

2025-10-25 11:45:42,301 p=3238502 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
5
----

2025-10-25 12:56:56,526 p=3264774 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ********************
2025-10-25 12:56:59,081 p=3264774 u=root n=ansible | TASK [Gathering Facts] *************************************************************************
2025-10-25 12:56:59,083 p=3264774 u=root n=ansible | ok: [k8s-m1]
2025-10-25 12:56:59,164 p=3264774 u=root n=ansible | ok: [k8s-w01]
2025-10-25 12:56:59,202 p=3264774 u=root n=ansible | ok: [k8s-m3]
2025-10-25 12:56:59,257 p=3264774 u=root n=ansible | ok: [k8s-w02]
2025-10-25 12:56:59,284 p=3264774 u=root n=ansible | ok: [k8s-m2]
2025-10-25 12:57:02,312 p=3264774 u=root n=ansible | ok: [k8s-w03]
2025-10-25 12:57:02,477 p=3264774 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 12:57:02,478 p=3264774 u=root n=ansible | ok: [k8s-m2]
2025-10-25 12:57:02,487 p=3264774 u=root n=ansible | ok: [k8s-m3]
2025-10-25 12:57:02,488 p=3264774 u=root n=ansible | ok: [k8s-m1]
2025-10-25 12:57:02,504 p=3264774 u=root n=ansible | ok: [k8s-w01]
2025-10-25 12:57:02,528 p=3264774 u=root n=ansible | ok: [k8s-w02]
2025-10-25 12:57:02,536 p=3264774 u=root n=ansible | ok: [k8s-w03]
2025-10-25 12:57:03,751 p=3264774 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] **************************************
2025-10-25 12:57:03,752 p=3264774 u=root n=ansible | changed: [k8s-m3]
2025-10-25 12:57:03,754 p=3264774 u=root n=ansible | changed: [k8s-m2]
2025-10-25 12:57:03,756 p=3264774 u=root n=ansible | changed: [k8s-w02]
2025-10-25 12:57:03,758 p=3264774 u=root n=ansible | changed: [k8s-w01]
2025-10-25 12:57:03,761 p=3264774 u=root n=ansible | changed: [k8s-m1]
2025-10-25 12:57:04,657 p=3264774 u=root n=ansible | changed: [k8s-w03]
2025-10-25 12:57:04,833 p=3264774 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] **********************************
2025-10-25 12:57:04,978 p=3264774 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 12:57:06,732 p=3264774 u=root n=ansible | TASK [rke2-1.49.0 : Create RKE2 artifacts folder] **********************************************
2025-10-25 12:57:06,733 p=3264774 u=root n=ansible | changed: [k8s-m1]
2025-10-25 12:57:06,735 p=3264774 u=root n=ansible | changed: [k8s-w02]
2025-10-25 12:57:06,735 p=3264774 u=root n=ansible | changed: [k8s-m2]
2025-10-25 12:57:06,736 p=3264774 u=root n=ansible | changed: [k8s-m3]
2025-10-25 12:57:06,736 p=3264774 u=root n=ansible | changed: [k8s-w01]
2025-10-25 12:57:07,094 p=3264774 u=root n=ansible | changed: [k8s-w03]
2025-10-25 12:57:08,036 p=3264774 u=root n=ansible | TASK [rke2-1.49.0 : Download sha256 checksum file ( airgap mode )] *****************************
2025-10-25 12:57:08,037 p=3264774 u=root n=ansible | [WARNING]: Module remote_tmp /root/.ansible/tmp did not exist and was created with a mode of
0700, this may cause issues when running as another user. To avoid this, create the remote_tmp
dir with the correct permissions manually

2025-10-25 12:57:08,037 p=3264774 u=root n=ansible | changed: [k8s-w02]
2025-10-25 12:57:08,038 p=3264774 u=root n=ansible | changed: [k8s-m2]
2025-10-25 12:57:08,044 p=3264774 u=root n=ansible | changed: [k8s-m1]
2025-10-25 12:57:08,045 p=3264774 u=root n=ansible | changed: [k8s-m3]
2025-10-25 12:57:08,049 p=3264774 u=root n=ansible | changed: [k8s-w01]
2025-10-25 12:57:08,573 p=3264774 u=root n=ansible | changed: [k8s-w03]
2025-10-25 12:57:10,137 p=3264774 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 artifacts and compare with checksums ( airgap mode )] ********
2025-10-25 12:57:10,137 p=3264774 u=root n=ansible | changed: [k8s-m2] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 12:57:10,223 p=3264774 u=root n=ansible | changed: [k8s-w02] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 12:57:10,319 p=3264774 u=root n=ansible | changed: [k8s-m1] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 12:57:10,330 p=3264774 u=root n=ansible | changed: [k8s-w01] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 12:57:10,375 p=3264774 u=root n=ansible | changed: [k8s-m3] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 12:57:32,962 p=3264774 u=root n=ansible | changed: [k8s-m2] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 12:57:33,090 p=3264774 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 12:57:34,274 p=3264774 u=root n=ansible | changed: [k8s-m3] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 12:57:35,286 p=3264774 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 12:57:47,367 p=3264774 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 12:57:48,542 p=3264774 u=root n=ansible | changed: [k8s-w03] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 12:57:48,561 p=3264774 u=root n=ansible | changed: [k8s-m2] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 12:57:48,888 p=3264774 u=root n=ansible | changed: [k8s-m3] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 12:57:49,734 p=3264774 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 12:58:13,639 p=3264774 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 12:58:23,635 p=3264774 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 12:58:24,181 p=3264774 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 12:58:35,298 p=3264774 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 12:58:35,868 p=3264774 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 install script ( airgap mode )] ******************************
2025-10-25 12:58:35,869 p=3264774 u=root n=ansible | changed: [k8s-m1]
2025-10-25 12:58:35,903 p=3264774 u=root n=ansible | changed: [k8s-w01]
2025-10-25 12:58:35,908 p=3264774 u=root n=ansible | changed: [k8s-m2]
2025-10-25 12:58:35,919 p=3264774 u=root n=ansible | changed: [k8s-m3]
2025-10-25 12:58:35,923 p=3264774 u=root n=ansible | changed: [k8s-w02]
2025-10-25 12:58:36,384 p=3264774 u=root n=ansible | changed: [k8s-w03]
2025-10-25 12:58:42,649 p=3264774 u=root n=ansible | TASK [rke2-1.49.0 : Populate service facts] ****************************************************
2025-10-25 12:58:42,649 p=3264774 u=root n=ansible | ok: [k8s-w01]
2025-10-25 12:58:42,687 p=3264774 u=root n=ansible | ok: [k8s-w02]
2025-10-25 12:58:42,717 p=3264774 u=root n=ansible | ok: [k8s-m2]
2025-10-25 12:58:42,745 p=3264774 u=root n=ansible | ok: [k8s-m1]
2025-10-25 12:58:42,752 p=3264774 u=root n=ansible | ok: [k8s-m3]
2025-10-25 12:58:48,390 p=3264774 u=root n=ansible | ok: [k8s-w03]
2025-10-25 12:58:48,894 p=3264774 u=root n=ansible | TASK [rke2-1.49.0 : Get stats of the FS object] ************************************************
2025-10-25 12:58:48,897 p=3264774 u=root n=ansible | ok: [k8s-m1]
2025-10-25 12:58:48,900 p=3264774 u=root n=ansible | ok: [k8s-w01]
2025-10-25 12:58:48,911 p=3264774 u=root n=ansible | ok: [k8s-m3]
2025-10-25 12:58:48,911 p=3264774 u=root n=ansible | ok: [k8s-m2]
2025-10-25 12:58:48,912 p=3264774 u=root n=ansible | ok: [k8s-w02]
2025-10-25 12:58:49,255 p=3264774 u=root n=ansible | ok: [k8s-w03]
2025-10-25 12:58:49,768 p=3264774 u=root n=ansible | TASK [rke2-1.49.0 : Check if separate partition] ***********************************************
2025-10-25 12:58:49,769 p=3264774 u=root n=ansible | ok: [k8s-m2]
2025-10-25 12:58:49,774 p=3264774 u=root n=ansible | ok: [k8s-w01]
2025-10-25 12:58:49,775 p=3264774 u=root n=ansible | ok: [k8s-w02]
2025-10-25 12:58:49,778 p=3264774 u=root n=ansible | ok: [k8s-m3]
2025-10-25 12:58:49,784 p=3264774 u=root n=ansible | ok: [k8s-m1]
2025-10-25 12:58:50,105 p=3264774 u=root n=ansible | ok: [k8s-w03]
2025-10-25 12:58:50,181 p=3264774 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 bin path] *********************************************************
2025-10-25 12:58:50,181 p=3264774 u=root n=ansible | ok: [k8s-m1]
2025-10-25 12:58:50,206 p=3264774 u=root n=ansible | ok: [k8s-m2]
2025-10-25 12:58:50,270 p=3264774 u=root n=ansible | ok: [k8s-m3]
2025-10-25 12:58:50,291 p=3264774 u=root n=ansible | ok: [k8s-w01]
2025-10-25 12:58:50,295 p=3264774 u=root n=ansible | ok: [k8s-w02]
2025-10-25 12:58:50,313 p=3264774 u=root n=ansible | ok: [k8s-w03]
2025-10-25 12:58:50,745 p=3264774 u=root n=ansible | TASK [rke2-1.49.0 : Check RKE2 version] ********************************************************
2025-10-25 12:58:50,746 p=3264774 u=root n=ansible | ok: [k8s-m2]
2025-10-25 12:58:50,748 p=3264774 u=root n=ansible | ok: [k8s-m1]
2025-10-25 12:58:50,749 p=3264774 u=root n=ansible | ok: [k8s-m3]
2025-10-25 12:58:50,755 p=3264774 u=root n=ansible | ok: [k8s-w01]
2025-10-25 12:58:50,786 p=3264774 u=root n=ansible | ok: [k8s-w02]
2025-10-25 12:58:51,117 p=3264774 u=root n=ansible | ok: [k8s-w03]
2025-10-25 12:58:51,215 p=3264774 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 versions] *********************************************************
2025-10-25 12:58:51,216 p=3264774 u=root n=ansible | ok: [k8s-m1]
2025-10-25 12:58:51,249 p=3264774 u=root n=ansible | ok: [k8s-m2]
2025-10-25 12:58:51,276 p=3264774 u=root n=ansible | ok: [k8s-m3]
2025-10-25 12:58:51,277 p=3264774 u=root n=ansible | ok: [k8s-w01]
2025-10-25 12:58:51,290 p=3264774 u=root n=ansible | ok: [k8s-w02]
2025-10-25 12:58:51,318 p=3264774 u=root n=ansible | ok: [k8s-w03]
2025-10-25 12:59:38,787 p=3264774 u=root n=ansible | TASK [rke2-1.49.0 : Run RKE2 install script with airgap variables] *****************************
2025-10-25 12:59:38,788 p=3264774 u=root n=ansible | ok: [k8s-m3]
2025-10-25 12:59:43,704 p=3264774 u=root n=ansible | ok: [k8s-m2]
2025-10-25 12:59:49,269 p=3264774 u=root n=ansible | ok: [k8s-m1]
2025-10-25 12:59:58,739 p=3264774 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:00:00,918 p=3264774 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:01:03,071 p=3264774 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:01:03,848 p=3264774 u=root n=ansible | TASK [rke2-1.49.0 : Find Active Server] ********************************************************
2025-10-25 13:01:04,052 p=3264774 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/find_active_server.yml for k8s-m1, k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 13:01:08,267 p=3264774 u=root n=ansible |  [ERROR]: User interrupted execution

2025-10-25 13:02:36,751 p=3275757 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ********************
2025-10-25 13:02:38,503 p=3275757 u=root n=ansible | TASK [Gathering Facts] *************************************************************************
2025-10-25 13:02:38,504 p=3275757 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:02:38,516 p=3275757 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:02:38,553 p=3275757 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:02:38,566 p=3275757 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:02:39,494 p=3275757 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:02:39,972 p=3275757 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:02:40,268 p=3275757 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 13:02:40,269 p=3275757 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:02:40,316 p=3275757 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:02:40,346 p=3275757 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:02:40,377 p=3275757 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:02:40,394 p=3275757 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:02:40,434 p=3275757 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:02:41,597 p=3275757 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] **************************************
2025-10-25 13:02:41,597 p=3275757 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:02:41,602 p=3275757 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:02:41,610 p=3275757 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:02:41,614 p=3275757 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:02:42,370 p=3275757 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:02:42,602 p=3275757 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:02:42,895 p=3275757 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] **********************************
2025-10-25 13:02:43,107 p=3275757 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 13:02:44,235 p=3275757 u=root n=ansible | ERROR! The requested handler 'reload sysctl' was not found in either the main handlers list nor in the listening handlers list
2025-10-25 13:05:05,007 p=3279666 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ********************
2025-10-25 13:05:06,681 p=3279666 u=root n=ansible | TASK [Gathering Facts] *************************************************************************
2025-10-25 13:05:06,682 p=3279666 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:05:06,697 p=3279666 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:05:06,711 p=3279666 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:05:06,730 p=3279666 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:05:06,736 p=3279666 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:05:08,080 p=3279666 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:05:08,513 p=3279666 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 13:05:08,514 p=3279666 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:05:08,514 p=3279666 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:05:08,564 p=3279666 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:05:08,573 p=3279666 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:05:08,594 p=3279666 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:05:09,317 p=3279666 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:05:10,459 p=3279666 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] **************************************
2025-10-25 13:05:10,460 p=3279666 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:05:10,465 p=3279666 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:05:10,469 p=3279666 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:05:10,474 p=3279666 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:05:10,477 p=3279666 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:05:11,199 p=3279666 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:05:11,483 p=3279666 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] **********************************
2025-10-25 13:05:11,752 p=3279666 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 13:05:12,800 p=3279666 u=root n=ansible | TASK [rke2-1.49.0 : Configure kernel modules] **************************************************
2025-10-25 13:05:12,801 p=3279666 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:05:12,805 p=3279666 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:05:12,810 p=3279666 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:05:12,810 p=3279666 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:05:12,814 p=3279666 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:05:14,742 p=3279666 u=root n=ansible | changed: [k8s-w03]
2025-10-25 13:05:15,562 p=3279666 u=root n=ansible | TASK [rke2-1.49.0 : Configure NetworkManager] **************************************************
2025-10-25 13:05:15,564 p=3279666 u=root n=ansible | fatal: [k8s-m1]: FAILED! => changed=false 
  checksum: c11aaa8955f6ea532d9ee2383df3e7fb9504d2a2
  msg: Destination directory /etc/NetworkManager/conf.d does not exist
2025-10-25 13:05:15,584 p=3279666 u=root n=ansible | fatal: [k8s-m2]: FAILED! => changed=false 
  checksum: c11aaa8955f6ea532d9ee2383df3e7fb9504d2a2
  msg: Destination directory /etc/NetworkManager/conf.d does not exist
2025-10-25 13:05:15,584 p=3279666 u=root n=ansible | fatal: [k8s-m3]: FAILED! => changed=false 
  checksum: c11aaa8955f6ea532d9ee2383df3e7fb9504d2a2
  msg: Destination directory /etc/NetworkManager/conf.d does not exist
2025-10-25 13:05:15,640 p=3279666 u=root n=ansible | fatal: [k8s-w01]: FAILED! => changed=false 
  checksum: c11aaa8955f6ea532d9ee2383df3e7fb9504d2a2
  msg: Destination directory /etc/NetworkManager/conf.d does not exist
2025-10-25 13:05:15,689 p=3279666 u=root n=ansible | fatal: [k8s-w02]: FAILED! => changed=false 
  checksum: c11aaa8955f6ea532d9ee2383df3e7fb9504d2a2
  msg: Destination directory /etc/NetworkManager/conf.d does not exist
2025-10-25 13:05:16,311 p=3279666 u=root n=ansible | fatal: [k8s-w03]: FAILED! => changed=false 
  checksum: c11aaa8955f6ea532d9ee2383df3e7fb9504d2a2
  msg: Destination directory /etc/NetworkManager/conf.d does not exist
2025-10-25 13:05:16,313 p=3279666 u=root n=ansible | PLAY RECAP *************************************************************************************
2025-10-25 13:05:16,313 p=3279666 u=root n=ansible | k8s-m1                     : ok=5    changed=0    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0   
2025-10-25 13:05:16,314 p=3279666 u=root n=ansible | k8s-m2                     : ok=5    changed=0    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0   
2025-10-25 13:05:16,314 p=3279666 u=root n=ansible | k8s-m3                     : ok=5    changed=0    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0   
2025-10-25 13:05:16,314 p=3279666 u=root n=ansible | k8s-w01                    : ok=5    changed=0    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0   
2025-10-25 13:05:16,314 p=3279666 u=root n=ansible | k8s-w02                    : ok=5    changed=0    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0   
2025-10-25 13:05:16,314 p=3279666 u=root n=ansible | k8s-w03                    : ok=5    changed=1    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0   
2025-10-25 13:08:08,052 p=3284073 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ********************
2025-10-25 13:08:09,768 p=3284073 u=root n=ansible | TASK [Gathering Facts] *************************************************************************
2025-10-25 13:08:09,770 p=3284073 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:08:09,779 p=3284073 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:08:09,786 p=3284073 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:08:09,799 p=3284073 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:08:09,848 p=3284073 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:08:12,117 p=3284073 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:08:12,456 p=3284073 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 13:08:12,458 p=3284073 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:08:12,502 p=3284073 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:08:12,529 p=3284073 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:08:12,530 p=3284073 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:08:12,552 p=3284073 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:08:12,587 p=3284073 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:08:13,712 p=3284073 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] **************************************
2025-10-25 13:08:13,712 p=3284073 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:08:13,721 p=3284073 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:08:13,729 p=3284073 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:08:13,733 p=3284073 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:08:13,747 p=3284073 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:08:14,510 p=3284073 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:08:14,783 p=3284073 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] **********************************
2025-10-25 13:08:15,024 p=3284073 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 13:08:16,247 p=3284073 u=root n=ansible | TASK [rke2-1.49.0 : Configure kernel modules] **************************************************
2025-10-25 13:08:16,248 p=3284073 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:08:16,261 p=3284073 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:08:16,262 p=3284073 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:08:16,266 p=3284073 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:08:16,301 p=3284073 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:08:17,071 p=3284073 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:08:17,457 p=3284073 u=root n=ansible | TASK [rke2-1.49.0 : Check NetworkManager conf.d directory] *************************************
2025-10-25 13:08:17,458 p=3284073 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:08:17,463 p=3284073 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:08:17,507 p=3284073 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:08:17,523 p=3284073 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:08:17,595 p=3284073 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:08:17,783 p=3284073 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:08:18,919 p=3284073 u=root n=ansible | TASK [rke2-1.49.0 : Configure sysctl parameters] ***********************************************
2025-10-25 13:08:18,920 p=3284073 u=root n=ansible | changed: [k8s-m2]
2025-10-25 13:08:18,924 p=3284073 u=root n=ansible | changed: [k8s-m1]
2025-10-25 13:08:18,925 p=3284073 u=root n=ansible | changed: [k8s-w02]
2025-10-25 13:08:18,925 p=3284073 u=root n=ansible | changed: [k8s-w01]
2025-10-25 13:08:18,928 p=3284073 u=root n=ansible | changed: [k8s-m3]
2025-10-25 13:08:19,668 p=3284073 u=root n=ansible | changed: [k8s-w03]
2025-10-25 13:08:20,336 p=3284073 u=root n=ansible | TASK [rke2-1.49.0 : Apply system configurations] ***********************************************
2025-10-25 13:08:20,337 p=3284073 u=root n=ansible | changed: [k8s-m3]
2025-10-25 13:08:20,340 p=3284073 u=root n=ansible | changed: [k8s-w02]
2025-10-25 13:08:20,340 p=3284073 u=root n=ansible | changed: [k8s-m1]
2025-10-25 13:08:20,342 p=3284073 u=root n=ansible | changed: [k8s-w01]
2025-10-25 13:08:20,354 p=3284073 u=root n=ansible | changed: [k8s-m2]
2025-10-25 13:08:20,862 p=3284073 u=root n=ansible | changed: [k8s-w03]
2025-10-25 13:08:21,102 p=3284073 u=root n=ansible | TASK [rke2-1.49.0 : Debug my_interface] ********************************************************
2025-10-25 13:08:21,104 p=3284073 u=root n=ansible | ok: [k8s-m1] => 
  network_interface: VARIABLE IS NOT DEFINED!
2025-10-25 13:08:21,131 p=3284073 u=root n=ansible | ok: [k8s-m2] => 
  network_interface: VARIABLE IS NOT DEFINED!
2025-10-25 13:08:21,169 p=3284073 u=root n=ansible | ok: [k8s-m3] => 
  network_interface: VARIABLE IS NOT DEFINED!
2025-10-25 13:08:21,196 p=3284073 u=root n=ansible | ok: [k8s-w01] => 
  network_interface: VARIABLE IS NOT DEFINED!
2025-10-25 13:08:21,228 p=3284073 u=root n=ansible | ok: [k8s-w02] => 
  network_interface: VARIABLE IS NOT DEFINED!
2025-10-25 13:08:21,243 p=3284073 u=root n=ansible | ok: [k8s-w03] => 
  network_interface: VARIABLE IS NOT DEFINED!
2025-10-25 13:08:22,009 p=3284073 u=root n=ansible | TASK [rke2-1.49.0 : delete RKE2 zone] **********************************************************
2025-10-25 13:08:22,011 p=3284073 u=root n=ansible | fatal: [k8s-m2]: FAILED! => changed=false 
  msg: Failed to import the required Python library (firewall) on k8s-m2's Python /usr/bin/python3. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter. Version 0.2.11 or newer required (0.3.9 or newer for offline operations)
2025-10-25 13:08:22,016 p=3284073 u=root n=ansible | fatal: [k8s-w01]: FAILED! => changed=false 
  msg: Failed to import the required Python library (firewall) on k8s-w01's Python /usr/bin/python3. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter. Version 0.2.11 or newer required (0.3.9 or newer for offline operations)
2025-10-25 13:08:22,016 p=3284073 u=root n=ansible | fatal: [k8s-w02]: FAILED! => changed=false 
  msg: Failed to import the required Python library (firewall) on k8s-w02's Python /usr/bin/python3. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter. Version 0.2.11 or newer required (0.3.9 or newer for offline operations)
2025-10-25 13:08:22,023 p=3284073 u=root n=ansible | fatal: [k8s-m1]: FAILED! => changed=false 
  msg: Failed to import the required Python library (firewall) on k8s-m1's Python /usr/bin/python3. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter. Version 0.2.11 or newer required (0.3.9 or newer for offline operations)
2025-10-25 13:08:22,031 p=3284073 u=root n=ansible | fatal: [k8s-m3]: FAILED! => changed=false 
  msg: Failed to import the required Python library (firewall) on k8s-m3's Python /usr/bin/python3. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter. Version 0.2.11 or newer required (0.3.9 or newer for offline operations)
2025-10-25 13:08:22,597 p=3284073 u=root n=ansible | fatal: [k8s-w03]: FAILED! => changed=false 
  msg: Failed to import the required Python library (firewall) on k8s-w03's Python /usr/bin/python3. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter. Version 0.2.11 or newer required (0.3.9 or newer for offline operations)
2025-10-25 13:08:22,598 p=3284073 u=root n=ansible | PLAY RECAP *************************************************************************************
2025-10-25 13:08:22,599 p=3284073 u=root n=ansible | k8s-m1                     : ok=9    changed=2    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0   
2025-10-25 13:08:22,599 p=3284073 u=root n=ansible | k8s-m2                     : ok=9    changed=2    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0   
2025-10-25 13:08:22,599 p=3284073 u=root n=ansible | k8s-m3                     : ok=9    changed=2    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0   
2025-10-25 13:08:22,599 p=3284073 u=root n=ansible | k8s-w01                    : ok=9    changed=2    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0   
2025-10-25 13:08:22,599 p=3284073 u=root n=ansible | k8s-w02                    : ok=9    changed=2    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0   
2025-10-25 13:08:22,599 p=3284073 u=root n=ansible | k8s-w03                    : ok=9    changed=2    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0   
2025-10-25 13:14:15,324 p=3292190 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ********************
2025-10-25 13:14:17,704 p=3292190 u=root n=ansible | TASK [Gathering Facts] *************************************************************************
2025-10-25 13:14:17,705 p=3292190 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:14:17,726 p=3292190 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:14:17,777 p=3292190 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:14:17,815 p=3292190 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:14:17,827 p=3292190 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:14:19,828 p=3292190 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:14:20,147 p=3292190 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 13:14:20,148 p=3292190 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:14:20,207 p=3292190 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:14:20,221 p=3292190 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:14:20,222 p=3292190 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:14:20,227 p=3292190 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:14:20,255 p=3292190 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:14:21,471 p=3292190 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] **************************************
2025-10-25 13:14:21,472 p=3292190 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:14:21,478 p=3292190 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:14:21,483 p=3292190 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:14:21,486 p=3292190 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:14:21,490 p=3292190 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:14:22,353 p=3292190 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:14:22,651 p=3292190 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] **********************************
2025-10-25 13:14:22,881 p=3292190 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 13:14:24,085 p=3292190 u=root n=ansible | TASK [rke2-1.49.0 : Configure kernel modules] **************************************************
2025-10-25 13:14:24,086 p=3292190 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:14:24,093 p=3292190 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:14:24,093 p=3292190 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:14:24,097 p=3292190 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:14:24,103 p=3292190 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:14:24,898 p=3292190 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:14:25,313 p=3292190 u=root n=ansible | TASK [rke2-1.49.0 : Check NetworkManager conf.d directory] *************************************
2025-10-25 13:14:25,314 p=3292190 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:14:25,315 p=3292190 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:14:25,336 p=3292190 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:14:25,352 p=3292190 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:14:25,358 p=3292190 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:14:25,724 p=3292190 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:14:26,766 p=3292190 u=root n=ansible | TASK [rke2-1.49.0 : Configure sysctl parameters] ***********************************************
2025-10-25 13:14:26,767 p=3292190 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:14:26,777 p=3292190 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:14:26,801 p=3292190 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:14:26,801 p=3292190 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:14:26,856 p=3292190 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:14:27,513 p=3292190 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:14:28,117 p=3292190 u=root n=ansible | TASK [rke2-1.49.0 : Apply system configurations] ***********************************************
2025-10-25 13:14:28,118 p=3292190 u=root n=ansible | changed: [k8s-m1]
2025-10-25 13:14:28,135 p=3292190 u=root n=ansible | changed: [k8s-w01]
2025-10-25 13:14:28,136 p=3292190 u=root n=ansible | changed: [k8s-w02]
2025-10-25 13:14:28,136 p=3292190 u=root n=ansible | changed: [k8s-m3]
2025-10-25 13:14:28,136 p=3292190 u=root n=ansible | changed: [k8s-m2]
2025-10-25 13:14:28,496 p=3292190 u=root n=ansible | changed: [k8s-w03]
2025-10-25 13:14:31,800 p=3292190 u=root n=ansible | TASK [rke2-1.49.0 : Ensure ufw is installed] ***************************************************
2025-10-25 13:14:31,801 p=3292190 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:14:31,841 p=3292190 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:14:31,888 p=3292190 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:14:32,044 p=3292190 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:14:32,053 p=3292190 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:14:33,181 p=3292190 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:14:33,621 p=3292190 u=root n=ansible | TASK [rke2-1.49.0 : Allow required ports with ufw] *********************************************
2025-10-25 13:14:33,622 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=22/tcp) => changed=false 
  ansible_loop_var: item
  item: 22/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:33,638 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=22/tcp) => changed=false 
  ansible_loop_var: item
  item: 22/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:33,669 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=22/tcp) => changed=false 
  ansible_loop_var: item
  item: 22/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:33,681 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=22/tcp) => changed=false 
  ansible_loop_var: item
  item: 22/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:33,715 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=22/tcp) => changed=false 
  ansible_loop_var: item
  item: 22/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:33,936 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=179/tcp) => changed=false 
  ansible_loop_var: item
  item: 179/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:33,956 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=179/tcp) => changed=false 
  ansible_loop_var: item
  item: 179/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:33,976 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=179/tcp) => changed=false 
  ansible_loop_var: item
  item: 179/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:33,988 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=179/tcp) => changed=false 
  ansible_loop_var: item
  item: 179/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,006 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=179/tcp) => changed=false 
  ansible_loop_var: item
  item: 179/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,224 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=9345/tcp) => changed=false 
  ansible_loop_var: item
  item: 9345/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,241 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=9345/tcp) => changed=false 
  ansible_loop_var: item
  item: 9345/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,263 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=9345/tcp) => changed=false 
  ansible_loop_var: item
  item: 9345/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,282 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=9345/tcp) => changed=false 
  ansible_loop_var: item
  item: 9345/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,302 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=9345/tcp) => changed=false 
  ansible_loop_var: item
  item: 9345/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,516 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=6443/tcp) => changed=false 
  ansible_loop_var: item
  item: 6443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,528 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=6443/tcp) => changed=false 
  ansible_loop_var: item
  item: 6443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,563 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=6443/tcp) => changed=false 
  ansible_loop_var: item
  item: 6443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,576 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=6443/tcp) => changed=false 
  ansible_loop_var: item
  item: 6443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,591 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=6443/tcp) => changed=false 
  ansible_loop_var: item
  item: 6443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,799 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=10250/tcp) => changed=false 
  ansible_loop_var: item
  item: 10250/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,816 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=10250/tcp) => changed=false 
  ansible_loop_var: item
  item: 10250/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,846 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=10250/tcp) => changed=false 
  ansible_loop_var: item
  item: 10250/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,863 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=10250/tcp) => changed=false 
  ansible_loop_var: item
  item: 10250/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:34,877 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=10250/tcp) => changed=false 
  ansible_loop_var: item
  item: 10250/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,081 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=2379:2381/tcp) => changed=false 
  ansible_loop_var: item
  item: 2379:2381/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,094 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=2379:2381/tcp) => changed=false 
  ansible_loop_var: item
  item: 2379:2381/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,133 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=2379:2381/tcp) => changed=false 
  ansible_loop_var: item
  item: 2379:2381/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,151 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=2379:2381/tcp) => changed=false 
  ansible_loop_var: item
  item: 2379:2381/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,167 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=2379:2381/tcp) => changed=false 
  ansible_loop_var: item
  item: 2379:2381/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,373 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=30000:32767/tcp) => changed=false 
  ansible_loop_var: item
  item: 30000:32767/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,375 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=30000:32767/tcp) => changed=false 
  ansible_loop_var: item
  item: 30000:32767/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,423 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=30000:32767/tcp) => changed=false 
  ansible_loop_var: item
  item: 30000:32767/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,461 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=30000:32767/tcp) => changed=false 
  ansible_loop_var: item
  item: 30000:32767/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,462 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=30000:32767/tcp) => changed=false 
  ansible_loop_var: item
  item: 30000:32767/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,662 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=8472/udp) => changed=false 
  ansible_loop_var: item
  item: 8472/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,663 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=8472/udp) => changed=false 
  ansible_loop_var: item
  item: 8472/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,744 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=8472/udp) => changed=false 
  ansible_loop_var: item
  item: 8472/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,764 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=8472/udp) => changed=false 
  ansible_loop_var: item
  item: 8472/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,787 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=8472/udp) => changed=false 
  ansible_loop_var: item
  item: 8472/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,951 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=6081/udp) => changed=false 
  ansible_loop_var: item
  item: 6081/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:35,952 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=6081/udp) => changed=false 
  ansible_loop_var: item
  item: 6081/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,026 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=6081/udp) => changed=false 
  ansible_loop_var: item
  item: 6081/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,049 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=6081/udp) => changed=false 
  ansible_loop_var: item
  item: 6081/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,079 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=6081/udp) => changed=false 
  ansible_loop_var: item
  item: 6081/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,239 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=4240/tcp) => changed=false 
  ansible_loop_var: item
  item: 4240/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,247 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=4240/tcp) => changed=false 
  ansible_loop_var: item
  item: 4240/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,305 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=4240/tcp) => changed=false 
  ansible_loop_var: item
  item: 4240/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,338 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=4240/tcp) => changed=false 
  ansible_loop_var: item
  item: 4240/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,363 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=4240/tcp) => changed=false 
  ansible_loop_var: item
  item: 4240/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,528 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=4244/tcp) => changed=false 
  ansible_loop_var: item
  item: 4244/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,529 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=4244/tcp) => changed=false 
  ansible_loop_var: item
  item: 4244/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,595 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=4244/tcp) => changed=false 
  ansible_loop_var: item
  item: 4244/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,631 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=4244/tcp) => changed=false 
  ansible_loop_var: item
  item: 4244/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,658 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=4244/tcp) => changed=false 
  ansible_loop_var: item
  item: 4244/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,817 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=4245/tcp) => changed=false 
  ansible_loop_var: item
  item: 4245/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,819 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=4245/tcp) => changed=false 
  ansible_loop_var: item
  item: 4245/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,875 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=4245/tcp) => changed=false 
  ansible_loop_var: item
  item: 4245/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,922 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=4245/tcp) => changed=false 
  ansible_loop_var: item
  item: 4245/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:36,953 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=4245/tcp) => changed=false 
  ansible_loop_var: item
  item: 4245/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,106 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=4222/tcp) => changed=false 
  ansible_loop_var: item
  item: 4222/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,110 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=4222/tcp) => changed=false 
  ansible_loop_var: item
  item: 4222/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,170 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=4222/tcp) => changed=false 
  ansible_loop_var: item
  item: 4222/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,213 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=4222/tcp) => changed=false 
  ansible_loop_var: item
  item: 4222/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,246 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=4222/tcp) => changed=false 
  ansible_loop_var: item
  item: 4222/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,397 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=9966/tcp) => changed=false 
  ansible_loop_var: item
  item: 9966/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,399 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=9966/tcp) => changed=false 
  ansible_loop_var: item
  item: 9966/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,461 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=9966/tcp) => changed=false 
  ansible_loop_var: item
  item: 9966/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,493 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=9966/tcp) => changed=false 
  ansible_loop_var: item
  item: 9966/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,535 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=9966/tcp) => changed=false 
  ansible_loop_var: item
  item: 9966/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,692 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=4250:4251/tcp) => changed=false 
  ansible_loop_var: item
  item: 4250:4251/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,693 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=4250:4251/tcp) => changed=false 
  ansible_loop_var: item
  item: 4250:4251/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,758 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=4250:4251/tcp) => changed=false 
  ansible_loop_var: item
  item: 4250:4251/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,773 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=4250:4251/tcp) => changed=false 
  ansible_loop_var: item
  item: 4250:4251/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,821 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=4250:4251/tcp) => changed=false 
  ansible_loop_var: item
  item: 4250:4251/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,978 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=6060:6062/tcp) => changed=false 
  ansible_loop_var: item
  item: 6060:6062/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:37,979 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=6060:6062/tcp) => changed=false 
  ansible_loop_var: item
  item: 6060:6062/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,048 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=6060:6062/tcp) => changed=false 
  ansible_loop_var: item
  item: 6060:6062/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,069 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=6060:6062/tcp) => changed=false 
  ansible_loop_var: item
  item: 6060:6062/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,105 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=6060:6062/tcp) => changed=false 
  ansible_loop_var: item
  item: 6060:6062/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,260 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=9878:9879/tcp) => changed=false 
  ansible_loop_var: item
  item: 9878:9879/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,264 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=9878:9879/tcp) => changed=false 
  ansible_loop_var: item
  item: 9878:9879/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,347 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=9878:9879/tcp) => changed=false 
  ansible_loop_var: item
  item: 9878:9879/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,349 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=9878:9879/tcp) => changed=false 
  ansible_loop_var: item
  item: 9878:9879/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,393 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=9878:9879/tcp) => changed=false 
  ansible_loop_var: item
  item: 9878:9879/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,554 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=9890:9893/tcp) => changed=false 
  ansible_loop_var: item
  item: 9890:9893/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,556 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=9890:9893/tcp) => changed=false 
  ansible_loop_var: item
  item: 9890:9893/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,628 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=9890:9893/tcp) => changed=false 
  ansible_loop_var: item
  item: 9890:9893/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,634 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=9890:9893/tcp) => changed=false 
  ansible_loop_var: item
  item: 9890:9893/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,720 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=9890:9893/tcp) => changed=false 
  ansible_loop_var: item
  item: 9890:9893/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,842 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=9901/tcp) => changed=false 
  ansible_loop_var: item
  item: 9901/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,843 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=9901/tcp) => changed=false 
  ansible_loop_var: item
  item: 9901/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,914 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=9901/tcp) => changed=false 
  ansible_loop_var: item
  item: 9901/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,921 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=9901/tcp) => changed=false 
  ansible_loop_var: item
  item: 9901/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:38,999 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=9901/tcp) => changed=false 
  ansible_loop_var: item
  item: 9901/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,130 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=9962:9964/tcp) => changed=false 
  ansible_loop_var: item
  item: 9962:9964/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,134 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=9962:9964/tcp) => changed=false 
  ansible_loop_var: item
  item: 9962:9964/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,196 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=9962:9964/tcp) => changed=false 
  ansible_loop_var: item
  item: 9962:9964/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,203 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=9962:9964/tcp) => changed=false 
  ansible_loop_var: item
  item: 9962:9964/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,294 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=9962:9964/tcp) => changed=false 
  ansible_loop_var: item
  item: 9962:9964/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,410 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=51871/udp) => changed=false 
  ansible_loop_var: item
  item: 51871/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,416 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=51871/udp) => changed=false 
  ansible_loop_var: item
  item: 51871/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,490 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=51871/udp) => changed=false 
  ansible_loop_var: item
  item: 51871/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,491 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=51871/udp) => changed=false 
  ansible_loop_var: item
  item: 51871/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,598 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=51871/udp) => changed=false 
  ansible_loop_var: item
  item: 51871/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,706 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=4567/tcp) => changed=false 
  ansible_loop_var: item
  item: 4567/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,726 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=4567/tcp) => changed=false 
  ansible_loop_var: item
  item: 4567/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,779 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=4567/tcp) => changed=false 
  ansible_loop_var: item
  item: 4567/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,789 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=4567/tcp) => changed=false 
  ansible_loop_var: item
  item: 4567/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,891 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=4567/tcp) => changed=false 
  ansible_loop_var: item
  item: 4567/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:39,997 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=4567/udp) => changed=false 
  ansible_loop_var: item
  item: 4567/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,014 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=4567/udp) => changed=false 
  ansible_loop_var: item
  item: 4567/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,069 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=4567/udp) => changed=false 
  ansible_loop_var: item
  item: 4567/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,070 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=4567/udp) => changed=false 
  ansible_loop_var: item
  item: 4567/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,183 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=4567/udp) => changed=false 
  ansible_loop_var: item
  item: 4567/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,277 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=4568/tcp) => changed=false 
  ansible_loop_var: item
  item: 4568/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,303 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=4568/tcp) => changed=false 
  ansible_loop_var: item
  item: 4568/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,353 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=4568/tcp) => changed=false 
  ansible_loop_var: item
  item: 4568/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,355 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=4568/tcp) => changed=false 
  ansible_loop_var: item
  item: 4568/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,472 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=4568/tcp) => changed=false 
  ansible_loop_var: item
  item: 4568/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,568 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=4444/tcp) => changed=false 
  ansible_loop_var: item
  item: 4444/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,597 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=4444/tcp) => changed=false 
  ansible_loop_var: item
  item: 4444/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,633 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=4444/tcp) => changed=false 
  ansible_loop_var: item
  item: 4444/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,634 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=4444/tcp) => changed=false 
  ansible_loop_var: item
  item: 4444/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,773 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=4444/tcp) => changed=false 
  ansible_loop_var: item
  item: 4444/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,862 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=7443/tcp) => changed=false 
  ansible_loop_var: item
  item: 7443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,883 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=7443/tcp) => changed=false 
  ansible_loop_var: item
  item: 7443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,916 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=7443/tcp) => changed=false 
  ansible_loop_var: item
  item: 7443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:40,921 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=7443/tcp) => changed=false 
  ansible_loop_var: item
  item: 7443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,056 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=7443/tcp) => changed=false 
  ansible_loop_var: item
  item: 7443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,157 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=7345/tcp) => changed=false 
  ansible_loop_var: item
  item: 7345/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,171 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=7345/tcp) => changed=false 
  ansible_loop_var: item
  item: 7345/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,206 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=7345/tcp) => changed=false 
  ansible_loop_var: item
  item: 7345/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,215 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=7345/tcp) => changed=false 
  ansible_loop_var: item
  item: 7345/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,343 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=7345/tcp) => changed=false 
  ansible_loop_var: item
  item: 7345/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,446 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=8080/tcp) => changed=false 
  ansible_loop_var: item
  item: 8080/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,459 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=8080/tcp) => changed=false 
  ansible_loop_var: item
  item: 8080/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,495 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=8080/tcp) => changed=false 
  ansible_loop_var: item
  item: 8080/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,499 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=8080/tcp) => changed=false 
  ansible_loop_var: item
  item: 8080/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,635 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=8080/tcp) => changed=false 
  ansible_loop_var: item
  item: 8080/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,732 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=8443/tcp) => changed=false 
  ansible_loop_var: item
  item: 8443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,745 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=8443/tcp) => changed=false 
  ansible_loop_var: item
  item: 8443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,785 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=8443/tcp) => changed=false 
  ansible_loop_var: item
  item: 8443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,789 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=8443/tcp) => changed=false 
  ansible_loop_var: item
  item: 8443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:41,915 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=8443/tcp) => changed=false 
  ansible_loop_var: item
  item: 8443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,015 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=3306/tcp) => changed=false 
  ansible_loop_var: item
  item: 3306/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,038 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=3306/tcp) => changed=false 
  ansible_loop_var: item
  item: 3306/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,063 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=3306/tcp) => changed=false 
  ansible_loop_var: item
  item: 3306/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,081 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=3306/tcp) => changed=false 
  ansible_loop_var: item
  item: 3306/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,206 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=3306/tcp) => changed=false 
  ansible_loop_var: item
  item: 3306/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,301 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=3307/tcp) => changed=false 
  ansible_loop_var: item
  item: 3307/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,323 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=3307/tcp) => changed=false 
  ansible_loop_var: item
  item: 3307/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,340 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=3307/tcp) => changed=false 
  ansible_loop_var: item
  item: 3307/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,367 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=3307/tcp) => changed=false 
  ansible_loop_var: item
  item: 3307/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,531 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=3307/tcp) => changed=false 
  ansible_loop_var: item
  item: 3307/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,596 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=80/tcp) => changed=false 
  ansible_loop_var: item
  item: 80/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,607 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=80/tcp) => changed=false 
  ansible_loop_var: item
  item: 80/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,626 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=80/tcp) => changed=false 
  ansible_loop_var: item
  item: 80/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,648 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=80/tcp) => changed=false 
  ansible_loop_var: item
  item: 80/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,827 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=80/tcp) => changed=false 
  ansible_loop_var: item
  item: 80/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,886 p=3292190 u=root n=ansible | failed: [k8s-w01] (item=443/tcp) => changed=false 
  ansible_loop_var: item
  item: 443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,889 p=3292190 u=root n=ansible | failed: [k8s-m2] (item=443/tcp) => changed=false 
  ansible_loop_var: item
  item: 443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,907 p=3292190 u=root n=ansible | failed: [k8s-m3] (item=443/tcp) => changed=false 
  ansible_loop_var: item
  item: 443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:42,933 p=3292190 u=root n=ansible | failed: [k8s-w02] (item=443/tcp) => changed=false 
  ansible_loop_var: item
  item: 443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:43,122 p=3292190 u=root n=ansible | failed: [k8s-m1] (item=443/tcp) => changed=false 
  ansible_loop_var: item
  item: 443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:43,200 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=22/tcp) => changed=false 
  ansible_loop_var: item
  item: 22/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:43,489 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=179/tcp) => changed=false 
  ansible_loop_var: item
  item: 179/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:43,804 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=9345/tcp) => changed=false 
  ansible_loop_var: item
  item: 9345/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:44,096 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=6443/tcp) => changed=false 
  ansible_loop_var: item
  item: 6443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:44,424 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=10250/tcp) => changed=false 
  ansible_loop_var: item
  item: 10250/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:44,777 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=2379:2381/tcp) => changed=false 
  ansible_loop_var: item
  item: 2379:2381/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:45,097 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=30000:32767/tcp) => changed=false 
  ansible_loop_var: item
  item: 30000:32767/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:45,432 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=8472/udp) => changed=false 
  ansible_loop_var: item
  item: 8472/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:45,773 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=6081/udp) => changed=false 
  ansible_loop_var: item
  item: 6081/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:46,111 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=4240/tcp) => changed=false 
  ansible_loop_var: item
  item: 4240/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:46,487 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=4244/tcp) => changed=false 
  ansible_loop_var: item
  item: 4244/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:46,810 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=4245/tcp) => changed=false 
  ansible_loop_var: item
  item: 4245/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:47,153 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=4222/tcp) => changed=false 
  ansible_loop_var: item
  item: 4222/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:47,521 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=9966/tcp) => changed=false 
  ansible_loop_var: item
  item: 9966/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:47,859 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=4250:4251/tcp) => changed=false 
  ansible_loop_var: item
  item: 4250:4251/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:48,177 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=6060:6062/tcp) => changed=false 
  ansible_loop_var: item
  item: 6060:6062/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:48,536 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=9878:9879/tcp) => changed=false 
  ansible_loop_var: item
  item: 9878:9879/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:48,859 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=9890:9893/tcp) => changed=false 
  ansible_loop_var: item
  item: 9890:9893/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:49,183 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=9901/tcp) => changed=false 
  ansible_loop_var: item
  item: 9901/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:49,599 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=9962:9964/tcp) => changed=false 
  ansible_loop_var: item
  item: 9962:9964/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:49,951 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=51871/udp) => changed=false 
  ansible_loop_var: item
  item: 51871/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:50,322 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=4567/tcp) => changed=false 
  ansible_loop_var: item
  item: 4567/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:50,808 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=4567/udp) => changed=false 
  ansible_loop_var: item
  item: 4567/udp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:51,097 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=4568/tcp) => changed=false 
  ansible_loop_var: item
  item: 4568/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:51,426 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=4444/tcp) => changed=false 
  ansible_loop_var: item
  item: 4444/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:51,755 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=7443/tcp) => changed=false 
  ansible_loop_var: item
  item: 7443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:52,060 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=7345/tcp) => changed=false 
  ansible_loop_var: item
  item: 7345/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:52,409 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=8080/tcp) => changed=false 
  ansible_loop_var: item
  item: 8080/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:52,766 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=8443/tcp) => changed=false 
  ansible_loop_var: item
  item: 8443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:53,103 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=3306/tcp) => changed=false 
  ansible_loop_var: item
  item: 3306/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:53,427 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=3307/tcp) => changed=false 
  ansible_loop_var: item
  item: 3307/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:53,761 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=80/tcp) => changed=false 
  ansible_loop_var: item
  item: 80/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:54,072 p=3292190 u=root n=ansible | failed: [k8s-w03] (item=443/tcp) => changed=false 
  ansible_loop_var: item
  item: 443/tcp
  msg: 'Unsupported parameters for (ansible.legacy.command) module: warn. Supported parameters include: _raw_params, _uses_shell, argv, chdir, creates, executable, expand_argument_vars, removes, stdin, stdin_add_newline, strip_empty_ends.'
2025-10-25 13:14:54,080 p=3292190 u=root n=ansible | PLAY RECAP *************************************************************************************
2025-10-25 13:14:54,080 p=3292190 u=root n=ansible | k8s-m1                     : ok=9    changed=1    unreachable=0    failed=1    skipped=10   rescued=0    ignored=0   
2025-10-25 13:14:54,081 p=3292190 u=root n=ansible | k8s-m2                     : ok=9    changed=1    unreachable=0    failed=1    skipped=10   rescued=0    ignored=0   
2025-10-25 13:14:54,081 p=3292190 u=root n=ansible | k8s-m3                     : ok=9    changed=1    unreachable=0    failed=1    skipped=10   rescued=0    ignored=0   
2025-10-25 13:14:54,081 p=3292190 u=root n=ansible | k8s-w01                    : ok=9    changed=1    unreachable=0    failed=1    skipped=10   rescued=0    ignored=0   
2025-10-25 13:14:54,081 p=3292190 u=root n=ansible | k8s-w02                    : ok=9    changed=1    unreachable=0    failed=1    skipped=10   rescued=0    ignored=0   
2025-10-25 13:14:54,081 p=3292190 u=root n=ansible | k8s-w03                    : ok=9    changed=1    unreachable=0    failed=1    skipped=10   rescued=0    ignored=0   
2025-10-25 13:16:43,126 p=3296188 u=root n=ansible | playbook: playbooks/rke2-airgap-install.yml
2025-10-25 13:17:26,569 p=3297147 u=root n=ansible | playbook: playbooks/rke2-airgap-install.yml
2025-10-25 13:17:34,106 p=3297371 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ********************
2025-10-25 13:17:35,792 p=3297371 u=root n=ansible | TASK [Gathering Facts] *************************************************************************
2025-10-25 13:17:35,794 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:17:35,799 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:17:35,804 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:17:35,834 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:17:35,839 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:17:38,184 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:17:38,513 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 13:17:38,514 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:17:38,550 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:17:38,587 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:17:38,588 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:17:38,609 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:17:38,636 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:17:39,790 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] **************************************
2025-10-25 13:17:39,790 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:17:39,796 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:17:39,802 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:17:39,807 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:17:39,832 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:17:40,607 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:17:40,887 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] **********************************
2025-10-25 13:17:41,115 p=3297371 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 13:17:42,224 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Configure kernel modules] **************************************************
2025-10-25 13:17:42,225 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:17:42,230 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:17:42,230 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:17:42,233 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:17:42,245 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:17:43,032 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:17:43,416 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Check NetworkManager conf.d directory] *************************************
2025-10-25 13:17:43,417 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:17:43,438 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:17:43,461 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:17:43,491 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:17:43,522 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:17:43,763 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:17:44,941 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Configure sysctl parameters] ***********************************************
2025-10-25 13:17:44,942 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:17:44,945 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:17:44,957 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:17:45,152 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:17:45,197 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:17:45,756 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:17:46,351 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Apply system configurations] ***********************************************
2025-10-25 13:17:46,352 p=3297371 u=root n=ansible | changed: [k8s-w01]
2025-10-25 13:17:46,357 p=3297371 u=root n=ansible | changed: [k8s-m1]
2025-10-25 13:17:46,364 p=3297371 u=root n=ansible | changed: [k8s-w02]
2025-10-25 13:17:46,365 p=3297371 u=root n=ansible | changed: [k8s-m2]
2025-10-25 13:17:46,381 p=3297371 u=root n=ansible | changed: [k8s-m3]
2025-10-25 13:17:46,712 p=3297371 u=root n=ansible | changed: [k8s-w03]
2025-10-25 13:17:49,816 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Ensure ufw is installed] ***************************************************
2025-10-25 13:17:49,817 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:17:49,819 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:17:49,819 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:17:49,872 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:17:49,879 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:17:51,071 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:17:51,702 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Allow required ports with ufw] *********************************************
2025-10-25 13:17:51,703 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=22/tcp)
2025-10-25 13:17:51,765 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=22/tcp)
2025-10-25 13:17:51,794 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=22/tcp)
2025-10-25 13:17:51,827 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=22/tcp)
2025-10-25 13:17:51,901 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=22/tcp)
2025-10-25 13:17:52,182 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=179/tcp)
2025-10-25 13:17:52,264 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=179/tcp)
2025-10-25 13:17:52,278 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=179/tcp)
2025-10-25 13:17:52,306 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=179/tcp)
2025-10-25 13:17:52,438 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=179/tcp)
2025-10-25 13:17:52,687 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=9345/tcp)
2025-10-25 13:17:52,734 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=9345/tcp)
2025-10-25 13:17:52,749 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=9345/tcp)
2025-10-25 13:17:52,775 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=9345/tcp)
2025-10-25 13:17:53,040 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=9345/tcp)
2025-10-25 13:17:53,138 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=6443/tcp)
2025-10-25 13:17:53,198 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=6443/tcp)
2025-10-25 13:17:53,231 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=6443/tcp)
2025-10-25 13:17:53,233 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=6443/tcp)
2025-10-25 13:17:53,579 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=6443/tcp)
2025-10-25 13:17:53,636 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=10250/tcp)
2025-10-25 13:17:53,664 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=10250/tcp)
2025-10-25 13:17:53,694 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=10250/tcp)
2025-10-25 13:17:53,725 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=10250/tcp)
2025-10-25 13:17:54,084 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=10250/tcp)
2025-10-25 13:17:54,117 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=2379:2381/tcp)
2025-10-25 13:17:54,128 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=2379:2381/tcp)
2025-10-25 13:17:54,159 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=2379:2381/tcp)
2025-10-25 13:17:54,181 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=2379:2381/tcp)
2025-10-25 13:17:54,555 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=2379:2381/tcp)
2025-10-25 13:17:54,593 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=30000:32767/tcp)
2025-10-25 13:17:54,618 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=30000:32767/tcp)
2025-10-25 13:17:54,620 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=30000:32767/tcp)
2025-10-25 13:17:54,653 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=30000:32767/tcp)
2025-10-25 13:17:55,015 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=30000:32767/tcp)
2025-10-25 13:17:55,044 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=8472/udp)
2025-10-25 13:17:55,077 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=8472/udp)
2025-10-25 13:17:55,080 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=8472/udp)
2025-10-25 13:17:55,116 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=8472/udp)
2025-10-25 13:17:55,473 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=8472/udp)
2025-10-25 13:17:55,502 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=6081/udp)
2025-10-25 13:17:55,532 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=6081/udp)
2025-10-25 13:17:55,556 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=6081/udp)
2025-10-25 13:17:55,579 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=6081/udp)
2025-10-25 13:17:55,934 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=6081/udp)
2025-10-25 13:17:55,977 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=4240/tcp)
2025-10-25 13:17:56,015 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=4240/tcp)
2025-10-25 13:17:56,033 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=4240/tcp)
2025-10-25 13:17:56,054 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=4240/tcp)
2025-10-25 13:17:56,398 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=4240/tcp)
2025-10-25 13:17:56,447 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=4244/tcp)
2025-10-25 13:17:56,472 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=4244/tcp)
2025-10-25 13:17:56,511 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=4244/tcp)
2025-10-25 13:17:56,543 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=4244/tcp)
2025-10-25 13:17:56,871 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=4244/tcp)
2025-10-25 13:17:56,901 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=4245/tcp)
2025-10-25 13:17:56,942 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=4245/tcp)
2025-10-25 13:17:56,976 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=4245/tcp)
2025-10-25 13:17:56,999 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=4245/tcp)
2025-10-25 13:17:57,336 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=4245/tcp)
2025-10-25 13:17:57,369 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=4222/tcp)
2025-10-25 13:17:57,404 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=4222/tcp)
2025-10-25 13:17:57,449 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=4222/tcp)
2025-10-25 13:17:57,461 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=4222/tcp)
2025-10-25 13:17:57,797 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=4222/tcp)
2025-10-25 13:17:57,824 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=9966/tcp)
2025-10-25 13:17:57,877 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=9966/tcp)
2025-10-25 13:17:57,913 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=9966/tcp)
2025-10-25 13:17:57,930 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=9966/tcp)
2025-10-25 13:17:58,261 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=9966/tcp)
2025-10-25 13:17:58,275 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=4250:4251/tcp)
2025-10-25 13:17:58,335 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=4250:4251/tcp)
2025-10-25 13:17:58,370 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=4250:4251/tcp)
2025-10-25 13:17:58,382 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=4250:4251/tcp)
2025-10-25 13:17:58,715 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=4250:4251/tcp)
2025-10-25 13:17:58,739 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=6060:6062/tcp)
2025-10-25 13:17:58,794 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=6060:6062/tcp)
2025-10-25 13:17:58,835 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=6060:6062/tcp)
2025-10-25 13:17:58,842 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=6060:6062/tcp)
2025-10-25 13:17:59,177 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=6060:6062/tcp)
2025-10-25 13:17:59,206 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=9878:9879/tcp)
2025-10-25 13:17:59,250 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=9878:9879/tcp)
2025-10-25 13:17:59,293 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=9878:9879/tcp)
2025-10-25 13:17:59,325 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=9878:9879/tcp)
2025-10-25 13:17:59,637 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=9878:9879/tcp)
2025-10-25 13:17:59,681 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=9890:9893/tcp)
2025-10-25 13:17:59,709 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=9890:9893/tcp)
2025-10-25 13:17:59,774 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=9890:9893/tcp)
2025-10-25 13:17:59,775 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=9890:9893/tcp)
2025-10-25 13:18:00,118 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=9890:9893/tcp)
2025-10-25 13:18:00,154 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=9901/tcp)
2025-10-25 13:18:00,167 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=9901/tcp)
2025-10-25 13:18:00,223 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=9901/tcp)
2025-10-25 13:18:00,234 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=9901/tcp)
2025-10-25 13:18:00,583 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=9901/tcp)
2025-10-25 13:18:00,615 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=9962:9964/tcp)
2025-10-25 13:18:00,645 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=9962:9964/tcp)
2025-10-25 13:18:00,678 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=9962:9964/tcp)
2025-10-25 13:18:00,694 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=9962:9964/tcp)
2025-10-25 13:18:01,053 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=9962:9964/tcp)
2025-10-25 13:18:01,091 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=51871/udp)
2025-10-25 13:18:01,092 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=51871/udp)
2025-10-25 13:18:01,134 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=51871/udp)
2025-10-25 13:18:01,158 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=51871/udp)
2025-10-25 13:18:01,502 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=51871/udp)
2025-10-25 13:18:01,542 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=4567/tcp)
2025-10-25 13:18:01,547 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=4567/tcp)
2025-10-25 13:18:01,583 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=4567/tcp)
2025-10-25 13:18:01,636 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=4567/tcp)
2025-10-25 13:18:01,972 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=4567/tcp)
2025-10-25 13:18:02,007 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=4567/udp)
2025-10-25 13:18:02,042 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=4567/udp)
2025-10-25 13:18:02,061 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=4567/udp)
2025-10-25 13:18:02,096 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=4567/udp)
2025-10-25 13:18:02,419 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=4567/udp)
2025-10-25 13:18:02,495 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=4568/tcp)
2025-10-25 13:18:02,496 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=4568/tcp)
2025-10-25 13:18:02,514 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=4568/tcp)
2025-10-25 13:18:02,570 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=4568/tcp)
2025-10-25 13:18:02,882 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=4568/tcp)
2025-10-25 13:18:02,957 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=4444/tcp)
2025-10-25 13:18:02,962 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=4444/tcp)
2025-10-25 13:18:02,994 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=4444/tcp)
2025-10-25 13:18:03,050 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=4444/tcp)
2025-10-25 13:18:03,361 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=4444/tcp)
2025-10-25 13:18:03,430 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=7443/tcp)
2025-10-25 13:18:03,431 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=7443/tcp)
2025-10-25 13:18:03,457 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=7443/tcp)
2025-10-25 13:18:03,536 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=7443/tcp)
2025-10-25 13:18:03,828 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=7443/tcp)
2025-10-25 13:18:03,890 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=7345/tcp)
2025-10-25 13:18:03,890 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=7345/tcp)
2025-10-25 13:18:03,918 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=7345/tcp)
2025-10-25 13:18:04,016 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=7345/tcp)
2025-10-25 13:18:04,306 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=7345/tcp)
2025-10-25 13:18:04,354 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=8080/tcp)
2025-10-25 13:18:04,354 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=8080/tcp)
2025-10-25 13:18:04,379 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=8080/tcp)
2025-10-25 13:18:04,479 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=8080/tcp)
2025-10-25 13:18:04,784 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=8080/tcp)
2025-10-25 13:18:04,804 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=8443/tcp)
2025-10-25 13:18:04,823 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=8443/tcp)
2025-10-25 13:18:04,844 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=8443/tcp)
2025-10-25 13:18:04,958 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=8443/tcp)
2025-10-25 13:18:05,241 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=8443/tcp)
2025-10-25 13:18:05,273 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=3306/tcp)
2025-10-25 13:18:05,315 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=3306/tcp)
2025-10-25 13:18:05,328 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=3306/tcp)
2025-10-25 13:18:05,425 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=3306/tcp)
2025-10-25 13:18:05,711 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=3306/tcp)
2025-10-25 13:18:05,735 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=3307/tcp)
2025-10-25 13:18:05,789 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=3307/tcp)
2025-10-25 13:18:05,793 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=3307/tcp)
2025-10-25 13:18:05,892 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=3307/tcp)
2025-10-25 13:18:06,161 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=3307/tcp)
2025-10-25 13:18:06,215 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=80/tcp)
2025-10-25 13:18:06,251 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=80/tcp)
2025-10-25 13:18:06,269 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=80/tcp)
2025-10-25 13:18:06,357 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=80/tcp)
2025-10-25 13:18:06,623 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=80/tcp)
2025-10-25 13:18:06,670 p=3297371 u=root n=ansible | changed: [k8s-w02] => (item=443/tcp)
2025-10-25 13:18:06,714 p=3297371 u=root n=ansible | changed: [k8s-m2] => (item=443/tcp)
2025-10-25 13:18:06,769 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=443/tcp)
2025-10-25 13:18:06,836 p=3297371 u=root n=ansible | changed: [k8s-m3] => (item=443/tcp)
2025-10-25 13:18:07,108 p=3297371 u=root n=ansible | changed: [k8s-w01] => (item=443/tcp)
2025-10-25 13:18:07,298 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=22/tcp)
2025-10-25 13:18:07,811 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=179/tcp)
2025-10-25 13:18:08,315 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=9345/tcp)
2025-10-25 13:18:08,817 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=6443/tcp)
2025-10-25 13:18:09,339 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=10250/tcp)
2025-10-25 13:18:09,833 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=2379:2381/tcp)
2025-10-25 13:18:10,390 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=30000:32767/tcp)
2025-10-25 13:18:10,940 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=8472/udp)
2025-10-25 13:18:11,529 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=6081/udp)
2025-10-25 13:18:12,106 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=4240/tcp)
2025-10-25 13:18:12,679 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=4244/tcp)
2025-10-25 13:18:13,223 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=4245/tcp)
2025-10-25 13:18:13,784 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=4222/tcp)
2025-10-25 13:18:14,343 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=9966/tcp)
2025-10-25 13:18:14,931 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=4250:4251/tcp)
2025-10-25 13:18:15,504 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=6060:6062/tcp)
2025-10-25 13:18:16,113 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=9878:9879/tcp)
2025-10-25 13:18:16,702 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=9890:9893/tcp)
2025-10-25 13:18:17,272 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=9901/tcp)
2025-10-25 13:18:17,868 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=9962:9964/tcp)
2025-10-25 13:18:18,482 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=51871/udp)
2025-10-25 13:18:19,048 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=4567/tcp)
2025-10-25 13:18:19,647 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=4567/udp)
2025-10-25 13:18:20,288 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=4568/tcp)
2025-10-25 13:18:20,851 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=4444/tcp)
2025-10-25 13:18:21,485 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=7443/tcp)
2025-10-25 13:18:22,067 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=7345/tcp)
2025-10-25 13:18:22,673 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=8080/tcp)
2025-10-25 13:18:23,271 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=8443/tcp)
2025-10-25 13:18:23,818 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=3306/tcp)
2025-10-25 13:18:24,408 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=3307/tcp)
2025-10-25 13:18:25,034 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=80/tcp)
2025-10-25 13:18:25,570 p=3297371 u=root n=ansible | changed: [k8s-w03] => (item=443/tcp)
2025-10-25 13:18:26,814 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Ensure ufw enabled] ********************************************************
2025-10-25 13:18:26,815 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:18:26,892 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:18:26,898 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:18:26,963 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:18:26,965 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:18:28,026 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:18:30,379 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Reload ufw] ****************************************************************
2025-10-25 13:18:30,379 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:18:30,408 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:18:30,415 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:18:30,464 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:18:30,472 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:18:32,779 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:18:33,632 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Create RKE2 artifacts folder] **********************************************
2025-10-25 13:18:33,633 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:18:33,634 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:18:33,673 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:18:33,683 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:18:33,719 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:18:33,994 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:18:34,756 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Download sha256 checksum file ( airgap mode )] *****************************
2025-10-25 13:18:34,756 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:18:34,760 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:18:34,781 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:18:34,884 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:18:34,895 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:18:35,307 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:18:36,033 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 artifacts and compare with checksums ( airgap mode )] ********
2025-10-25 13:18:36,034 p=3297371 u=root n=ansible | ok: [k8s-m1] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 13:18:36,094 p=3297371 u=root n=ansible | ok: [k8s-w01] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 13:18:36,106 p=3297371 u=root n=ansible | ok: [k8s-m2] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 13:18:36,109 p=3297371 u=root n=ansible | ok: [k8s-m3] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 13:18:36,157 p=3297371 u=root n=ansible | ok: [k8s-w02] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 13:18:40,045 p=3297371 u=root n=ansible | ok: [k8s-w01] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 13:18:40,088 p=3297371 u=root n=ansible | ok: [k8s-w02] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 13:18:40,174 p=3297371 u=root n=ansible | ok: [k8s-m1] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 13:18:40,185 p=3297371 u=root n=ansible | ok: [k8s-m2] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 13:18:40,330 p=3297371 u=root n=ansible | ok: [k8s-m3] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 13:18:42,482 p=3297371 u=root n=ansible | ok: [k8s-w01] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 13:18:42,601 p=3297371 u=root n=ansible | ok: [k8s-w02] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 13:18:42,657 p=3297371 u=root n=ansible | ok: [k8s-m1] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 13:18:42,677 p=3297371 u=root n=ansible | ok: [k8s-m2] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 13:18:42,827 p=3297371 u=root n=ansible | ok: [k8s-m3] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 13:18:43,194 p=3297371 u=root n=ansible | ok: [k8s-w03] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 13:18:47,544 p=3297371 u=root n=ansible | ok: [k8s-w03] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 13:18:50,379 p=3297371 u=root n=ansible | ok: [k8s-w03] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 13:18:51,080 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 install script ( airgap mode )] ******************************
2025-10-25 13:18:51,081 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:18:51,085 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:18:51,087 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:18:51,137 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:18:51,163 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:18:51,855 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:18:58,901 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Populate service facts] ****************************************************
2025-10-25 13:18:58,901 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:18:58,908 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:18:58,925 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:18:58,934 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:18:59,028 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:19:04,498 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:19:04,904 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Get stats of the FS object] ************************************************
2025-10-25 13:19:04,905 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:19:04,912 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:19:04,943 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:19:04,960 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:19:04,998 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:19:05,257 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:19:05,643 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Check if separate partition] ***********************************************
2025-10-25 13:19:05,644 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:19:05,653 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:19:05,692 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:19:05,706 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:19:05,717 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:19:05,987 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:19:06,060 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 bin path] *********************************************************
2025-10-25 13:19:06,061 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:19:06,082 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:19:06,105 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:19:06,130 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:19:06,130 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:19:06,149 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:19:06,858 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Check RKE2 version] ********************************************************
2025-10-25 13:19:06,858 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:19:06,864 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:19:06,867 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:19:06,889 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:19:06,904 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:19:07,495 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:19:07,582 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 versions] *********************************************************
2025-10-25 13:19:07,583 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:19:07,609 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:19:07,631 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:19:07,634 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:19:07,662 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:19:07,682 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:19:07,823 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Prevent accidental RKE2 downgrade] *****************************************
2025-10-25 13:19:07,825 p=3297371 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: |-
    Version check passed. Current running version: ['1.34.1'], desired version: ['1.34.1']."
2025-10-25 13:19:07,826 p=3297371 u=root n=ansible | ok: [k8s-m2] => changed=false 
  msg: |-
    Version check passed. Current running version: ['1.34.1'], desired version: ['1.34.1']."
2025-10-25 13:19:07,826 p=3297371 u=root n=ansible | ok: [k8s-m3] => changed=false 
  msg: |-
    Version check passed. Current running version: ['1.34.1'], desired version: ['1.34.1']."
2025-10-25 13:19:07,826 p=3297371 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: |-
    Version check passed. Current running version: ['1.34.1'], desired version: ['1.34.1']."
2025-10-25 13:19:07,844 p=3297371 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: |-
    Version check passed. Current running version: ['1.34.1'], desired version: ['1.34.1']."
2025-10-25 13:19:07,874 p=3297371 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: |-
    Version check passed. Current running version: ['1.34.1'], desired version: ['1.34.1']."
2025-10-25 13:19:08,847 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Find Active Server] ********************************************************
2025-10-25 13:19:09,028 p=3297371 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/find_active_server.yml for k8s-m1, k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 13:19:14,242 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Populate services facts] ***************************************************
2025-10-25 13:19:14,243 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:19:14,300 p=3297371 u=root n=ansible | ok: [k8s-m2]
2025-10-25 13:19:14,312 p=3297371 u=root n=ansible | ok: [k8s-m3]
2025-10-25 13:19:14,319 p=3297371 u=root n=ansible | ok: [k8s-w01]
2025-10-25 13:19:14,339 p=3297371 u=root n=ansible | ok: [k8s-w02]
2025-10-25 13:19:19,804 p=3297371 u=root n=ansible | ok: [k8s-w03]
2025-10-25 13:19:20,341 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Copy kube-vip manifests to the masternode] *********************************
2025-10-25 13:19:20,443 p=3297371 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/kubevip.yml for k8s-m1
2025-10-25 13:19:20,891 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Create the RKE2 manifests directory] ***************************************
2025-10-25 13:19:20,892 p=3297371 u=root n=ansible | changed: [k8s-m1]
2025-10-25 13:19:21,905 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Copy kube-vip files to first server] ***************************************
2025-10-25 13:19:21,906 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=/opt/git/ansible/roles/rke2-1.49.0/templates/kube-vip/kube-vip.yml.j2)
2025-10-25 13:19:22,684 p=3297371 u=root n=ansible | changed: [k8s-m1] => (item=/opt/git/ansible/roles/rke2-1.49.0/templates/kube-vip/kube-vip-rbac.yml.j2)
2025-10-25 13:19:23,116 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Prepare very first server node in the cluster] *****************************
2025-10-25 13:19:23,212 p=3297371 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/first_server.yml for k8s-m1
2025-10-25 13:19:23,679 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Create the RKE2 config dir] ************************************************
2025-10-25 13:19:23,679 p=3297371 u=root n=ansible | changed: [k8s-m1]
2025-10-25 13:19:23,746 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Set server taints] *********************************************************
2025-10-25 13:19:23,747 p=3297371 u=root n=ansible | ok: [k8s-m1]
2025-10-25 13:19:24,745 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Copy rke2 config] **********************************************************
2025-10-25 13:19:24,746 p=3297371 u=root n=ansible | changed: [k8s-m1]
2025-10-25 13:19:25,652 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Copy Containerd Registry Configuration file] *******************************
2025-10-25 13:19:25,652 p=3297371 u=root n=ansible | changed: [k8s-m1]
2025-10-25 13:19:28,066 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Start RKE2 service on the first server] ************************************
2025-10-25 13:19:28,067 p=3297371 u=root n=ansible | fatal: [k8s-m1]: FAILED! => changed=false 
  msg: |-
    Unable to start service rke2-server.service: Job for rke2-server.service failed because the control process exited with error code.
    See "systemctl status rke2-server.service" and "journalctl -xeu rke2-server.service" for details.
2025-10-25 13:19:29,496 p=3297371 u=root n=ansible | TASK [rke2-1.49.0 : Final steps] ***************************************************************
2025-10-25 13:19:29,624 p=3297371 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/summary.yml for k8s-m2, k8s-m3, k8s-w01, k8s-w02, k8s-w03
2025-10-25 13:19:30,382 p=3297371 u=root n=ansible | PLAY RECAP *************************************************************************************
2025-10-25 13:19:30,383 p=3297371 u=root n=ansible | k8s-m1                     : ok=33   changed=7    unreachable=0    failed=1    skipped=38   rescued=0    ignored=0   
2025-10-25 13:19:30,383 p=3297371 u=root n=ansible | k8s-m2                     : ok=26   changed=2    unreachable=0    failed=0    skipped=40   rescued=0    ignored=0   
2025-10-25 13:19:30,384 p=3297371 u=root n=ansible | k8s-m3                     : ok=26   changed=2    unreachable=0    failed=0    skipped=40   rescued=0    ignored=0   
2025-10-25 13:19:30,384 p=3297371 u=root n=ansible | k8s-w01                    : ok=26   changed=2    unreachable=0    failed=0    skipped=40   rescued=0    ignored=0   
2025-10-25 13:19:30,384 p=3297371 u=root n=ansible | k8s-w02                    : ok=26   changed=2    unreachable=0    failed=0    skipped=40   rescued=0    ignored=0   
2025-10-25 13:19:30,385 p=3297371 u=root n=ansible | k8s-w03                    : ok=26   changed=2    unreachable=0    failed=0    skipped=40   rescued=0    ignored=0   
2025-10-25 16:17:26,663 p=3358610 u=root n=ansible | [WARNING]: Could not match supplied host pattern, ignoring:
./playbooks/rke2-airgap-install.yml

2025-10-25 16:17:26,663 p=3358610 u=root n=ansible | [WARNING]: No hosts matched, nothing to do

2025-10-25 16:17:26,669 p=3358610 u=root n=ansible | ERROR! No argument passed to command module (did you mean to run ansible-playbook?)
2025-10-25 16:17:38,453 p=3358940 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ****
2025-10-25 16:17:41,078 p=3358940 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 16:17:41,079 p=3358940 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:17:41,116 p=3358940 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:17:41,155 p=3358940 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:17:41,179 p=3358940 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:17:41,286 p=3358940 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 16:17:41,287 p=3358940 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:17:41,296 p=3358940 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:17:41,314 p=3358940 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:17:41,328 p=3358940 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:17:42,439 p=3358940 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] **********************
2025-10-25 16:17:42,440 p=3358940 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:17:42,443 p=3358940 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:17:42,445 p=3358940 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:17:42,453 p=3358940 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:17:42,550 p=3358940 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] ******************
2025-10-25 16:17:42,646 p=3358940 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-w01, k8s-w02, k8s-w03
2025-10-25 16:17:43,771 p=3358940 u=root n=ansible | TASK [rke2-1.49.0 : Configure kernel modules] **********************************
2025-10-25 16:17:43,772 p=3358940 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:17:43,779 p=3358940 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:17:43,780 p=3358940 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:17:43,789 p=3358940 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:17:44,593 p=3358940 u=root n=ansible | TASK [rke2-1.49.0 : Configure NetworkManager] **********************************
2025-10-25 16:17:44,594 p=3358940 u=root n=ansible | fatal: [k8s-w01]: FAILED! => changed=false 
  checksum: c11aaa8955f6ea532d9ee2383df3e7fb9504d2a2
  msg: Destination directory /etc/NetworkManager/conf.d does not exist
2025-10-25 16:17:44,606 p=3358940 u=root n=ansible | fatal: [k8s-m1]: FAILED! => changed=false 
  checksum: c11aaa8955f6ea532d9ee2383df3e7fb9504d2a2
  msg: Destination directory /etc/NetworkManager/conf.d does not exist
2025-10-25 16:17:44,623 p=3358940 u=root n=ansible | fatal: [k8s-w03]: FAILED! => changed=false 
  checksum: c11aaa8955f6ea532d9ee2383df3e7fb9504d2a2
  msg: Destination directory /etc/NetworkManager/conf.d does not exist
2025-10-25 16:17:44,631 p=3358940 u=root n=ansible | fatal: [k8s-w02]: FAILED! => changed=false 
  checksum: c11aaa8955f6ea532d9ee2383df3e7fb9504d2a2
  msg: Destination directory /etc/NetworkManager/conf.d does not exist
2025-10-25 16:17:44,633 p=3358940 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 16:17:44,633 p=3358940 u=root n=ansible | k8s-m1                     : ok=5    changed=2    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0   
2025-10-25 16:17:44,633 p=3358940 u=root n=ansible | k8s-w01                    : ok=5    changed=2    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0   
2025-10-25 16:17:44,633 p=3358940 u=root n=ansible | k8s-w02                    : ok=5    changed=2    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0   
2025-10-25 16:17:44,634 p=3358940 u=root n=ansible | k8s-w03                    : ok=5    changed=2    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0   
2025-10-25 16:23:01,912 p=3363426 u=root n=ansible | playbook: playbooks/rke2-airgap-install.yml
2025-10-25 16:23:50,821 p=3364537 u=root n=ansible | playbook: playbooks/rke2-airgap-install.yml
2025-10-25 16:23:55,469 p=3364663 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ****
2025-10-25 16:23:57,946 p=3364663 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 16:23:57,947 p=3364663 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:23:57,972 p=3364663 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:23:57,988 p=3364663 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:23:58,018 p=3364663 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:23:58,137 p=3364663 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 16:23:58,137 p=3364663 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:23:58,137 p=3364663 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:23:58,145 p=3364663 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:23:58,177 p=3364663 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:23:59,242 p=3364663 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] **********************
2025-10-25 16:23:59,242 p=3364663 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:23:59,261 p=3364663 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:23:59,270 p=3364663 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:23:59,286 p=3364663 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:23:59,390 p=3364663 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] ******************
2025-10-25 16:23:59,509 p=3364663 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-w01, k8s-w02, k8s-w03
2025-10-25 16:24:00,566 p=3364663 u=root n=ansible | TASK [rke2-1.49.0 : Configure kernel modules] **********************************
2025-10-25 16:24:00,567 p=3364663 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:24:00,567 p=3364663 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:24:00,580 p=3364663 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:24:00,606 p=3364663 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:24:01,680 p=3364663 u=root n=ansible | TASK [rke2-1.49.0 : Configure NetworkManager] **********************************
2025-10-25 16:24:01,682 p=3364663 u=root n=ansible | fatal: [k8s-w01]: FAILED! => changed=false 
  checksum: c11aaa8955f6ea532d9ee2383df3e7fb9504d2a2
  msg: Destination directory /etc/NetworkManager/conf.d does not exist
2025-10-25 16:24:01,703 p=3364663 u=root n=ansible | fatal: [k8s-m1]: FAILED! => changed=false 
  checksum: c11aaa8955f6ea532d9ee2383df3e7fb9504d2a2
  msg: Destination directory /etc/NetworkManager/conf.d does not exist
2025-10-25 16:24:01,709 p=3364663 u=root n=ansible | fatal: [k8s-w02]: FAILED! => changed=false 
  checksum: c11aaa8955f6ea532d9ee2383df3e7fb9504d2a2
  msg: Destination directory /etc/NetworkManager/conf.d does not exist
2025-10-25 16:24:01,716 p=3364663 u=root n=ansible | fatal: [k8s-w03]: FAILED! => changed=false 
  checksum: c11aaa8955f6ea532d9ee2383df3e7fb9504d2a2
  msg: Destination directory /etc/NetworkManager/conf.d does not exist
2025-10-25 16:24:01,718 p=3364663 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 16:24:01,719 p=3364663 u=root n=ansible | k8s-m1                     : ok=5    changed=0    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0   
2025-10-25 16:24:01,719 p=3364663 u=root n=ansible | k8s-w01                    : ok=5    changed=0    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0   
2025-10-25 16:24:01,719 p=3364663 u=root n=ansible | k8s-w02                    : ok=5    changed=0    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0   
2025-10-25 16:24:01,719 p=3364663 u=root n=ansible | k8s-w03                    : ok=5    changed=0    unreachable=0    failed=1    skipped=1    rescued=0    ignored=0   
2025-10-25 16:26:50,961 p=3367871 u=root n=ansible | playbook: playbooks/rke2-airgap-install.yml
2025-10-25 16:27:56,140 p=3369269 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ****
2025-10-25 16:27:57,847 p=3369269 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 16:27:57,849 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:27:57,899 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:27:57,902 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:27:58,938 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:27:59,042 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 16:27:59,042 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:27:59,058 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:27:59,070 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:27:59,077 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:28:00,459 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] **********************
2025-10-25 16:28:00,460 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:28:00,464 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:28:00,469 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:28:00,473 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:28:00,586 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] ******************
2025-10-25 16:28:00,699 p=3369269 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-w01, k8s-w02, k8s-w03
2025-10-25 16:28:01,854 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Configure kernel modules] **********************************
2025-10-25 16:28:01,855 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:28:01,858 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:28:01,871 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:28:01,872 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:28:02,229 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Detect NetworkManager] *************************************
2025-10-25 16:28:02,229 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:28:02,241 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:28:02,252 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:28:02,269 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:28:03,355 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Configure sysctl parameters] *******************************
2025-10-25 16:28:03,356 p=3369269 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:28:03,358 p=3369269 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:28:03,368 p=3369269 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:28:03,380 p=3369269 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:28:04,017 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Apply system configurations] *******************************
2025-10-25 16:28:04,018 p=3369269 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:28:04,020 p=3369269 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:28:04,045 p=3369269 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:28:04,064 p=3369269 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:28:06,337 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Ensure UFW is installed] ***********************************
2025-10-25 16:28:06,338 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:28:06,367 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:28:06,372 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:28:06,428 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:28:08,034 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Enable UFW] ************************************************
2025-10-25 16:28:08,035 p=3369269 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:28:08,083 p=3369269 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:28:08,106 p=3369269 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:28:08,139 p=3369269 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:28:10,717 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Set default incoming policy to deny] ***********************
2025-10-25 16:28:10,717 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:28:10,734 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:28:10,771 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:28:10,792 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:28:13,376 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Set default outgoing policy to allow] **********************
2025-10-25 16:28:13,376 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:28:13,388 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:28:13,417 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:28:13,421 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:28:14,357 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Allow required ports] **************************************
2025-10-25 16:28:14,358 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=22/tcp)
2025-10-25 16:28:14,379 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=22/tcp)
2025-10-25 16:28:14,400 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=22/tcp)
2025-10-25 16:28:14,421 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=22/tcp)
2025-10-25 16:28:15,311 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=179/tcp)
2025-10-25 16:28:15,348 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=179/tcp)
2025-10-25 16:28:15,389 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=179/tcp)
2025-10-25 16:28:15,435 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=179/tcp)
2025-10-25 16:28:16,250 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=9345/tcp)
2025-10-25 16:28:16,282 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=9345/tcp)
2025-10-25 16:28:16,370 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=9345/tcp)
2025-10-25 16:28:16,444 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=9345/tcp)
2025-10-25 16:28:17,161 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=6443/tcp)
2025-10-25 16:28:17,183 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=6443/tcp)
2025-10-25 16:28:17,306 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=6443/tcp)
2025-10-25 16:28:17,336 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=6443/tcp)
2025-10-25 16:28:18,073 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=10250/tcp)
2025-10-25 16:28:18,090 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=10250/tcp)
2025-10-25 16:28:18,232 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=10250/tcp)
2025-10-25 16:28:18,236 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=10250/tcp)
2025-10-25 16:28:18,964 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=2379/tcp)
2025-10-25 16:28:18,989 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=2379/tcp)
2025-10-25 16:28:19,131 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=2379/tcp)
2025-10-25 16:28:19,191 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=2379/tcp)
2025-10-25 16:28:19,875 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=2380/tcp)
2025-10-25 16:28:19,875 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=2380/tcp)
2025-10-25 16:28:20,045 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=2380/tcp)
2025-10-25 16:28:20,085 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=2380/tcp)
2025-10-25 16:28:20,737 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=2381/tcp)
2025-10-25 16:28:20,749 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=2381/tcp)
2025-10-25 16:28:20,939 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=2381/tcp)
2025-10-25 16:28:20,948 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=2381/tcp)
2025-10-25 16:28:21,635 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=30000:32767/tcp)
2025-10-25 16:28:21,648 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=30000:32767/tcp)
2025-10-25 16:28:21,808 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=30000:32767/tcp)
2025-10-25 16:28:21,843 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=30000:32767/tcp)
2025-10-25 16:28:22,540 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=8472/udp)
2025-10-25 16:28:22,541 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=8472/udp)
2025-10-25 16:28:22,709 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=8472/udp)
2025-10-25 16:28:22,716 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=8472/udp)
2025-10-25 16:28:23,524 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=6081/udp)
2025-10-25 16:28:23,539 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=6081/udp)
2025-10-25 16:28:23,686 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=6081/udp)
2025-10-25 16:28:23,720 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=6081/udp)
2025-10-25 16:28:24,486 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=4240/tcp)
2025-10-25 16:28:24,487 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=4240/tcp)
2025-10-25 16:28:24,608 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=4240/tcp)
2025-10-25 16:28:24,675 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=4240/tcp)
2025-10-25 16:28:25,486 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=4244/tcp)
2025-10-25 16:28:25,558 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=4244/tcp)
2025-10-25 16:28:25,643 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=4244/tcp)
2025-10-25 16:28:25,734 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=4244/tcp)
2025-10-25 16:28:26,432 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=4245/tcp)
2025-10-25 16:28:26,544 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=4245/tcp)
2025-10-25 16:28:26,606 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=4245/tcp)
2025-10-25 16:28:26,730 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=4245/tcp)
2025-10-25 16:28:27,375 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=4222/tcp)
2025-10-25 16:28:27,565 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=4222/tcp)
2025-10-25 16:28:27,600 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=4222/tcp)
2025-10-25 16:28:27,675 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=4222/tcp)
2025-10-25 16:28:28,393 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=9966/tcp)
2025-10-25 16:28:28,543 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=9966/tcp)
2025-10-25 16:28:28,553 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=9966/tcp)
2025-10-25 16:28:28,715 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=9966/tcp)
2025-10-25 16:28:29,390 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=4250:4251/tcp)
2025-10-25 16:28:29,628 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=4250:4251/tcp)
2025-10-25 16:28:29,634 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=4250:4251/tcp)
2025-10-25 16:28:29,699 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=4250:4251/tcp)
2025-10-25 16:28:30,375 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=6060:6062/tcp)
2025-10-25 16:28:30,615 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=6060:6062/tcp)
2025-10-25 16:28:30,645 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=6060:6062/tcp)
2025-10-25 16:28:31,380 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=9878:9879/tcp)
2025-10-25 16:28:31,590 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=6060:6062/tcp)
2025-10-25 16:28:31,664 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=9878:9879/tcp)
2025-10-25 16:28:32,305 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=9890:9893/tcp)
2025-10-25 16:28:32,503 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=9878:9879/tcp)
2025-10-25 16:28:32,581 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=9878:9879/tcp)
2025-10-25 16:28:32,599 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=9890:9893/tcp)
2025-10-25 16:28:33,215 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=9901/tcp)
2025-10-25 16:28:33,422 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=9890:9893/tcp)
2025-10-25 16:28:33,473 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=9901/tcp)
2025-10-25 16:28:33,521 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=9890:9893/tcp)
2025-10-25 16:28:34,152 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=9962:9964/tcp)
2025-10-25 16:28:34,359 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=9901/tcp)
2025-10-25 16:28:34,389 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=9962:9964/tcp)
2025-10-25 16:28:34,466 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=9901/tcp)
2025-10-25 16:28:35,143 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=51871/udp)
2025-10-25 16:28:35,292 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=9962:9964/tcp)
2025-10-25 16:28:35,329 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=9962:9964/tcp)
2025-10-25 16:28:35,353 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=51871/udp)
2025-10-25 16:28:36,073 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=4567/tcp)
2025-10-25 16:28:36,238 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=51871/udp)
2025-10-25 16:28:36,240 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=51871/udp)
2025-10-25 16:28:36,259 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=4567/tcp)
2025-10-25 16:28:37,004 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=4567/udp)
2025-10-25 16:28:37,154 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=4567/tcp)
2025-10-25 16:28:37,191 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=4567/tcp)
2025-10-25 16:28:37,234 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=4567/udp)
2025-10-25 16:28:37,963 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=4568/tcp)
2025-10-25 16:28:38,183 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=4568/tcp)
2025-10-25 16:28:38,186 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=4567/udp)
2025-10-25 16:28:38,199 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=4567/udp)
2025-10-25 16:28:38,943 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=4444/tcp)
2025-10-25 16:28:39,177 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=4568/tcp)
2025-10-25 16:28:39,178 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=4568/tcp)
2025-10-25 16:28:39,210 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=4444/tcp)
2025-10-25 16:28:39,900 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=7443/tcp)
2025-10-25 16:28:40,088 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=4444/tcp)
2025-10-25 16:28:40,231 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=4444/tcp)
2025-10-25 16:28:40,419 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=7443/tcp)
2025-10-25 16:28:40,848 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=7345/tcp)
2025-10-25 16:28:41,059 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=7443/tcp)
2025-10-25 16:28:41,156 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=7443/tcp)
2025-10-25 16:28:41,384 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=7345/tcp)
2025-10-25 16:28:41,792 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=8080/tcp)
2025-10-25 16:28:42,080 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=7345/tcp)
2025-10-25 16:28:42,351 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=8080/tcp)
2025-10-25 16:28:42,747 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=8443/tcp)
2025-10-25 16:28:43,021 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=8080/tcp)
2025-10-25 16:28:43,223 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=7345/tcp)
2025-10-25 16:28:43,413 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=8443/tcp)
2025-10-25 16:28:43,777 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=3306/tcp)
2025-10-25 16:28:44,013 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=8443/tcp)
2025-10-25 16:28:44,230 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=8080/tcp)
2025-10-25 16:28:44,381 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=3306/tcp)
2025-10-25 16:28:44,729 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=3307/tcp)
2025-10-25 16:28:44,942 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=3306/tcp)
2025-10-25 16:28:45,210 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=8443/tcp)
2025-10-25 16:28:45,301 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=3307/tcp)
2025-10-25 16:28:45,709 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=80/tcp)
2025-10-25 16:28:45,855 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=3307/tcp)
2025-10-25 16:28:46,158 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=3306/tcp)
2025-10-25 16:28:46,236 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=80/tcp)
2025-10-25 16:28:46,655 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=443/tcp)
2025-10-25 16:28:46,809 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=80/tcp)
2025-10-25 16:28:47,248 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=443/tcp)
2025-10-25 16:28:47,250 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=3307/tcp)
2025-10-25 16:28:47,799 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=443/tcp)
2025-10-25 16:28:48,312 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=80/tcp)
2025-10-25 16:28:49,378 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=443/tcp)
2025-10-25 16:28:49,953 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Create RKE2 artifacts folder] ******************************
2025-10-25 16:28:49,954 p=3369269 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:28:49,954 p=3369269 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:28:49,975 p=3369269 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:28:50,017 p=3369269 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:28:51,070 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Download sha256 checksum file ( airgap mode )] *************
2025-10-25 16:28:51,071 p=3369269 u=root n=ansible | [WARNING]: Module remote_tmp /root/.ansible/tmp did not exist and was created
with a mode of 0700, this may cause issues when running as another user. To
avoid this, create the remote_tmp dir with the correct permissions manually

2025-10-25 16:28:51,071 p=3369269 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:28:51,072 p=3369269 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:28:51,079 p=3369269 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:28:51,083 p=3369269 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:28:52,457 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 artifacts and compare with checksums ( airgap mode )] ***
2025-10-25 16:28:52,457 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 16:28:52,689 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 16:28:52,761 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 16:28:52,772 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 16:29:15,074 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 16:29:15,693 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 16:29:16,180 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 16:29:41,115 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 16:29:41,210 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 16:29:42,570 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 16:29:44,942 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 16:30:34,225 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 16:30:35,121 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 16:30:35,123 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 16:30:53,062 p=3369269 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 16:30:53,063 p=3369269 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 16:30:53,163 p=3369269 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 16:30:55,038 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 16:31:09,628 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 16:31:13,391 p=3369269 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 16:31:15,905 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 install script ( airgap mode )] **************
2025-10-25 16:31:15,907 p=3369269 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:31:15,907 p=3369269 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:31:15,912 p=3369269 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:31:15,918 p=3369269 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:31:24,327 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Populate service facts] ************************************
2025-10-25 16:31:24,328 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:31:24,404 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:31:24,456 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:31:24,812 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:31:25,183 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Get stats of the FS object] ********************************
2025-10-25 16:31:25,184 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:31:25,195 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:31:25,198 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:31:25,378 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:31:25,770 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Check if separate partition] *******************************
2025-10-25 16:31:25,771 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:31:25,782 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:31:25,783 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:31:25,829 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:31:25,889 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 bin path] *****************************************
2025-10-25 16:31:25,890 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:31:25,912 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:31:25,915 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:31:25,931 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:31:26,328 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Check RKE2 version] ****************************************
2025-10-25 16:31:26,329 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:31:26,374 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:31:26,376 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:31:26,392 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:31:26,480 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 versions] *****************************************
2025-10-25 16:31:26,481 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:31:26,482 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:31:26,495 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:31:26,530 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:32:12,406 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Run RKE2 install script with airgap variables] *************
2025-10-25 16:32:12,406 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:32:37,354 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:32:38,480 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:32:41,708 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:32:42,269 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Find Active Server] ****************************************
2025-10-25 16:32:42,394 p=3369269 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/find_active_server.yml for k8s-m1, k8s-w01, k8s-w02, k8s-w03
2025-10-25 16:32:47,739 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Populate services facts] ***********************************
2025-10-25 16:32:47,740 p=3369269 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:32:47,783 p=3369269 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:32:47,853 p=3369269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:32:47,948 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:32:48,503 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Prepare very first server node in the cluster] *************
2025-10-25 16:32:48,550 p=3369269 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/first_server.yml for k8s-m1
2025-10-25 16:32:49,087 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Create the RKE2 config dir] ********************************
2025-10-25 16:32:49,088 p=3369269 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:32:49,140 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Set server taints] *****************************************
2025-10-25 16:32:49,140 p=3369269 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:32:50,277 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Copy rke2 config] ******************************************
2025-10-25 16:32:50,277 p=3369269 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:32:51,579 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Copy Containerd Registry Configuration file] ***************
2025-10-25 16:32:51,579 p=3369269 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:32:54,003 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Start RKE2 service on the first server] ********************
2025-10-25 16:32:54,004 p=3369269 u=root n=ansible | fatal: [k8s-m1]: FAILED! => changed=false 
  msg: |-
    Unable to start service rke2-server.service: Job for rke2-server.service failed because the control process exited with error code.
    See "systemctl status rke2-server.service" and "journalctl -xeu rke2-server.service" for details.
2025-10-25 16:32:55,077 p=3369269 u=root n=ansible | RUNNING HANDLER [rke2-1.49.0 : reload sysctl] **********************************
2025-10-25 16:32:55,077 p=3369269 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:32:55,083 p=3369269 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:32:55,112 p=3369269 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:32:55,291 p=3369269 u=root n=ansible | TASK [rke2-1.49.0 : Final steps] ***********************************************
2025-10-25 16:32:55,361 p=3369269 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/summary.yml for k8s-w01, k8s-w02, k8s-w03
2025-10-25 16:32:55,719 p=3369269 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 16:32:55,719 p=3369269 u=root n=ansible | k8s-m1                     : ok=31   changed=11   unreachable=0    failed=1    skipped=39   rescued=0    ignored=0   
2025-10-25 16:32:55,720 p=3369269 u=root n=ansible | k8s-w01                    : ok=28   changed=9    unreachable=0    failed=0    skipped=41   rescued=0    ignored=0   
2025-10-25 16:32:55,720 p=3369269 u=root n=ansible | k8s-w02                    : ok=28   changed=9    unreachable=0    failed=0    skipped=41   rescued=0    ignored=0   
2025-10-25 16:32:55,720 p=3369269 u=root n=ansible | k8s-w03                    : ok=28   changed=9    unreachable=0    failed=0    skipped=41   rescued=0    ignored=0   
2025-10-25 16:39:04,535 p=3384970 u=root n=ansible | [WARNING]: Could not match supplied host pattern, ignoring: k8s-m2

2025-10-25 16:39:04,535 p=3384970 u=root n=ansible | [WARNING]: No hosts matched, nothing to do

2025-10-25 16:39:32,313 p=3385648 u=root n=ansible | @all:
  |--@ungrouped:
  |--@k8s_cluster:
  |  |--@masters:
  |  |  |--k8s-m1
  |  |--@workers:
  |  |  |--k8s-w01
  |  |  |--k8s-w02
  |  |  |--k8s-w03
2025-10-25 16:39:59,147 p=3386271 u=root n=ansible | k8s-m1 | FAILED | rc=3 >>
 rke2-server.service - Rancher Kubernetes Engine v2 (server)
     Loaded: loaded (/usr/local/lib/systemd/system/rke2-server.service; enabled; preset: enabled)
     Active: activating (auto-restart) (Result: exit-code) since Sat 2025-10-25 09:39:56 UTC; 4s ago
       Docs: https://github.com/rancher/rke2#readme
    Process: 11832 ExecStartPre=/bin/sh -xc ! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service (code=exited, status=0/SUCCESS)
    Process: 11835 ExecStartPre=/sbin/modprobe br_netfilter (code=exited, status=0/SUCCESS)
    Process: 11837 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)
    Process: 11839 ExecStart=/usr/local/bin/rke2 server (code=exited, status=1/FAILURE)
    Process: 11850 ExecStopPost=/bin/sh -c systemd-cgls /system.slice/rke2-server.service | grep -Eo '[0-9]+ (containerd|kubelet)' | awk '{print $1}' | xargs -r kill (code=exited, status=0/SUCCESS)
   Main PID: 11839 (code=exited, status=1/FAILURE)
        CPU: 300msnon-zero return code

2025-10-25 16:40:36,500 p=3387034 u=root n=ansible | k8s-m1 | FAILED | rc=3 >>
 rke2-server.service - Rancher Kubernetes Engine v2 (server)
     Loaded: loaded (/usr/local/lib/systemd/system/rke2-server.service; enabled; preset: enabled)
     Active: activating (auto-restart) (Result: exit-code) since Sat 2025-10-25 09:40:35 UTC; 3s ago
       Docs: https://github.com/rancher/rke2#readme
    Process: 12045 ExecStartPre=/bin/sh -xc ! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service (code=exited, status=0/SUCCESS)
    Process: 12048 ExecStartPre=/sbin/modprobe br_netfilter (code=exited, status=0/SUCCESS)
    Process: 12050 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)
    Process: 12052 ExecStart=/usr/local/bin/rke2 server (code=exited, status=1/FAILURE)
    Process: 12066 ExecStopPost=/bin/sh -c systemd-cgls /system.slice/rke2-server.service | grep -Eo '[0-9]+ (containerd|kubelet)' | awk '{print $1}' | xargs -r kill (code=exited, status=0/SUCCESS)
   Main PID: 12052 (code=exited, status=1/FAILURE)
        CPU: 230msnon-zero return code

2025-10-25 16:41:01,470 p=3387547 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
Oct 25 09:40:23 k8s-m1 rke2[12026]: time="2025-10-25T09:40:23Z" level=info msg="Applying Pod Security Admission Configuration"
Oct 25 09:40:23 k8s-m1 rke2[12026]: time="2025-10-25T09:40:23Z" level=fatal msg="Error: no default routes found in \"/proc/net/route\" or \"/proc/net/ipv6_route\""
Oct 25 09:40:23 k8s-m1 systemd[1]: rke2-server.service: Main process exited, code=exited, status=1/FAILURE
Oct 25 09:40:23 k8s-m1 systemd[1]: rke2-server.service: Failed with result 'exit-code'.
Oct 25 09:40:23 k8s-m1 systemd[1]: Failed to start rke2-server.service - Rancher Kubernetes Engine v2 (server).
Oct 25 09:40:28 k8s-m1 systemd[1]: rke2-server.service: Scheduled restart job, restart counter is at 82.
Oct 25 09:40:34 k8s-m1 systemd[1]: Starting rke2-server.service - Rancher Kubernetes Engine v2 (server)...
Oct 25 09:40:34 k8s-m1 sh[12045]: + /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service
Oct 25 09:40:35 k8s-m1 rke2[12052]: time="2025-10-25T09:40:35Z" level=warning msg="not running in CIS mode"
Oct 25 09:40:35 k8s-m1 rke2[12052]: time="2025-10-25T09:40:35Z" level=info msg="Applying Pod Security Admission Configuration"
Oct 25 09:40:35 k8s-m1 rke2[12052]: time="2025-10-25T09:40:35Z" level=fatal msg="Error: no default routes found in \"/proc/net/route\" or \"/proc/net/ipv6_route\""
Oct 25 09:40:35 k8s-m1 systemd[1]: rke2-server.service: Main process exited, code=exited, status=1/FAILURE
Oct 25 09:40:35 k8s-m1 systemd[1]: rke2-server.service: Failed with result 'exit-code'.
Oct 25 09:40:35 k8s-m1 systemd[1]: Failed to start rke2-server.service - Rancher Kubernetes Engine v2 (server).
Oct 25 09:40:40 k8s-m1 systemd[1]: rke2-server.service: Scheduled restart job, restart counter is at 83.
Oct 25 09:40:40 k8s-m1 systemd[1]: Starting rke2-server.service - Rancher Kubernetes Engine v2 (server)...
Oct 25 09:40:40 k8s-m1 sh[12077]: + /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service
Oct 25 09:40:40 k8s-m1 rke2[12084]: time="2025-10-25T09:40:40Z" level=warning msg="not running in CIS mode"
Oct 25 09:40:40 k8s-m1 rke2[12084]: time="2025-10-25T09:40:40Z" level=info msg="Applying Pod Security Admission Configuration"
Oct 25 09:40:40 k8s-m1 rke2[12084]: time="2025-10-25T09:40:40Z" level=fatal msg="Error: no default routes found in \"/proc/net/route\" or \"/proc/net/ipv6_route\""
Oct 25 09:40:40 k8s-m1 systemd[1]: rke2-server.service: Main process exited, code=exited, status=1/FAILURE
Oct 25 09:40:40 k8s-m1 systemd[1]: rke2-server.service: Failed with result 'exit-code'.
Oct 25 09:40:40 k8s-m1 systemd[1]: Failed to start rke2-server.service - Rancher Kubernetes Engine v2 (server).
Oct 25 09:40:45 k8s-m1 systemd[1]: rke2-server.service: Scheduled restart job, restart counter is at 84.
Oct 25 09:40:45 k8s-m1 systemd[1]: Starting rke2-server.service - Rancher Kubernetes Engine v2 (server)...
Oct 25 09:40:45 k8s-m1 sh[12101]: + /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service
Oct 25 09:40:45 k8s-m1 rke2[12107]: time="2025-10-25T09:40:45Z" level=warning msg="not running in CIS mode"
Oct 25 09:40:45 k8s-m1 rke2[12107]: time="2025-10-25T09:40:45Z" level=info msg="Applying Pod Security Admission Configuration"
Oct 25 09:40:45 k8s-m1 rke2[12107]: time="2025-10-25T09:40:45Z" level=fatal msg="Error: no default routes found in \"/proc/net/route\" or \"/proc/net/ipv6_route\""
Oct 25 09:40:45 k8s-m1 systemd[1]: rke2-server.service: Main process exited, code=exited, status=1/FAILURE
Oct 25 09:40:45 k8s-m1 systemd[1]: rke2-server.service: Failed with result 'exit-code'.
Oct 25 09:40:45 k8s-m1 systemd[1]: Failed to start rke2-server.service - Rancher Kubernetes Engine v2 (server).
Oct 25 09:40:51 k8s-m1 systemd[1]: rke2-server.service: Scheduled restart job, restart counter is at 85.
Oct 25 09:40:51 k8s-m1 systemd[1]: Starting rke2-server.service - Rancher Kubernetes Engine v2 (server)...
Oct 25 09:40:51 k8s-m1 sh[12126]: + /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service
Oct 25 09:40:51 k8s-m1 rke2[12133]: time="2025-10-25T09:40:51Z" level=warning msg="not running in CIS mode"
Oct 25 09:40:51 k8s-m1 rke2[12133]: time="2025-10-25T09:40:51Z" level=info msg="Applying Pod Security Admission Configuration"
Oct 25 09:40:51 k8s-m1 rke2[12133]: time="2025-10-25T09:40:51Z" level=fatal msg="Error: no default routes found in \"/proc/net/route\" or \"/proc/net/ipv6_route\""
Oct 25 09:40:51 k8s-m1 systemd[1]: rke2-server.service: Main process exited, code=exited, status=1/FAILURE
Oct 25 09:40:51 k8s-m1 systemd[1]: rke2-server.service: Failed with result 'exit-code'.
Oct 25 09:40:51 k8s-m1 systemd[1]: Failed to start rke2-server.service - Rancher Kubernetes Engine v2 (server).
Oct 25 09:40:56 k8s-m1 systemd[1]: rke2-server.service: Scheduled restart job, restart counter is at 86.
Oct 25 09:41:00 k8s-m1 systemd[1]: Starting rke2-server.service - Rancher Kubernetes Engine v2 (server)...
Oct 25 09:41:00 k8s-m1 sh[12151]: + /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service
Oct 25 09:41:00 k8s-m1 rke2[12157]: time="2025-10-25T09:41:00Z" level=warning msg="not running in CIS mode"
Oct 25 09:41:00 k8s-m1 rke2[12157]: time="2025-10-25T09:41:00Z" level=info msg="Applying Pod Security Admission Configuration"
Oct 25 09:41:00 k8s-m1 rke2[12157]: time="2025-10-25T09:41:00Z" level=fatal msg="Error: no default routes found in \"/proc/net/route\" or \"/proc/net/ipv6_route\""
Oct 25 09:41:00 k8s-m1 systemd[1]: rke2-server.service: Main process exited, code=exited, status=1/FAILURE
Oct 25 09:41:00 k8s-m1 systemd[1]: rke2-server.service: Failed with result 'exit-code'.
Oct 25 09:41:00 k8s-m1 systemd[1]: Failed to start rke2-server.service - Rancher Kubernetes Engine v2 (server).

2025-10-25 16:41:31,621 p=3388135 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
FOUND /etc/rancher/rke2/config.yaml
-rw------- 1 root root 932 Oct 25 09:32 /etc/rancher/rke2/config.yaml
owner=root group=root mode=600
--- BEGIN CONTENT ---
agent-token: O+HkGqhGqCukrMS5
token: O+HkGqhGqCukrMS5
data-dir: /var/lib/rancher/rke2
cni: ['canal']
tls-san:
  - cluster.local
  - 10.0.6.11
cluster-domain: cluster.local
node-taint:
  - CriticalAddonsOnly=true:NoExecute
  - node-role.kubernetes.io/control-plane=true:NoSchedule
  - node-role.kubernetes.io/etcd=true:NoExecute
disable: ['rke2-metrics-server']
kube-apiserver-arg: ['--default-not-ready-toleration-seconds=60', '--default-unreachable-toleration-seconds=40']
snapshotter: overlayfs
node-name: k8s-m1
kube-controller-manager-arg:
  - --node-monitor-period=4s
  - --allocate-node-cidrs=true
  - --bind-address=0.0.0.0
  - --terminated-pod-gc-threshold=50
kube-scheduler-arg:
  - --bind-address=0.0.0.0
kubelet-arg:
  - --node-status-update-frequency=4s
  - --max-pods=100
disable-cloud-controller: true
cloud-provider-name: ""
cluster-cidr: "10.42.0.0/16"
service-cidr: "10.43.0.0/16"
ingress-controller: ingress-nginx
--- END CONTENT ---

2025-10-25 16:41:52,809 p=3388543 u=root n=ansible | k8s-m1 | FAILED | rc=1 >>
Job for rke2-server.service failed because the control process exited with error code.
See "systemctl status rke2-server.service" and "journalctl -xeu rke2-server.service" for details.non-zero return code

2025-10-25 16:42:01,293 p=3388794 u=root n=ansible | k8s-m1 | FAILED | rc=3 >>
 rke2-server.service - Rancher Kubernetes Engine v2 (server)
     Loaded: loaded (/usr/local/lib/systemd/system/rke2-server.service; enabled; preset: enabled)
     Active: activating (auto-restart) (Result: exit-code) since Sat 2025-10-25 09:42:00 UTC; 2s ago
       Docs: https://github.com/rancher/rke2#readme
    Process: 12445 ExecStartPre=/bin/sh -xc ! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service (code=exited, status=0/SUCCESS)
    Process: 12447 ExecStartPre=/sbin/modprobe br_netfilter (code=exited, status=0/SUCCESS)
    Process: 12449 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)
    Process: 12452 ExecStart=/usr/local/bin/rke2 server (code=exited, status=1/FAILURE)
    Process: 12464 ExecStopPost=/bin/sh -c systemd-cgls /system.slice/rke2-server.service | grep -Eo '[0-9]+ (containerd|kubelet)' | awk '{print $1}' | xargs -r kill (code=exited, status=0/SUCCESS)
   Main PID: 12452 (code=exited, status=1/FAILURE)
        CPU: 298msnon-zero return code

2025-10-25 16:42:09,122 p=3389025 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
Oct 25 09:41:40 k8s-m1 systemd[1]: rke2-server.service: Failed with result 'exit-code'.
Oct 25 09:41:40 k8s-m1 systemd[1]: Failed to start rke2-server.service - Rancher Kubernetes Engine v2 (server).
Oct 25 09:41:45 k8s-m1 systemd[1]: rke2-server.service: Scheduled restart job, restart counter is at 94.
Oct 25 09:41:45 k8s-m1 systemd[1]: Starting rke2-server.service - Rancher Kubernetes Engine v2 (server)...
Oct 25 09:41:45 k8s-m1 sh[12366]: + /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service
Oct 25 09:41:46 k8s-m1 rke2[12372]: time="2025-10-25T09:41:46Z" level=warning msg="not running in CIS mode"
Oct 25 09:41:46 k8s-m1 rke2[12372]: time="2025-10-25T09:41:46Z" level=info msg="Applying Pod Security Admission Configuration"
Oct 25 09:41:46 k8s-m1 rke2[12372]: time="2025-10-25T09:41:46Z" level=fatal msg="Error: no default routes found in \"/proc/net/route\" or \"/proc/net/ipv6_route\""
Oct 25 09:41:46 k8s-m1 systemd[1]: rke2-server.service: Main process exited, code=exited, status=1/FAILURE
Oct 25 09:41:46 k8s-m1 systemd[1]: rke2-server.service: Failed with result 'exit-code'.
Oct 25 09:41:46 k8s-m1 systemd[1]: Failed to start rke2-server.service - Rancher Kubernetes Engine v2 (server).
Oct 25 09:41:51 k8s-m1 systemd[1]: rke2-server.service: Scheduled restart job, restart counter is at 95.
Oct 25 09:41:51 k8s-m1 systemd[1]: Starting rke2-server.service - Rancher Kubernetes Engine v2 (server)...
Oct 25 09:41:51 k8s-m1 sh[12391]: + /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service
Oct 25 09:41:51 k8s-m1 rke2[12398]: time="2025-10-25T09:41:51Z" level=warning msg="not running in CIS mode"
Oct 25 09:41:51 k8s-m1 rke2[12398]: time="2025-10-25T09:41:51Z" level=info msg="Applying Pod Security Admission Configuration"
Oct 25 09:41:51 k8s-m1 rke2[12398]: time="2025-10-25T09:41:51Z" level=fatal msg="Error: no default routes found in \"/proc/net/route\" or \"/proc/net/ipv6_route\""
Oct 25 09:41:51 k8s-m1 systemd[1]: rke2-server.service: Main process exited, code=exited, status=1/FAILURE
Oct 25 09:41:51 k8s-m1 systemd[1]: rke2-server.service: Failed with result 'exit-code'.
Oct 25 09:41:51 k8s-m1 systemd[1]: Failed to start rke2-server.service - Rancher Kubernetes Engine v2 (server).
Oct 25 09:41:54 k8s-m1 systemd[1]: Stopped rke2-server.service - Rancher Kubernetes Engine v2 (server).
Oct 25 09:41:54 k8s-m1 systemd[1]: Starting rke2-server.service - Rancher Kubernetes Engine v2 (server)...
Oct 25 09:41:54 k8s-m1 sh[12421]: + /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service
Oct 25 09:41:54 k8s-m1 rke2[12428]: time="2025-10-25T09:41:54Z" level=warning msg="not running in CIS mode"
Oct 25 09:41:54 k8s-m1 rke2[12428]: time="2025-10-25T09:41:54Z" level=info msg="Applying Pod Security Admission Configuration"
Oct 25 09:41:54 k8s-m1 rke2[12428]: time="2025-10-25T09:41:54Z" level=fatal msg="Error: no default routes found in \"/proc/net/route\" or \"/proc/net/ipv6_route\""
Oct 25 09:41:54 k8s-m1 systemd[1]: rke2-server.service: Main process exited, code=exited, status=1/FAILURE
Oct 25 09:41:54 k8s-m1 systemd[1]: rke2-server.service: Failed with result 'exit-code'.
Oct 25 09:41:54 k8s-m1 systemd[1]: Failed to start rke2-server.service - Rancher Kubernetes Engine v2 (server).
Oct 25 09:41:59 k8s-m1 systemd[1]: rke2-server.service: Scheduled restart job, restart counter is at 96.
Oct 25 09:41:59 k8s-m1 systemd[1]: Starting rke2-server.service - Rancher Kubernetes Engine v2 (server)...
Oct 25 09:41:59 k8s-m1 sh[12445]: + /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service
Oct 25 09:42:00 k8s-m1 rke2[12452]: time="2025-10-25T09:42:00Z" level=warning msg="not running in CIS mode"
Oct 25 09:42:00 k8s-m1 rke2[12452]: time="2025-10-25T09:42:00Z" level=info msg="Applying Pod Security Admission Configuration"
Oct 25 09:42:00 k8s-m1 rke2[12452]: time="2025-10-25T09:42:00Z" level=fatal msg="Error: no default routes found in \"/proc/net/route\" or \"/proc/net/ipv6_route\""
Oct 25 09:42:00 k8s-m1 systemd[1]: rke2-server.service: Main process exited, code=exited, status=1/FAILURE
Oct 25 09:42:00 k8s-m1 systemd[1]: rke2-server.service: Failed with result 'exit-code'.
Oct 25 09:42:00 k8s-m1 systemd[1]: Failed to start rke2-server.service - Rancher Kubernetes Engine v2 (server).
Oct 25 09:42:05 k8s-m1 systemd[1]: rke2-server.service: Scheduled restart job, restart counter is at 97.
Oct 25 09:42:05 k8s-m1 systemd[1]: Starting rke2-server.service - Rancher Kubernetes Engine v2 (server)...
Oct 25 09:42:05 k8s-m1 sh[12476]: + /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service
Oct 25 09:42:05 k8s-m1 rke2[12482]: time="2025-10-25T09:42:05Z" level=warning msg="not running in CIS mode"
Oct 25 09:42:05 k8s-m1 rke2[12482]: time="2025-10-25T09:42:05Z" level=info msg="Applying Pod Security Admission Configuration"
Oct 25 09:42:05 k8s-m1 rke2[12482]: time="2025-10-25T09:42:05Z" level=fatal msg="Error: no default routes found in \"/proc/net/route\" or \"/proc/net/ipv6_route\""
Oct 25 09:42:05 k8s-m1 systemd[1]: rke2-server.service: Main process exited, code=exited, status=1/FAILURE
Oct 25 09:42:05 k8s-m1 systemd[1]: rke2-server.service: Failed with result 'exit-code'.
Oct 25 09:42:05 k8s-m1 systemd[1]: Failed to start rke2-server.service - Rancher Kubernetes Engine v2 (server).
Oct 25 09:42:10 k8s-m1 systemd[1]: rke2-server.service: Scheduled restart job, restart counter is at 98.
Oct 25 09:42:10 k8s-m1 systemd[1]: Starting rke2-server.service - Rancher Kubernetes Engine v2 (server)...
Oct 25 09:42:10 k8s-m1 sh[12504]: + /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service

2025-10-25 16:42:44,918 p=3389689 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>
== IP ROUTE ==
10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.11 
== DEFAULT ROUTE (IPv4) ==
== IPv6 DEFAULT ==
== /proc/net/route ==
Iface	Destination	Gateway 	Flags	RefCnt	Use	Metric	Mask		MTU	Window	IRTT                                                       
ens34	0006000A	00000000	0001	0	0	0	00FFFFFF	0	0	0                                                                              
== NETPLAN FILES ==
total 4
-rw------- 1 root root 552 Oct 25 00:52 01-static.yaml
drwxr-xr-x 4 root root 174 Oct 25 00:52 backups
--- /etc/netplan/01-static.yaml ---
# Netplan static IPv4 configuration template
# Variables expected:
# - netplan_renderer: networkd | NetworkManager (default: networkd)
# - netplan_interface: e.g. ens160
# - netplan_address: e.g. 10.0.6.12
# - netplan_prefix: e.g. 24
# - netplan_gateway: e.g. 10.0.6.1 (optional)
# - netplan_nameservers: list, e.g. ["8.8.8.8", "1.1.1.1"] (optional)
# - netplan_search: list of search domains (optional)

network:
  version: 2
  renderer: networkd
  ethernets:
    ens34:
      dhcp4: false
      dhcp6: false
      addresses:
        - "10.0.6.11/24"
== DISK USAGE ==
Filesystem                         Size  Used Avail Use% Mounted on
tmpfs                              387M  1.6M  386M   1% /run
/dev/mapper/ubuntu--vg-ubuntu--lv   58G   13G   46G  22% /
tmpfs                              1.9G     0  1.9G   0% /dev/shm
tmpfs                              5.0M     0  5.0M   0% /run/lock
/dev/sda2                          2.0G  101M  1.7G   6% /boot
tmpfs                              387M   12K  387M   1% /run/user/1000
== MEMORY ==
               total        used        free      shared  buff/cache   available
Mem:           3.8Gi       535Mi       342Mi       1.5Mi       3.1Gi       3.3Gi
Swap:          3.8Gi        12Ki       3.8Gi
== TOP ==
top - 09:42:46 up  2:31,  4 users,  load average: 0.17, 0.26, 0.24
Tasks: 276 total,   2 running, 274 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.0 us,  2.1 sy,  0.0 ni, 97.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st 
MiB Mem :   3868.0 total,    341.7 free,    536.0 used,   3218.0 buff/cache     
MiB Swap:   3868.0 total,   3868.0 free,      0.0 used.   3332.1 avail Mem 

    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND
  12671 root      20   0   11900   5552   3504 R  16.7   0.1   0:00.04 top
      1 root      20   0   22548  13260   9036 R   0.0   0.3   0:16.52 systemd
      2 root      20   0       0      0      0 S   0.0   0.0   0:00.09 kthreadd
      3 root      20   0       0      0      0 S   0.0   0.0   0:00.00 pool_wo+
      4 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+
      5 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+
      6 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+
      7 root       0 -20       0      0      0 I   0.0   0.0   0:00.00 kworker+

2025-10-25 16:43:18,417 p=3390337 u=root n=ansible | k8s-m1 | FAILED | rc=1 >>
Object "route;" is unknown, try "ip help".non-zero return code

2025-10-25 16:43:55,681 p=3390882 u=root n=ansible | k8s-m1 | FAILED | rc=2 >>
/bin/sh: 1: set: Illegal option -o pipefailnon-zero return code

2025-10-25 16:44:23,524 p=3391573 u=root n=ansible | k8s-m1 | CHANGED | rc=0 >>


2025-10-25 16:44:43,608 p=3392166 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 16:44:45,466 p=3392166 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 16:44:45,467 p=3392166 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:44:46,115 p=3392166 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:44:46,124 p=3392166 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:44:46,178 p=3392166 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:44:47,618 p=3392166 u=root n=ansible | TASK [Gather facts] ************************************************************
2025-10-25 16:44:47,618 p=3392166 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:44:47,625 p=3392166 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:44:47,668 p=3392166 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:44:47,673 p=3392166 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:44:49,156 p=3392166 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] *********************
2025-10-25 16:44:49,157 p=3392166 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:44:49,170 p=3392166 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:44:49,199 p=3392166 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:44:49,276 p=3392166 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:44:49,772 p=3392166 u=root n=ansible | TASK [Detect default interface via ip route] ***********************************
2025-10-25 16:44:49,772 p=3392166 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:44:49,783 p=3392166 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:44:49,990 p=3392166 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:44:49,991 p=3392166 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:44:50,112 p=3392166 u=root n=ansible | TASK [Set default values for netplan variables] ********************************
2025-10-25 16:44:50,114 p=3392166 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:44:50,115 p=3392166 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:44:50,122 p=3392166 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:44:50,155 p=3392166 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:44:50,233 p=3392166 u=root n=ansible | TASK [Validate required variables] *********************************************
2025-10-25 16:44:50,234 p=3392166 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: All assertions passed
2025-10-25 16:44:50,260 p=3392166 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: All assertions passed
2025-10-25 16:44:50,261 p=3392166 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: All assertions passed
2025-10-25 16:44:50,281 p=3392166 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: All assertions passed
2025-10-25 16:44:50,686 p=3392166 u=root n=ansible | TASK [Move existing netplan YAMLs to timestamped backup directory] *************
2025-10-25 16:44:50,687 p=3392166 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:44:50,702 p=3392166 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:44:50,703 p=3392166 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:44:50,729 p=3392166 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:44:51,996 p=3392166 u=root n=ansible | TASK [Render static netplan config] ********************************************
2025-10-25 16:44:51,996 p=3392166 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:44:52,013 p=3392166 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:44:52,015 p=3392166 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:44:52,021 p=3392166 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:44:52,667 p=3392166 u=root n=ansible | TASK [Apply netplan] ***********************************************************
2025-10-25 16:44:52,671 p=3392166 u=root n=ansible | fatal: [k8s-w01]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.248040'
  end: '2025-10-25 09:44:54.524975'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 09:44:54.276935'
  stderr: |-
    /etc/netplan/01-static.yaml:15:5: Invalid YAML: did not find expected key:
        :
        ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 16:44:52,672 p=3392166 u=root n=ansible | fatal: [k8s-m1]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.282082'
  end: '2025-10-25 09:44:54.518384'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 09:44:54.236302'
  stderr: |-
    /etc/netplan/01-static.yaml:15:5: Invalid YAML: did not find expected key:
        :
        ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 16:44:52,703 p=3392166 u=root n=ansible | fatal: [k8s-w02]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.276944'
  end: '2025-10-25 09:44:54.570138'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 09:44:54.293194'
  stderr: |-
    /etc/netplan/01-static.yaml:15:5: Invalid YAML: did not find expected key:
        :
        ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 16:44:52,750 p=3392166 u=root n=ansible | fatal: [k8s-w03]: FAILED! => changed=true 
  cmd:
  - netplan
  - apply
  delta: '0:00:00.257672'
  end: '2025-10-25 09:44:54.601423'
  msg: non-zero return code
  rc: 78
  start: '2025-10-25 09:44:54.343751'
  stderr: |-
    /etc/netplan/01-static.yaml:15:5: Invalid YAML: did not find expected key:
        :
        ^
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 16:44:52,751 p=3392166 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 16:44:52,751 p=3392166 u=root n=ansible | k8s-m1                     : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 16:44:52,752 p=3392166 u=root n=ansible | k8s-w01                    : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 16:44:52,752 p=3392166 u=root n=ansible | k8s-w02                    : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 16:44:52,752 p=3392166 u=root n=ansible | k8s-w03                    : ok=8    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-10-25 16:47:22,329 p=3395622 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 16:47:24,045 p=3395622 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 16:47:24,045 p=3395622 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:47:24,053 p=3395622 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:47:24,063 p=3395622 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:47:24,083 p=3395622 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:47:25,647 p=3395622 u=root n=ansible | TASK [Gather facts] ************************************************************
2025-10-25 16:47:25,648 p=3395622 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:47:25,661 p=3395622 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:47:25,671 p=3395622 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:47:26,657 p=3395622 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:47:28,257 p=3395622 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] *********************
2025-10-25 16:47:28,258 p=3395622 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:47:28,309 p=3395622 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:47:28,309 p=3395622 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:47:28,342 p=3395622 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:47:28,835 p=3395622 u=root n=ansible | TASK [Detect default interface via ip route] ***********************************
2025-10-25 16:47:28,836 p=3395622 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:47:28,837 p=3395622 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:47:28,837 p=3395622 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:47:28,847 p=3395622 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:47:28,950 p=3395622 u=root n=ansible | TASK [Set default values for netplan variables] ********************************
2025-10-25 16:47:28,951 p=3395622 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:47:28,951 p=3395622 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:47:28,958 p=3395622 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:47:28,992 p=3395622 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:47:29,073 p=3395622 u=root n=ansible | TASK [Validate required variables] *********************************************
2025-10-25 16:47:29,075 p=3395622 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: All assertions passed
2025-10-25 16:47:29,101 p=3395622 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: All assertions passed
2025-10-25 16:47:29,104 p=3395622 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: All assertions passed
2025-10-25 16:47:29,128 p=3395622 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: All assertions passed
2025-10-25 16:47:29,496 p=3395622 u=root n=ansible | TASK [Move existing netplan YAMLs to timestamped backup directory] *************
2025-10-25 16:47:29,497 p=3395622 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:47:29,545 p=3395622 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:47:29,546 p=3395622 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:47:29,573 p=3395622 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:47:30,727 p=3395622 u=root n=ansible | TASK [Render static netplan config] ********************************************
2025-10-25 16:47:30,728 p=3395622 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:47:30,731 p=3395622 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:47:30,737 p=3395622 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:47:30,748 p=3395622 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:47:32,132 p=3395622 u=root n=ansible | TASK [Apply netplan] ***********************************************************
2025-10-25 16:47:32,133 p=3395622 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:47:32,151 p=3395622 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:47:32,163 p=3395622 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:47:32,275 p=3395622 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:47:32,657 p=3395622 u=root n=ansible | TASK [Verify network configuration] ********************************************
2025-10-25 16:47:32,657 p=3395622 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:47:32,701 p=3395622 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:47:32,703 p=3395622 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:47:32,751 p=3395622 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:47:32,823 p=3395622 u=root n=ansible | TASK [Show verification output] ************************************************
2025-10-25 16:47:32,825 p=3395622 u=root n=ansible | ok: [k8s-m1] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.11/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.11
2025-10-25 16:47:32,851 p=3395622 u=root n=ansible | ok: [k8s-w01] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.14/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.14
2025-10-25 16:47:32,853 p=3395622 u=root n=ansible | ok: [k8s-w02] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.15/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.15
2025-10-25 16:47:32,866 p=3395622 u=root n=ansible | ok: [k8s-w03] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.16/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.16
2025-10-25 16:47:33,124 p=3395622 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 16:47:33,124 p=3395622 u=root n=ansible | k8s-m1                     : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 16:47:33,124 p=3395622 u=root n=ansible | k8s-w01                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 16:47:33,125 p=3395622 u=root n=ansible | k8s-w02                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 16:47:33,125 p=3395622 u=root n=ansible | k8s-w03                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 16:47:44,352 p=3396428 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ****
2025-10-25 16:47:45,974 p=3396428 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 16:47:45,975 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:47:46,007 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:47:46,016 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:47:46,022 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:47:46,251 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 16:47:46,252 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:47:46,275 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:47:46,301 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:47:46,320 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:47:47,479 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] **********************
2025-10-25 16:47:47,479 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:47:47,488 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:47:47,492 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:47:47,497 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:47:47,693 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] ******************
2025-10-25 16:47:48,016 p=3396428 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-w01, k8s-w02, k8s-w03
2025-10-25 16:47:49,341 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Configure kernel modules] **********************************
2025-10-25 16:47:49,342 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:47:49,541 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:47:49,567 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:47:49,569 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:47:49,968 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Detect NetworkManager] *************************************
2025-10-25 16:47:49,970 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:47:49,971 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:47:49,975 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:47:49,984 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:47:50,975 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Configure sysctl parameters] *******************************
2025-10-25 16:47:50,975 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:47:50,977 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:47:50,997 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:47:51,029 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:47:51,567 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Apply system configurations] *******************************
2025-10-25 16:47:51,568 p=3396428 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:47:51,581 p=3396428 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:47:51,595 p=3396428 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:47:51,617 p=3396428 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:47:54,291 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Ensure UFW is installed] ***********************************
2025-10-25 16:47:54,291 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:47:54,295 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:47:54,327 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:47:54,350 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:47:55,680 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Enable UFW] ************************************************
2025-10-25 16:47:55,681 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:47:55,711 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:47:55,718 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:47:55,739 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:47:58,360 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Set default incoming policy to deny] ***********************
2025-10-25 16:47:58,360 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:47:58,396 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:47:58,470 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:47:58,517 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:48:01,220 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Set default outgoing policy to allow] **********************
2025-10-25 16:48:01,220 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:48:01,251 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:48:01,252 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:48:01,253 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:48:02,138 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Allow required ports] **************************************
2025-10-25 16:48:02,138 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=22/tcp)
2025-10-25 16:48:02,145 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=22/tcp)
2025-10-25 16:48:02,200 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=22/tcp)
2025-10-25 16:48:02,230 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=22/tcp)
2025-10-25 16:48:02,952 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=179/tcp)
2025-10-25 16:48:02,975 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=179/tcp)
2025-10-25 16:48:03,062 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=179/tcp)
2025-10-25 16:48:03,074 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=179/tcp)
2025-10-25 16:48:03,771 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=9345/tcp)
2025-10-25 16:48:03,780 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=9345/tcp)
2025-10-25 16:48:03,922 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=9345/tcp)
2025-10-25 16:48:03,928 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=9345/tcp)
2025-10-25 16:48:04,659 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=6443/tcp)
2025-10-25 16:48:04,678 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=6443/tcp)
2025-10-25 16:48:04,765 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=6443/tcp)
2025-10-25 16:48:04,796 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=6443/tcp)
2025-10-25 16:48:05,522 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=10250/tcp)
2025-10-25 16:48:05,526 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=10250/tcp)
2025-10-25 16:48:05,597 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=10250/tcp)
2025-10-25 16:48:05,634 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=10250/tcp)
2025-10-25 16:48:06,388 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=2379/tcp)
2025-10-25 16:48:06,458 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=2379/tcp)
2025-10-25 16:48:06,459 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=2379/tcp)
2025-10-25 16:48:06,507 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=2379/tcp)
2025-10-25 16:48:07,253 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=2380/tcp)
2025-10-25 16:48:07,291 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=2380/tcp)
2025-10-25 16:48:07,303 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=2380/tcp)
2025-10-25 16:48:07,392 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=2380/tcp)
2025-10-25 16:48:08,096 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=2381/tcp)
2025-10-25 16:48:08,148 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=2381/tcp)
2025-10-25 16:48:08,165 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=2381/tcp)
2025-10-25 16:48:08,243 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=2381/tcp)
2025-10-25 16:48:08,986 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=30000:32767/tcp)
2025-10-25 16:48:09,013 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=30000:32767/tcp)
2025-10-25 16:48:09,019 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=30000:32767/tcp)
2025-10-25 16:48:09,119 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=30000:32767/tcp)
2025-10-25 16:48:09,838 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=8472/udp)
2025-10-25 16:48:09,845 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=8472/udp)
2025-10-25 16:48:09,872 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=8472/udp)
2025-10-25 16:48:09,991 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=8472/udp)
2025-10-25 16:48:10,693 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=6081/udp)
2025-10-25 16:48:10,725 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=6081/udp)
2025-10-25 16:48:10,750 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=6081/udp)
2025-10-25 16:48:10,828 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=6081/udp)
2025-10-25 16:48:11,564 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=4240/tcp)
2025-10-25 16:48:11,577 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=4240/tcp)
2025-10-25 16:48:11,636 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=4240/tcp)
2025-10-25 16:48:11,678 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=4240/tcp)
2025-10-25 16:48:12,416 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=4244/tcp)
2025-10-25 16:48:12,418 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=4244/tcp)
2025-10-25 16:48:12,481 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=4244/tcp)
2025-10-25 16:48:12,515 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=4244/tcp)
2025-10-25 16:48:13,289 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=4245/tcp)
2025-10-25 16:48:13,307 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=4245/tcp)
2025-10-25 16:48:13,326 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=4245/tcp)
2025-10-25 16:48:13,379 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=4245/tcp)
2025-10-25 16:48:14,103 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=4222/tcp)
2025-10-25 16:48:14,142 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=4222/tcp)
2025-10-25 16:48:14,153 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=4222/tcp)
2025-10-25 16:48:14,195 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=4222/tcp)
2025-10-25 16:48:14,965 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=9966/tcp)
2025-10-25 16:48:14,984 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=9966/tcp)
2025-10-25 16:48:15,025 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=9966/tcp)
2025-10-25 16:48:15,026 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=9966/tcp)
2025-10-25 16:48:15,822 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=4250:4251/tcp)
2025-10-25 16:48:15,837 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=4250:4251/tcp)
2025-10-25 16:48:15,878 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=4250:4251/tcp)
2025-10-25 16:48:15,932 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=4250:4251/tcp)
2025-10-25 16:48:16,748 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=6060:6062/tcp)
2025-10-25 16:48:16,787 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=6060:6062/tcp)
2025-10-25 16:48:16,818 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=6060:6062/tcp)
2025-10-25 16:48:16,844 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=6060:6062/tcp)
2025-10-25 16:48:17,653 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=9878:9879/tcp)
2025-10-25 16:48:17,677 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=9878:9879/tcp)
2025-10-25 16:48:17,739 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=9878:9879/tcp)
2025-10-25 16:48:17,750 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=9878:9879/tcp)
2025-10-25 16:48:18,534 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=9890:9893/tcp)
2025-10-25 16:48:18,535 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=9890:9893/tcp)
2025-10-25 16:48:18,584 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=9890:9893/tcp)
2025-10-25 16:48:18,586 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=9890:9893/tcp)
2025-10-25 16:48:19,402 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=9901/tcp)
2025-10-25 16:48:19,409 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=9901/tcp)
2025-10-25 16:48:19,428 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=9901/tcp)
2025-10-25 16:48:19,429 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=9901/tcp)
2025-10-25 16:48:20,241 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=9962:9964/tcp)
2025-10-25 16:48:20,255 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=9962:9964/tcp)
2025-10-25 16:48:20,281 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=9962:9964/tcp)
2025-10-25 16:48:20,290 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=9962:9964/tcp)
2025-10-25 16:48:21,111 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=51871/udp)
2025-10-25 16:48:21,112 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=51871/udp)
2025-10-25 16:48:21,182 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=51871/udp)
2025-10-25 16:48:21,187 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=51871/udp)
2025-10-25 16:48:21,988 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=4567/tcp)
2025-10-25 16:48:22,007 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=4567/tcp)
2025-10-25 16:48:22,024 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=4567/tcp)
2025-10-25 16:48:22,039 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=4567/tcp)
2025-10-25 16:48:22,842 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=4567/udp)
2025-10-25 16:48:22,872 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=4567/udp)
2025-10-25 16:48:22,873 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=4567/udp)
2025-10-25 16:48:22,874 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=4567/udp)
2025-10-25 16:48:23,817 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=4568/tcp)
2025-10-25 16:48:23,828 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=4568/tcp)
2025-10-25 16:48:23,848 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=4568/tcp)
2025-10-25 16:48:23,873 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=4568/tcp)
2025-10-25 16:48:24,724 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=4444/tcp)
2025-10-25 16:48:24,736 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=4444/tcp)
2025-10-25 16:48:24,737 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=4444/tcp)
2025-10-25 16:48:24,811 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=4444/tcp)
2025-10-25 16:48:25,550 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=7443/tcp)
2025-10-25 16:48:25,569 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=7443/tcp)
2025-10-25 16:48:25,582 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=7443/tcp)
2025-10-25 16:48:25,745 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=7443/tcp)
2025-10-25 16:48:26,462 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=7345/tcp)
2025-10-25 16:48:26,463 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=7345/tcp)
2025-10-25 16:48:26,466 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=7345/tcp)
2025-10-25 16:48:26,642 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=7345/tcp)
2025-10-25 16:48:27,380 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=8080/tcp)
2025-10-25 16:48:27,398 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=8080/tcp)
2025-10-25 16:48:27,398 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=8080/tcp)
2025-10-25 16:48:27,564 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=8080/tcp)
2025-10-25 16:48:28,242 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=8443/tcp)
2025-10-25 16:48:28,255 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=8443/tcp)
2025-10-25 16:48:28,277 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=8443/tcp)
2025-10-25 16:48:28,466 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=8443/tcp)
2025-10-25 16:48:29,098 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=3306/tcp)
2025-10-25 16:48:29,101 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=3306/tcp)
2025-10-25 16:48:29,129 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=3306/tcp)
2025-10-25 16:48:29,315 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=3306/tcp)
2025-10-25 16:48:29,968 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=3307/tcp)
2025-10-25 16:48:29,997 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=3307/tcp)
2025-10-25 16:48:30,209 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=3307/tcp)
2025-10-25 16:48:30,899 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=80/tcp)
2025-10-25 16:48:30,936 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=80/tcp)
2025-10-25 16:48:30,960 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=3307/tcp)
2025-10-25 16:48:31,137 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=80/tcp)
2025-10-25 16:48:31,796 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=443/tcp)
2025-10-25 16:48:31,815 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=443/tcp)
2025-10-25 16:48:31,844 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=80/tcp)
2025-10-25 16:48:32,017 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=443/tcp)
2025-10-25 16:48:32,783 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=443/tcp)
2025-10-25 16:48:33,433 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Create RKE2 artifacts folder] ******************************
2025-10-25 16:48:33,434 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:48:33,449 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:48:33,496 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:48:33,560 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:48:34,362 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Download sha256 checksum file ( airgap mode )] *************
2025-10-25 16:48:34,362 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:48:34,365 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:48:34,367 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:48:34,375 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:48:35,140 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 artifacts and compare with checksums ( airgap mode )] ***
2025-10-25 16:48:35,141 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 16:48:35,152 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 16:48:35,181 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 16:48:35,186 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 16:48:39,159 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 16:48:39,194 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 16:48:39,260 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 16:48:39,783 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 16:48:41,682 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 16:48:41,742 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 16:48:41,841 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 16:48:42,303 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 16:48:44,271 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 16:48:44,285 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 16:48:44,422 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 16:48:44,845 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 16:48:45,403 p=3396428 u=root n=ansible | ok: [k8s-w03] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 16:48:45,413 p=3396428 u=root n=ansible | ok: [k8s-w02] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 16:48:45,505 p=3396428 u=root n=ansible | ok: [k8s-w01] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 16:48:46,211 p=3396428 u=root n=ansible | ok: [k8s-m1] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 16:48:46,819 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 install script ( airgap mode )] **************
2025-10-25 16:48:46,820 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:48:46,832 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:48:46,846 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:48:46,899 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:48:53,523 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Populate service facts] ************************************
2025-10-25 16:48:53,524 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:48:53,531 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:48:53,537 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:48:53,547 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:48:53,926 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Get stats of the FS object] ********************************
2025-10-25 16:48:53,927 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:48:53,961 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:48:53,964 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:48:53,964 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:48:54,337 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Check if separate partition] *******************************
2025-10-25 16:48:54,337 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:48:54,338 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:48:54,375 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:48:54,392 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:48:54,456 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 bin path] *****************************************
2025-10-25 16:48:54,457 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:48:54,477 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:48:54,479 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:48:54,496 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:48:55,174 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Check RKE2 version] ****************************************
2025-10-25 16:48:55,175 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:48:55,175 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:48:55,191 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:48:55,197 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:48:55,283 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 versions] *****************************************
2025-10-25 16:48:55,284 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:48:55,287 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:48:55,296 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:48:55,322 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:48:55,461 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Prevent accidental RKE2 downgrade] *************************
2025-10-25 16:48:55,463 p=3396428 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: |-
    Version check passed. Current running version: ['1.34.1'], desired version: ['1.34.1']."
2025-10-25 16:48:55,476 p=3396428 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: |-
    Version check passed. Current running version: ['1.34.1'], desired version: ['1.34.1']."
2025-10-25 16:48:55,504 p=3396428 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: |-
    Version check passed. Current running version: ['1.34.1'], desired version: ['1.34.1']."
2025-10-25 16:48:55,518 p=3396428 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: |-
    Version check passed. Current running version: ['1.34.1'], desired version: ['1.34.1']."
2025-10-25 16:48:56,176 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Find Active Server] ****************************************
2025-10-25 16:48:56,277 p=3396428 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/find_active_server.yml for k8s-m1, k8s-w01, k8s-w02, k8s-w03
2025-10-25 16:49:01,441 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Populate services facts] ***********************************
2025-10-25 16:49:01,442 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:49:01,504 p=3396428 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:49:01,580 p=3396428 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:49:01,612 p=3396428 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:49:02,201 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Prepare very first server node in the cluster] *************
2025-10-25 16:49:02,243 p=3396428 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/first_server.yml for k8s-m1
2025-10-25 16:49:02,603 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Create the RKE2 config dir] ********************************
2025-10-25 16:49:02,604 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:49:02,656 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Set server taints] *****************************************
2025-10-25 16:49:02,656 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:49:03,637 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Copy rke2 config] ******************************************
2025-10-25 16:49:03,637 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:49:04,688 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Copy Containerd Registry Configuration file] ***************
2025-10-25 16:49:04,689 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:49:06,306 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Start RKE2 service on the first server] ********************
2025-10-25 16:49:06,306 p=3396428 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:49:07,478 p=3396428 u=root n=ansible | TASK [rke2-1.49.0 : Mask RKE2 agent service on the first server] ***************
2025-10-25 16:49:07,479 p=3396428 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:52:57,579 p=3396428 u=root n=ansible |  [ERROR]: User interrupted execution

2025-10-25 16:53:05,508 p=3406427 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 16:53:07,340 p=3406427 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 16:53:07,341 p=3406427 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:53:08,144 p=3406427 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:53:09,129 p=3406427 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:53:09,223 p=3406427 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:53:10,769 p=3406427 u=root n=ansible | TASK [Gather facts] ************************************************************
2025-10-25 16:53:10,770 p=3406427 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:53:10,787 p=3406427 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:53:10,805 p=3406427 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:53:10,871 p=3406427 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:53:12,443 p=3406427 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] *********************
2025-10-25 16:53:12,443 p=3406427 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:53:12,469 p=3406427 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:53:12,625 p=3406427 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:53:12,787 p=3406427 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:53:13,310 p=3406427 u=root n=ansible | TASK [Detect default interface via ip route] ***********************************
2025-10-25 16:53:13,310 p=3406427 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:53:13,322 p=3406427 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:53:13,344 p=3406427 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:53:13,353 p=3406427 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:53:13,443 p=3406427 u=root n=ansible | TASK [Set default values for netplan variables] ********************************
2025-10-25 16:53:13,444 p=3406427 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:53:13,473 p=3406427 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:53:13,499 p=3406427 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:53:13,528 p=3406427 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:53:13,607 p=3406427 u=root n=ansible | TASK [Validate required variables] *********************************************
2025-10-25 16:53:13,609 p=3406427 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: All assertions passed
2025-10-25 16:53:13,636 p=3406427 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: All assertions passed
2025-10-25 16:53:13,638 p=3406427 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: All assertions passed
2025-10-25 16:53:13,671 p=3406427 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: All assertions passed
2025-10-25 16:53:14,092 p=3406427 u=root n=ansible | TASK [Move existing netplan YAMLs to timestamped backup directory] *************
2025-10-25 16:53:14,093 p=3406427 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:53:14,111 p=3406427 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:53:14,123 p=3406427 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:53:14,143 p=3406427 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:53:15,300 p=3406427 u=root n=ansible | TASK [Render static netplan config] ********************************************
2025-10-25 16:53:15,300 p=3406427 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:53:15,303 p=3406427 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:53:15,306 p=3406427 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:53:15,324 p=3406427 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:53:16,648 p=3406427 u=root n=ansible | TASK [Apply netplan] ***********************************************************
2025-10-25 16:53:16,648 p=3406427 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:53:16,656 p=3406427 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:53:16,896 p=3406427 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:53:16,965 p=3406427 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:53:17,371 p=3406427 u=root n=ansible | TASK [Verify network configuration] ********************************************
2025-10-25 16:53:17,371 p=3406427 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:53:17,397 p=3406427 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:53:17,419 p=3406427 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:53:17,430 p=3406427 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:53:17,478 p=3406427 u=root n=ansible | TASK [Show verification output] ************************************************
2025-10-25 16:53:17,480 p=3406427 u=root n=ansible | ok: [k8s-m1] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.11/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.11
2025-10-25 16:53:17,503 p=3406427 u=root n=ansible | ok: [k8s-w01] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.14/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.14
2025-10-25 16:53:17,524 p=3406427 u=root n=ansible | ok: [k8s-w02] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.15/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.15
2025-10-25 16:53:17,541 p=3406427 u=root n=ansible | ok: [k8s-w03] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.16/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.16
2025-10-25 16:53:17,844 p=3406427 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 16:53:17,844 p=3406427 u=root n=ansible | k8s-m1                     : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 16:53:17,844 p=3406427 u=root n=ansible | k8s-w01                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 16:53:17,845 p=3406427 u=root n=ansible | k8s-w02                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 16:53:17,845 p=3406427 u=root n=ansible | k8s-w03                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 16:55:59,616 p=3410655 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ****
2025-10-25 16:56:01,388 p=3410655 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 16:56:01,389 p=3410655 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:56:01,467 p=3410655 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:56:01,473 p=3410655 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:56:01,484 p=3410655 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:56:01,738 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 16:56:01,739 p=3410655 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:56:01,765 p=3410655 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:56:01,785 p=3410655 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:56:01,812 p=3410655 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:56:02,908 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] **********************
2025-10-25 16:56:02,908 p=3410655 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:56:02,951 p=3410655 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:56:02,955 p=3410655 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:56:02,992 p=3410655 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:56:03,156 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] ******************
2025-10-25 16:56:03,328 p=3410655 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-w01, k8s-w02, k8s-w03
2025-10-25 16:56:04,459 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Configure kernel modules] **********************************
2025-10-25 16:56:04,460 p=3410655 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:56:04,480 p=3410655 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:56:04,484 p=3410655 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:56:04,506 p=3410655 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:56:04,852 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Detect NetworkManager] *************************************
2025-10-25 16:56:04,852 p=3410655 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:56:04,938 p=3410655 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:56:04,943 p=3410655 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:56:04,959 p=3410655 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:56:06,035 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Configure sysctl parameters] *******************************
2025-10-25 16:56:06,036 p=3410655 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:56:06,040 p=3410655 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:56:06,040 p=3410655 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:56:06,041 p=3410655 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:56:06,739 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Apply system configurations] *******************************
2025-10-25 16:56:06,740 p=3410655 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:56:06,753 p=3410655 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:56:06,766 p=3410655 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:56:06,819 p=3410655 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:56:09,409 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Ensure UFW is installed] ***********************************
2025-10-25 16:56:09,409 p=3410655 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:56:09,457 p=3410655 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:56:09,466 p=3410655 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:56:09,592 p=3410655 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:56:11,102 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Enable UFW] ************************************************
2025-10-25 16:56:11,103 p=3410655 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:56:11,127 p=3410655 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:56:11,133 p=3410655 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:56:11,187 p=3410655 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:56:13,877 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Set default incoming policy to deny] ***********************
2025-10-25 16:56:13,878 p=3410655 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:56:13,880 p=3410655 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:56:13,885 p=3410655 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:56:13,929 p=3410655 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:56:16,634 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Set default outgoing policy to allow] **********************
2025-10-25 16:56:16,635 p=3410655 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:56:17,053 p=3410655 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:56:17,199 p=3410655 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:56:17,420 p=3410655 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:56:18,453 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Allow required ports] **************************************
2025-10-25 16:56:18,454 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=22/tcp)
2025-10-25 16:56:18,478 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=22/tcp)
2025-10-25 16:56:18,481 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=22/tcp)
2025-10-25 16:56:18,484 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=22/tcp)
2025-10-25 16:56:19,371 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=179/tcp)
2025-10-25 16:56:19,382 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=179/tcp)
2025-10-25 16:56:19,385 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=179/tcp)
2025-10-25 16:56:19,435 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=179/tcp)
2025-10-25 16:56:20,269 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=9345/tcp)
2025-10-25 16:56:20,270 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=9345/tcp)
2025-10-25 16:56:20,286 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=9345/tcp)
2025-10-25 16:56:20,367 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=9345/tcp)
2025-10-25 16:56:21,186 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=6443/tcp)
2025-10-25 16:56:21,201 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=6443/tcp)
2025-10-25 16:56:21,220 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=6443/tcp)
2025-10-25 16:56:21,296 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=6443/tcp)
2025-10-25 16:56:22,124 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=10250/tcp)
2025-10-25 16:56:22,137 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=10250/tcp)
2025-10-25 16:56:22,138 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=10250/tcp)
2025-10-25 16:56:22,203 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=10250/tcp)
2025-10-25 16:56:23,024 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=2379/tcp)
2025-10-25 16:56:23,038 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=2379/tcp)
2025-10-25 16:56:23,109 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=2379/tcp)
2025-10-25 16:56:23,118 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=2379/tcp)
2025-10-25 16:56:23,988 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=2380/tcp)
2025-10-25 16:56:24,019 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=2380/tcp)
2025-10-25 16:56:24,025 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=2380/tcp)
2025-10-25 16:56:24,053 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=2380/tcp)
2025-10-25 16:56:24,949 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=2381/tcp)
2025-10-25 16:56:24,970 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=2381/tcp)
2025-10-25 16:56:24,974 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=2381/tcp)
2025-10-25 16:56:24,980 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=2381/tcp)
2025-10-25 16:56:25,863 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=30000:32767/tcp)
2025-10-25 16:56:25,889 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=30000:32767/tcp)
2025-10-25 16:56:25,912 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=30000:32767/tcp)
2025-10-25 16:56:25,917 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=30000:32767/tcp)
2025-10-25 16:56:26,776 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=8472/udp)
2025-10-25 16:56:26,791 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=8472/udp)
2025-10-25 16:56:26,842 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=8472/udp)
2025-10-25 16:56:26,843 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=8472/udp)
2025-10-25 16:56:27,715 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=6081/udp)
2025-10-25 16:56:27,728 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=6081/udp)
2025-10-25 16:56:27,750 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=6081/udp)
2025-10-25 16:56:27,755 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=6081/udp)
2025-10-25 16:56:28,625 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=4240/tcp)
2025-10-25 16:56:28,648 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=4240/tcp)
2025-10-25 16:56:28,684 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=4240/tcp)
2025-10-25 16:56:28,696 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=4240/tcp)
2025-10-25 16:56:29,558 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=4244/tcp)
2025-10-25 16:56:29,571 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=4244/tcp)
2025-10-25 16:56:29,629 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=4244/tcp)
2025-10-25 16:56:29,665 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=4244/tcp)
2025-10-25 16:56:30,519 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=4245/tcp)
2025-10-25 16:56:30,643 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=4245/tcp)
2025-10-25 16:56:30,666 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=4245/tcp)
2025-10-25 16:56:31,534 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=4222/tcp)
2025-10-25 16:56:31,563 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=4245/tcp)
2025-10-25 16:56:31,631 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=4222/tcp)
2025-10-25 16:56:31,635 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=4222/tcp)
2025-10-25 16:56:32,454 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=9966/tcp)
2025-10-25 16:56:32,465 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=4222/tcp)
2025-10-25 16:56:32,533 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=9966/tcp)
2025-10-25 16:56:32,567 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=9966/tcp)
2025-10-25 16:56:33,377 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=4250:4251/tcp)
2025-10-25 16:56:33,382 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=9966/tcp)
2025-10-25 16:56:33,440 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=4250:4251/tcp)
2025-10-25 16:56:33,497 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=4250:4251/tcp)
2025-10-25 16:56:34,312 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=6060:6062/tcp)
2025-10-25 16:56:34,325 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=4250:4251/tcp)
2025-10-25 16:56:34,395 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=6060:6062/tcp)
2025-10-25 16:56:34,427 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=6060:6062/tcp)
2025-10-25 16:56:35,224 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=9878:9879/tcp)
2025-10-25 16:56:35,227 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=6060:6062/tcp)
2025-10-25 16:56:35,286 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=9878:9879/tcp)
2025-10-25 16:56:35,335 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=9878:9879/tcp)
2025-10-25 16:56:36,141 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=9890:9893/tcp)
2025-10-25 16:56:36,168 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=9878:9879/tcp)
2025-10-25 16:56:36,215 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=9890:9893/tcp)
2025-10-25 16:56:36,263 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=9890:9893/tcp)
2025-10-25 16:56:37,150 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=9901/tcp)
2025-10-25 16:56:37,160 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=9890:9893/tcp)
2025-10-25 16:56:37,337 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=9901/tcp)
2025-10-25 16:56:37,411 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=9901/tcp)
2025-10-25 16:56:38,110 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=9962:9964/tcp)
2025-10-25 16:56:38,111 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=9901/tcp)
2025-10-25 16:56:38,295 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=9962:9964/tcp)
2025-10-25 16:56:38,311 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=9962:9964/tcp)
2025-10-25 16:56:39,116 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=51871/udp)
2025-10-25 16:56:39,226 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=51871/udp)
2025-10-25 16:56:39,239 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=51871/udp)
2025-10-25 16:56:40,084 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=4567/tcp)
2025-10-25 16:56:40,089 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=9962:9964/tcp)
2025-10-25 16:56:40,212 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=4567/tcp)
2025-10-25 16:56:40,245 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=4567/tcp)
2025-10-25 16:56:41,023 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=4567/udp)
2025-10-25 16:56:41,169 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=51871/udp)
2025-10-25 16:56:41,230 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=4567/udp)
2025-10-25 16:56:41,343 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=4567/udp)
2025-10-25 16:56:41,986 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=4568/tcp)
2025-10-25 16:56:42,222 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=4568/tcp)
2025-10-25 16:56:42,330 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=4568/tcp)
2025-10-25 16:56:42,969 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=4444/tcp)
2025-10-25 16:56:43,099 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=4567/tcp)
2025-10-25 16:56:43,226 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=4444/tcp)
2025-10-25 16:56:43,326 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=4444/tcp)
2025-10-25 16:56:43,904 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=7443/tcp)
2025-10-25 16:56:44,039 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=4567/udp)
2025-10-25 16:56:44,154 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=7443/tcp)
2025-10-25 16:56:44,244 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=7443/tcp)
2025-10-25 16:56:44,893 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=7345/tcp)
2025-10-25 16:56:45,040 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=4568/tcp)
2025-10-25 16:56:45,128 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=7345/tcp)
2025-10-25 16:56:45,174 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=7345/tcp)
2025-10-25 16:56:45,846 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=8080/tcp)
2025-10-25 16:56:46,026 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=8080/tcp)
2025-10-25 16:56:46,028 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=4444/tcp)
2025-10-25 16:56:46,147 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=8080/tcp)
2025-10-25 16:56:46,854 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=8443/tcp)
2025-10-25 16:56:47,173 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=7443/tcp)
2025-10-25 16:56:47,174 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=8443/tcp)
2025-10-25 16:56:47,175 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=8443/tcp)
2025-10-25 16:56:47,824 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=3306/tcp)
2025-10-25 16:56:48,123 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=7345/tcp)
2025-10-25 16:56:48,204 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=3306/tcp)
2025-10-25 16:56:48,241 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=3306/tcp)
2025-10-25 16:56:48,772 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=3307/tcp)
2025-10-25 16:56:49,114 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=8080/tcp)
2025-10-25 16:56:49,176 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=3307/tcp)
2025-10-25 16:56:49,246 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=3307/tcp)
2025-10-25 16:56:49,754 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=80/tcp)
2025-10-25 16:56:50,076 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=8443/tcp)
2025-10-25 16:56:50,119 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=80/tcp)
2025-10-25 16:56:50,196 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=80/tcp)
2025-10-25 16:56:50,652 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=443/tcp)
2025-10-25 16:56:51,027 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=3306/tcp)
2025-10-25 16:56:51,054 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=443/tcp)
2025-10-25 16:56:51,170 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=443/tcp)
2025-10-25 16:56:52,043 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=3307/tcp)
2025-10-25 16:56:53,088 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=80/tcp)
2025-10-25 16:56:54,112 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=443/tcp)
2025-10-25 16:56:54,979 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Create RKE2 artifacts folder] ******************************
2025-10-25 16:56:54,980 p=3410655 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:56:54,988 p=3410655 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:56:54,989 p=3410655 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:56:55,009 p=3410655 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:56:55,756 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Download sha256 checksum file ( airgap mode )] *************
2025-10-25 16:56:55,757 p=3410655 u=root n=ansible | [WARNING]: Module remote_tmp /root/.ansible/tmp did not exist and was created
with a mode of 0700, this may cause issues when running as another user. To
avoid this, create the remote_tmp dir with the correct permissions manually

2025-10-25 16:56:55,757 p=3410655 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:56:55,766 p=3410655 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:56:55,778 p=3410655 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:56:55,782 p=3410655 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:56:57,020 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 artifacts and compare with checksums ( airgap mode )] ***
2025-10-25 16:56:57,020 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 16:56:57,251 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 16:56:57,355 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 16:56:57,386 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 16:57:17,647 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 16:57:18,582 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 16:57:18,888 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 16:57:19,244 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 16:57:28,801 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 16:57:31,087 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 16:57:31,174 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 16:57:31,597 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 16:57:39,987 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 16:57:41,786 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 16:57:43,363 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 16:57:43,755 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 16:57:43,945 p=3410655 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 16:57:45,179 p=3410655 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 16:57:47,024 p=3410655 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 16:57:47,312 p=3410655 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 16:57:47,939 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 install script ( airgap mode )] **************
2025-10-25 16:57:47,940 p=3410655 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:57:47,941 p=3410655 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:57:47,960 p=3410655 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:57:48,009 p=3410655 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:57:54,702 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Populate service facts] ************************************
2025-10-25 16:57:54,703 p=3410655 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:57:54,715 p=3410655 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:57:54,724 p=3410655 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:57:54,833 p=3410655 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:57:55,218 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Get stats of the FS object] ********************************
2025-10-25 16:57:55,219 p=3410655 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:57:55,220 p=3410655 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:57:55,238 p=3410655 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:57:55,242 p=3410655 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:57:55,610 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Check if separate partition] *******************************
2025-10-25 16:57:55,611 p=3410655 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:57:55,651 p=3410655 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:57:55,674 p=3410655 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:57:56,691 p=3410655 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:57:56,765 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 bin path] *****************************************
2025-10-25 16:57:56,765 p=3410655 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:57:56,791 p=3410655 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:57:56,792 p=3410655 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:57:56,814 p=3410655 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:57:57,227 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Check RKE2 version] ****************************************
2025-10-25 16:57:57,227 p=3410655 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:57:57,278 p=3410655 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:57:57,320 p=3410655 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:57:57,347 p=3410655 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:57:57,422 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 versions] *****************************************
2025-10-25 16:57:57,423 p=3410655 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:57:57,453 p=3410655 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:57:57,475 p=3410655 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:57:57,499 p=3410655 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:58:47,364 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Run RKE2 install script with airgap variables] *************
2025-10-25 16:58:47,366 p=3410655 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:59:09,453 p=3410655 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:59:10,184 p=3410655 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:59:21,512 p=3410655 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:59:22,110 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Find Active Server] ****************************************
2025-10-25 16:59:22,214 p=3410655 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/find_active_server.yml for k8s-m1, k8s-w01, k8s-w02, k8s-w03
2025-10-25 16:59:29,332 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Populate services facts] ***********************************
2025-10-25 16:59:29,333 p=3410655 u=root n=ansible | ok: [k8s-w02]
2025-10-25 16:59:29,531 p=3410655 u=root n=ansible | ok: [k8s-w01]
2025-10-25 16:59:30,340 p=3410655 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:59:30,506 p=3410655 u=root n=ansible | ok: [k8s-w03]
2025-10-25 16:59:31,378 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Prepare very first server node in the cluster] *************
2025-10-25 16:59:31,427 p=3410655 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/first_server.yml for k8s-m1
2025-10-25 16:59:31,987 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Create the RKE2 config dir] ********************************
2025-10-25 16:59:31,987 p=3410655 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:59:32,046 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Set server taints] *****************************************
2025-10-25 16:59:32,047 p=3410655 u=root n=ansible | ok: [k8s-m1]
2025-10-25 16:59:33,141 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Copy rke2 config] ******************************************
2025-10-25 16:59:33,142 p=3410655 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:59:34,112 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Copy Containerd Registry Configuration file] ***************
2025-10-25 16:59:34,112 p=3410655 u=root n=ansible | changed: [k8s-m1]
2025-10-25 16:59:36,362 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Start RKE2 service on the first server] ********************
2025-10-25 16:59:36,363 p=3410655 u=root n=ansible | fatal: [k8s-m1]: FAILED! => changed=false 
  msg: |-
    Unable to start service rke2-server.service: Job for rke2-server.service failed because the control process exited with error code.
    See "systemctl status rke2-server.service" and "journalctl -xeu rke2-server.service" for details.
2025-10-25 16:59:37,427 p=3410655 u=root n=ansible | RUNNING HANDLER [rke2-1.49.0 : reload sysctl] **********************************
2025-10-25 16:59:37,427 p=3410655 u=root n=ansible | changed: [k8s-w02]
2025-10-25 16:59:37,428 p=3410655 u=root n=ansible | changed: [k8s-w03]
2025-10-25 16:59:37,428 p=3410655 u=root n=ansible | changed: [k8s-w01]
2025-10-25 16:59:37,651 p=3410655 u=root n=ansible | TASK [rke2-1.49.0 : Final steps] ***********************************************
2025-10-25 16:59:37,704 p=3410655 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/summary.yml for k8s-w01, k8s-w02, k8s-w03
2025-10-25 16:59:38,116 p=3410655 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 16:59:38,116 p=3410655 u=root n=ansible | k8s-m1                     : ok=31   changed=13   unreachable=0    failed=1    skipped=39   rescued=0    ignored=0   
2025-10-25 16:59:38,116 p=3410655 u=root n=ansible | k8s-w01                    : ok=28   changed=11   unreachable=0    failed=0    skipped=41   rescued=0    ignored=0   
2025-10-25 16:59:38,116 p=3410655 u=root n=ansible | k8s-w02                    : ok=28   changed=11   unreachable=0    failed=0    skipped=41   rescued=0    ignored=0   
2025-10-25 16:59:38,117 p=3410655 u=root n=ansible | k8s-w03                    : ok=28   changed=11   unreachable=0    failed=0    skipped=41   rescued=0    ignored=0   
2025-10-25 17:03:01,898 p=3424269 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 17:03:03,619 p=3424269 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 17:03:03,620 p=3424269 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:05:43,746 p=3424269 u=root n=ansible |  [ERROR]: User interrupted execution

2025-10-25 17:05:45,499 p=3429647 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 17:05:47,336 p=3429647 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 17:05:47,337 p=3429647 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:06:23,001 p=3429647 u=root n=ansible |  [ERROR]: User interrupted execution

2025-10-25 17:14:07,467 p=6192 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ****************************************************
2025-10-25 17:14:10,487 p=6192 u=root n=ansible | TASK [Gathering Facts] **********************************************************************
2025-10-25 17:14:10,489 p=6192 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:14:10,719 p=6192 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:14:11,338 p=6192 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:14:11,471 p=6192 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:14:12,908 p=6192 u=root n=ansible | TASK [Gather facts] *************************************************************************
2025-10-25 17:14:12,909 p=6192 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:14:12,965 p=6192 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:14:12,969 p=6192 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:14:12,999 p=6192 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:14:14,535 p=6192 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] **********************************
2025-10-25 17:14:14,536 p=6192 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:14:14,539 p=6192 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:14:14,558 p=6192 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:14:14,564 p=6192 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:14:15,076 p=6192 u=root n=ansible | TASK [Detect default interface via ip route] ************************************************
2025-10-25 17:14:15,077 p=6192 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:14:15,088 p=6192 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:14:15,246 p=6192 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:14:15,257 p=6192 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:14:15,368 p=6192 u=root n=ansible | TASK [Set default values for netplan variables] *********************************************
2025-10-25 17:14:15,369 p=6192 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:14:15,372 p=6192 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:14:15,385 p=6192 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:14:15,409 p=6192 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:14:15,492 p=6192 u=root n=ansible | TASK [Validate required variables] **********************************************************
2025-10-25 17:14:15,493 p=6192 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: All assertions passed
2025-10-25 17:14:15,495 p=6192 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: All assertions passed
2025-10-25 17:14:15,497 p=6192 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: All assertions passed
2025-10-25 17:14:15,518 p=6192 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: All assertions passed
2025-10-25 17:14:15,912 p=6192 u=root n=ansible | TASK [Move existing netplan YAMLs to timestamped backup directory] **************************
2025-10-25 17:14:15,912 p=6192 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:14:15,923 p=6192 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:14:15,938 p=6192 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:14:15,963 p=6192 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:14:17,193 p=6192 u=root n=ansible | TASK [Render static netplan config] *********************************************************
2025-10-25 17:14:17,193 p=6192 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:14:17,201 p=6192 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:14:17,201 p=6192 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:14:17,281 p=6192 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:14:18,657 p=6192 u=root n=ansible | TASK [Apply netplan] ************************************************************************
2025-10-25 17:14:18,657 p=6192 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:14:18,704 p=6192 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:14:18,756 p=6192 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:14:18,759 p=6192 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:14:19,183 p=6192 u=root n=ansible | TASK [Verify network configuration] *********************************************************
2025-10-25 17:14:19,184 p=6192 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:14:19,186 p=6192 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:14:19,204 p=6192 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:14:19,210 p=6192 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:14:19,262 p=6192 u=root n=ansible | TASK [Show verification output] *************************************************************
2025-10-25 17:14:19,264 p=6192 u=root n=ansible | ok: [k8s-m1] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.11/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.11
2025-10-25 17:14:19,288 p=6192 u=root n=ansible | ok: [k8s-w01] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.14/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.14
2025-10-25 17:14:19,310 p=6192 u=root n=ansible | ok: [k8s-w02] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.15/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.15
2025-10-25 17:14:19,326 p=6192 u=root n=ansible | ok: [k8s-w03] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.16/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.16
2025-10-25 17:14:19,561 p=6192 u=root n=ansible | PLAY RECAP **********************************************************************************
2025-10-25 17:14:19,561 p=6192 u=root n=ansible | k8s-m1                     : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 17:14:19,561 p=6192 u=root n=ansible | k8s-w01                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 17:14:19,562 p=6192 u=root n=ansible | k8s-w02                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 17:14:19,562 p=6192 u=root n=ansible | k8s-w03                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 17:16:43,804 p=10274 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 17:16:45,573 p=10274 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 17:16:45,574 p=10274 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:16:45,596 p=10274 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:16:45,601 p=10274 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:16:45,606 p=10274 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:16:47,102 p=10274 u=root n=ansible | TASK [Gather facts] ************************************************************
2025-10-25 17:16:47,102 p=10274 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:16:47,107 p=10274 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:16:47,112 p=10274 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:16:47,126 p=10274 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:16:48,655 p=10274 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] *********************
2025-10-25 17:16:48,656 p=10274 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:16:48,717 p=10274 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:16:48,720 p=10274 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:16:48,763 p=10274 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:16:49,271 p=10274 u=root n=ansible | TASK [Detect default interface via ip route] ***********************************
2025-10-25 17:16:49,271 p=10274 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:16:49,272 p=10274 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:16:49,274 p=10274 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:16:49,313 p=10274 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:16:49,406 p=10274 u=root n=ansible | TASK [Set default values for netplan variables] ********************************
2025-10-25 17:16:49,407 p=10274 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:16:49,407 p=10274 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:16:49,434 p=10274 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:16:49,445 p=10274 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:16:49,515 p=10274 u=root n=ansible | TASK [Validate required variables] *********************************************
2025-10-25 17:16:49,517 p=10274 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: All assertions passed
2025-10-25 17:16:49,536 p=10274 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: All assertions passed
2025-10-25 17:16:49,542 p=10274 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: All assertions passed
2025-10-25 17:16:49,563 p=10274 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: All assertions passed
2025-10-25 17:16:49,961 p=10274 u=root n=ansible | TASK [Move existing netplan YAMLs to timestamped backup directory] *************
2025-10-25 17:16:49,961 p=10274 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:16:49,989 p=10274 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:16:50,027 p=10274 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:16:50,031 p=10274 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:16:51,163 p=10274 u=root n=ansible | TASK [Render static netplan config] ********************************************
2025-10-25 17:16:51,163 p=10274 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:16:51,170 p=10274 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:16:51,188 p=10274 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:16:51,202 p=10274 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:20:14,042 p=10274 u=root n=ansible |  [ERROR]: User interrupted execution

2025-10-25 17:20:16,615 p=17712 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 17:21:17,291 p=17712 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 17:21:17,292 p=17712 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:21:31,707 p=17712 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:21:44,144 p=17712 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:21:49,446 p=17712 u=root n=ansible |  [ERROR]: User interrupted execution

2025-10-25 17:23:52,762 p=23108 u=root n=ansible | PLAY [Configure static IPv4 via Netplan] ***************************************
2025-10-25 17:23:54,533 p=23108 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 17:23:54,534 p=23108 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:23:54,544 p=23108 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:23:54,549 p=23108 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:23:55,429 p=23108 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:23:56,991 p=23108 u=root n=ansible | TASK [Gather facts] ************************************************************
2025-10-25 17:23:56,992 p=23108 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:23:57,036 p=23108 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:23:57,117 p=23108 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:23:59,090 p=23108 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:24:00,726 p=23108 u=root n=ansible | TASK [Ensure netplan is installed on Debian-based systems] *********************
2025-10-25 17:24:00,726 p=23108 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:24:00,793 p=23108 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:24:00,805 p=23108 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:24:00,870 p=23108 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:24:01,431 p=23108 u=root n=ansible | TASK [Detect default interface via ip route] ***********************************
2025-10-25 17:24:01,432 p=23108 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:24:01,435 p=23108 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:24:01,437 p=23108 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:24:01,451 p=23108 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:24:01,536 p=23108 u=root n=ansible | TASK [Set default values for netplan variables] ********************************
2025-10-25 17:24:01,537 p=23108 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:24:01,567 p=23108 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:24:01,577 p=23108 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:24:01,618 p=23108 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:24:01,727 p=23108 u=root n=ansible | TASK [Validate required variables] *********************************************
2025-10-25 17:24:01,730 p=23108 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: All assertions passed
2025-10-25 17:24:01,731 p=23108 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: All assertions passed
2025-10-25 17:24:01,731 p=23108 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: All assertions passed
2025-10-25 17:24:01,752 p=23108 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: All assertions passed
2025-10-25 17:24:02,158 p=23108 u=root n=ansible | TASK [Move existing netplan YAMLs to timestamped backup directory] *************
2025-10-25 17:24:02,159 p=23108 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:24:02,159 p=23108 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:24:02,188 p=23108 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:24:02,253 p=23108 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:24:03,418 p=23108 u=root n=ansible | TASK [Render static netplan config] ********************************************
2025-10-25 17:24:03,419 p=23108 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:24:03,430 p=23108 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:24:03,453 p=23108 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:24:03,462 p=23108 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:24:04,931 p=23108 u=root n=ansible | TASK [Apply netplan] ***********************************************************
2025-10-25 17:24:04,931 p=23108 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:24:04,939 p=23108 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:24:04,980 p=23108 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:24:05,010 p=23108 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:24:06,455 p=23108 u=root n=ansible | TASK [Verify network configuration] ********************************************
2025-10-25 17:24:06,455 p=23108 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:24:06,467 p=23108 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:24:06,472 p=23108 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:24:06,502 p=23108 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:24:06,593 p=23108 u=root n=ansible | TASK [Show verification output] ************************************************
2025-10-25 17:24:06,596 p=23108 u=root n=ansible | ok: [k8s-m1] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.11/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    default via 10.0.6.1 dev ens34 proto static
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.11
  
    PING 10.0.6.1 (10.0.6.1) 56(84) bytes of data.
    64 bytes from 10.0.6.1: icmp_seq=1 ttl=128 time=0.795 ms
    64 bytes from 10.0.6.1: icmp_seq=2 ttl=128 time=0.589 ms
  
    --- 10.0.6.1 ping statistics ---
    2 packets transmitted, 2 received, 0% packet loss, time 1002ms
    rtt min/avg/max/mdev = 0.589/0.692/0.795/0.103 ms
2025-10-25 17:24:06,624 p=23108 u=root n=ansible | ok: [k8s-w01] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.14/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    default via 10.0.6.1 dev ens34 proto static
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.14
  
    PING 10.0.6.1 (10.0.6.1) 56(84) bytes of data.
    64 bytes from 10.0.6.1: icmp_seq=1 ttl=128 time=0.984 ms
    64 bytes from 10.0.6.1: icmp_seq=2 ttl=128 time=0.620 ms
  
    --- 10.0.6.1 ping statistics ---
    2 packets transmitted, 2 received, 0% packet loss, time 1002ms
    rtt min/avg/max/mdev = 0.620/0.802/0.984/0.182 ms
2025-10-25 17:24:06,627 p=23108 u=root n=ansible | ok: [k8s-w02] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.15/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    default via 10.0.6.1 dev ens34 proto static
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.15
  
    PING 10.0.6.1 (10.0.6.1) 56(84) bytes of data.
    64 bytes from 10.0.6.1: icmp_seq=1 ttl=128 time=5.13 ms
    64 bytes from 10.0.6.1: icmp_seq=2 ttl=128 time=0.629 ms
  
    --- 10.0.6.1 ping statistics ---
    2 packets transmitted, 2 received, 0% packet loss, time 1002ms
    rtt min/avg/max/mdev = 0.629/2.880/5.132/2.251 ms
2025-10-25 17:24:06,643 p=23108 u=root n=ansible | ok: [k8s-w03] => 
  net_verify.stdout: |-
    2: ens34: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
        altname enp2s2
        inet 10.0.6.16/24 brd 10.0.6.255 scope global ens34
           valid_lft forever preferred_lft forever
  
    default via 10.0.6.1 dev ens34 proto static
    10.0.6.0/24 dev ens34 proto kernel scope link src 10.0.6.16
  
    PING 10.0.6.1 (10.0.6.1) 56(84) bytes of data.
    64 bytes from 10.0.6.1: icmp_seq=1 ttl=128 time=0.562 ms
    64 bytes from 10.0.6.1: icmp_seq=2 ttl=128 time=0.735 ms
  
    --- 10.0.6.1 ping statistics ---
    2 packets transmitted, 2 received, 0% packet loss, time 1031ms
    rtt min/avg/max/mdev = 0.562/0.648/0.735/0.086 ms
2025-10-25 17:24:06,933 p=23108 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 17:24:06,933 p=23108 u=root n=ansible | k8s-m1                     : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 17:24:06,933 p=23108 u=root n=ansible | k8s-w01                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 17:24:06,933 p=23108 u=root n=ansible | k8s-w02                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 17:24:06,934 p=23108 u=root n=ansible | k8s-w03                    : ok=11   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-10-25 17:25:17,022 p=25294 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] ****
2025-10-25 17:25:18,784 p=25294 u=root n=ansible | TASK [Gathering Facts] *********************************************************
2025-10-25 17:25:18,786 p=25294 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:25:18,799 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:25:18,805 p=25294 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:25:18,814 p=25294 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:25:19,022 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 17:25:19,023 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:25:19,049 p=25294 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:25:19,084 p=25294 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:25:19,091 p=25294 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:25:20,257 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] **********************
2025-10-25 17:25:20,258 p=25294 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:25:20,263 p=25294 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:25:20,268 p=25294 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:25:20,285 p=25294 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:25:20,452 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] ******************
2025-10-25 17:25:20,640 p=25294 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-w01, k8s-w02, k8s-w03
2025-10-25 17:25:21,823 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Configure kernel modules] **********************************
2025-10-25 17:25:21,823 p=25294 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:25:21,824 p=25294 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:25:21,838 p=25294 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:25:21,853 p=25294 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:25:22,201 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Detect NetworkManager] *************************************
2025-10-25 17:25:22,202 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:25:22,235 p=25294 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:25:22,245 p=25294 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:25:22,298 p=25294 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:25:23,347 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Configure sysctl parameters] *******************************
2025-10-25 17:25:23,347 p=25294 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:25:23,366 p=25294 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:25:23,380 p=25294 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:25:23,392 p=25294 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:25:24,134 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Apply system configurations] *******************************
2025-10-25 17:25:24,135 p=25294 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:25:24,151 p=25294 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:25:24,161 p=25294 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:25:24,172 p=25294 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:25:26,805 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Ensure UFW is installed] ***********************************
2025-10-25 17:25:26,806 p=25294 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:25:26,824 p=25294 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:25:26,836 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:25:26,888 p=25294 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:25:28,452 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Enable UFW] ************************************************
2025-10-25 17:25:28,452 p=25294 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:25:28,470 p=25294 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:25:28,478 p=25294 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:25:29,541 p=25294 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:25:32,231 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Set default incoming policy to deny] ***********************
2025-10-25 17:25:32,232 p=25294 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:25:32,272 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:25:32,276 p=25294 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:25:32,505 p=25294 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:25:35,250 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Set default outgoing policy to allow] **********************
2025-10-25 17:25:35,251 p=25294 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:25:35,383 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:25:35,494 p=25294 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:25:35,979 p=25294 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:25:36,972 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Allow required ports] **************************************
2025-10-25 17:25:36,973 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=22/tcp)
2025-10-25 17:25:36,985 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=22/tcp)
2025-10-25 17:25:37,029 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=22/tcp)
2025-10-25 17:25:37,153 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=22/tcp)
2025-10-25 17:25:37,878 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=179/tcp)
2025-10-25 17:25:37,899 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=179/tcp)
2025-10-25 17:25:37,912 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=179/tcp)
2025-10-25 17:25:38,045 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=179/tcp)
2025-10-25 17:25:38,764 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=9345/tcp)
2025-10-25 17:25:38,779 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=9345/tcp)
2025-10-25 17:25:38,816 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=9345/tcp)
2025-10-25 17:25:38,928 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=9345/tcp)
2025-10-25 17:25:39,719 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=6443/tcp)
2025-10-25 17:25:39,727 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=6443/tcp)
2025-10-25 17:25:39,739 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=6443/tcp)
2025-10-25 17:25:39,914 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=6443/tcp)
2025-10-25 17:25:40,642 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=10250/tcp)
2025-10-25 17:25:40,656 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=10250/tcp)
2025-10-25 17:25:40,656 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=10250/tcp)
2025-10-25 17:25:40,839 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=10250/tcp)
2025-10-25 17:25:41,536 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=2379/tcp)
2025-10-25 17:25:41,555 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=2379/tcp)
2025-10-25 17:25:41,570 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=2379/tcp)
2025-10-25 17:25:41,759 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=2379/tcp)
2025-10-25 17:25:42,433 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=2380/tcp)
2025-10-25 17:25:42,441 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=2380/tcp)
2025-10-25 17:25:42,461 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=2380/tcp)
2025-10-25 17:25:42,643 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=2380/tcp)
2025-10-25 17:25:43,320 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=2381/tcp)
2025-10-25 17:25:43,321 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=2381/tcp)
2025-10-25 17:25:43,322 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=2381/tcp)
2025-10-25 17:25:43,558 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=2381/tcp)
2025-10-25 17:25:44,228 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=30000:32767/tcp)
2025-10-25 17:25:44,239 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=30000:32767/tcp)
2025-10-25 17:25:44,288 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=30000:32767/tcp)
2025-10-25 17:25:44,502 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=30000:32767/tcp)
2025-10-25 17:25:45,141 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=8472/udp)
2025-10-25 17:25:45,156 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=8472/udp)
2025-10-25 17:25:45,170 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=8472/udp)
2025-10-25 17:25:45,390 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=8472/udp)
2025-10-25 17:25:46,006 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=6081/udp)
2025-10-25 17:25:46,059 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=6081/udp)
2025-10-25 17:25:46,067 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=6081/udp)
2025-10-25 17:25:46,253 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=6081/udp)
2025-10-25 17:25:46,900 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=4240/tcp)
2025-10-25 17:25:46,942 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=4240/tcp)
2025-10-25 17:25:46,985 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=4240/tcp)
2025-10-25 17:25:47,161 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=4240/tcp)
2025-10-25 17:25:47,795 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=4244/tcp)
2025-10-25 17:25:47,825 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=4244/tcp)
2025-10-25 17:25:47,861 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=4244/tcp)
2025-10-25 17:25:48,030 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=4244/tcp)
2025-10-25 17:25:48,713 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=4245/tcp)
2025-10-25 17:25:48,721 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=4245/tcp)
2025-10-25 17:25:48,763 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=4245/tcp)
2025-10-25 17:25:48,924 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=4245/tcp)
2025-10-25 17:25:49,599 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=4222/tcp)
2025-10-25 17:25:49,601 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=4222/tcp)
2025-10-25 17:25:49,644 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=4222/tcp)
2025-10-25 17:25:49,863 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=4222/tcp)
2025-10-25 17:25:50,530 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=9966/tcp)
2025-10-25 17:25:50,550 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=9966/tcp)
2025-10-25 17:25:50,571 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=9966/tcp)
2025-10-25 17:25:50,752 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=9966/tcp)
2025-10-25 17:25:51,432 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=4250:4251/tcp)
2025-10-25 17:25:51,492 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=4250:4251/tcp)
2025-10-25 17:25:51,493 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=4250:4251/tcp)
2025-10-25 17:25:51,678 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=4250:4251/tcp)
2025-10-25 17:25:52,316 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=6060:6062/tcp)
2025-10-25 17:25:52,372 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=6060:6062/tcp)
2025-10-25 17:25:52,388 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=6060:6062/tcp)
2025-10-25 17:25:52,551 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=6060:6062/tcp)
2025-10-25 17:25:53,219 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=9878:9879/tcp)
2025-10-25 17:25:53,260 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=9878:9879/tcp)
2025-10-25 17:25:53,272 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=9878:9879/tcp)
2025-10-25 17:25:53,450 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=9878:9879/tcp)
2025-10-25 17:25:54,144 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=9890:9893/tcp)
2025-10-25 17:25:54,200 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=9890:9893/tcp)
2025-10-25 17:25:54,217 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=9890:9893/tcp)
2025-10-25 17:25:54,390 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=9890:9893/tcp)
2025-10-25 17:25:55,062 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=9901/tcp)
2025-10-25 17:25:55,118 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=9901/tcp)
2025-10-25 17:25:55,124 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=9901/tcp)
2025-10-25 17:25:55,283 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=9901/tcp)
2025-10-25 17:25:56,008 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=9962:9964/tcp)
2025-10-25 17:25:56,050 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=9962:9964/tcp)
2025-10-25 17:25:56,073 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=9962:9964/tcp)
2025-10-25 17:25:56,174 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=9962:9964/tcp)
2025-10-25 17:25:56,943 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=51871/udp)
2025-10-25 17:25:56,952 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=51871/udp)
2025-10-25 17:25:57,036 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=51871/udp)
2025-10-25 17:25:57,070 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=51871/udp)
2025-10-25 17:25:57,868 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=4567/tcp)
2025-10-25 17:25:57,942 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=4567/tcp)
2025-10-25 17:25:57,974 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=4567/tcp)
2025-10-25 17:25:58,065 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=4567/tcp)
2025-10-25 17:25:58,829 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=4567/udp)
2025-10-25 17:25:58,884 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=4567/udp)
2025-10-25 17:25:58,892 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=4567/udp)
2025-10-25 17:25:58,964 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=4567/udp)
2025-10-25 17:25:59,744 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=4568/tcp)
2025-10-25 17:25:59,801 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=4568/tcp)
2025-10-25 17:25:59,808 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=4568/tcp)
2025-10-25 17:25:59,863 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=4568/tcp)
2025-10-25 17:26:00,732 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=4444/tcp)
2025-10-25 17:26:00,737 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=4444/tcp)
2025-10-25 17:26:00,743 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=4444/tcp)
2025-10-25 17:26:00,775 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=4444/tcp)
2025-10-25 17:26:01,660 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=7443/tcp)
2025-10-25 17:26:01,674 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=7443/tcp)
2025-10-25 17:26:01,680 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=7443/tcp)
2025-10-25 17:26:01,765 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=7443/tcp)
2025-10-25 17:26:02,588 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=7345/tcp)
2025-10-25 17:26:02,621 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=7345/tcp)
2025-10-25 17:26:02,639 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=7345/tcp)
2025-10-25 17:26:02,712 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=7345/tcp)
2025-10-25 17:26:03,551 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=8080/tcp)
2025-10-25 17:26:03,582 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=8080/tcp)
2025-10-25 17:26:03,699 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=8080/tcp)
2025-10-25 17:26:03,730 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=8080/tcp)
2025-10-25 17:26:04,490 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=8443/tcp)
2025-10-25 17:26:04,727 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=8443/tcp)
2025-10-25 17:26:04,730 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=8443/tcp)
2025-10-25 17:26:04,731 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=8443/tcp)
2025-10-25 17:26:05,466 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=3306/tcp)
2025-10-25 17:26:05,719 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=3306/tcp)
2025-10-25 17:26:05,728 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=3306/tcp)
2025-10-25 17:26:05,776 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=3306/tcp)
2025-10-25 17:26:06,431 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=3307/tcp)
2025-10-25 17:26:06,652 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=3307/tcp)
2025-10-25 17:26:06,678 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=3307/tcp)
2025-10-25 17:26:06,838 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=3307/tcp)
2025-10-25 17:26:07,406 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=80/tcp)
2025-10-25 17:26:07,557 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=80/tcp)
2025-10-25 17:26:07,663 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=80/tcp)
2025-10-25 17:26:07,752 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=80/tcp)
2025-10-25 17:26:08,341 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=443/tcp)
2025-10-25 17:26:08,497 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=443/tcp)
2025-10-25 17:26:08,684 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=443/tcp)
2025-10-25 17:26:08,704 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=443/tcp)
2025-10-25 17:26:09,457 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Create RKE2 artifacts folder] ******************************
2025-10-25 17:26:09,458 p=25294 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:26:09,472 p=25294 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:26:09,483 p=25294 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:26:09,490 p=25294 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:26:13,057 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Download sha256 checksum file ( airgap mode )] *************
2025-10-25 17:26:13,063 p=25294 u=root n=ansible | [WARNING]: Module remote_tmp /root/.ansible/tmp did not exist and was created
with a mode of 0700, this may cause issues when running as another user. To
avoid this, create the remote_tmp dir with the correct permissions manually

2025-10-25 17:26:13,063 p=25294 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:26:13,064 p=25294 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:26:13,064 p=25294 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:26:13,065 p=25294 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:26:14,516 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 artifacts and compare with checksums ( airgap mode )] ***
2025-10-25 17:26:14,517 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 17:26:14,563 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 17:26:14,733 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 17:26:14,769 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 17:26:34,075 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 17:26:35,558 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 17:26:35,668 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 17:26:44,730 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 17:26:46,707 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 17:26:47,050 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 17:26:58,017 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 17:26:58,501 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 17:27:01,609 p=25294 u=root n=ansible | changed: [k8s-m1] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 17:27:02,127 p=25294 u=root n=ansible | changed: [k8s-w03] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 17:30:05,328 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 17:30:16,368 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 17:30:55,587 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 17:30:59,194 p=25294 u=root n=ansible | changed: [k8s-w02] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 17:31:10,480 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 17:31:14,487 p=25294 u=root n=ansible | changed: [k8s-w01] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 17:31:15,100 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 install script ( airgap mode )] **************
2025-10-25 17:31:15,101 p=25294 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:31:15,113 p=25294 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:31:15,119 p=25294 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:31:15,142 p=25294 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:31:21,809 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Populate service facts] ************************************
2025-10-25 17:31:21,809 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:31:21,822 p=25294 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:31:21,828 p=25294 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:31:21,839 p=25294 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:31:22,226 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Get stats of the FS object] ********************************
2025-10-25 17:31:22,227 p=25294 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:31:22,246 p=25294 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:31:22,252 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:31:22,260 p=25294 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:31:22,627 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Check if separate partition] *******************************
2025-10-25 17:31:22,627 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:31:22,637 p=25294 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:31:22,637 p=25294 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:31:22,683 p=25294 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:31:22,759 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 bin path] *****************************************
2025-10-25 17:31:22,761 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:31:22,762 p=25294 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:31:22,762 p=25294 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:31:22,776 p=25294 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:31:23,178 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Check RKE2 version] ****************************************
2025-10-25 17:31:23,178 p=25294 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:31:23,202 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:31:23,210 p=25294 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:31:23,210 p=25294 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:31:23,294 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 versions] *****************************************
2025-10-25 17:31:23,295 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:31:23,295 p=25294 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:31:23,319 p=25294 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:31:23,329 p=25294 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:32:15,137 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Run RKE2 install script with airgap variables] *************
2025-10-25 17:32:15,138 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:32:35,837 p=25294 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:32:40,531 p=25294 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:32:41,328 p=25294 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:32:41,910 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Find Active Server] ****************************************
2025-10-25 17:32:42,013 p=25294 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/find_active_server.yml for k8s-m1, k8s-w01, k8s-w02, k8s-w03
2025-10-25 17:32:48,045 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Populate services facts] ***********************************
2025-10-25 17:32:48,046 p=25294 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:32:48,064 p=25294 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:32:48,094 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:32:48,165 p=25294 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:32:48,808 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Prepare very first server node in the cluster] *************
2025-10-25 17:32:48,857 p=25294 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/first_server.yml for k8s-m1
2025-10-25 17:32:49,234 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Create the RKE2 config dir] ********************************
2025-10-25 17:32:49,234 p=25294 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:32:49,281 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Set server taints] *****************************************
2025-10-25 17:32:49,281 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:32:50,249 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Copy rke2 config] ******************************************
2025-10-25 17:32:50,249 p=25294 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:32:51,133 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Copy Containerd Registry Configuration file] ***************
2025-10-25 17:32:51,133 p=25294 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:35:37,787 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Start RKE2 service on the first server] ********************
2025-10-25 17:35:37,788 p=25294 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:35:39,112 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Mask RKE2 agent service on the first server] ***************
2025-10-25 17:35:39,112 p=25294 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:36:27,347 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Wait for the first server be ready - with CNI] *************
2025-10-25 17:36:27,347 p=25294 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:36:27,694 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Set an Active Server variable] *****************************
2025-10-25 17:36:27,694 p=25294 u=root n=ansible | ok: [k8s-m1] => (item=k8s-m1)
2025-10-25 17:36:27,716 p=25294 u=root n=ansible | ok: [k8s-m1 -> k8s-w01(10.0.6.14)] => (item=k8s-w01)
2025-10-25 17:36:27,736 p=25294 u=root n=ansible | ok: [k8s-m1 -> k8s-w02(10.0.6.15)] => (item=k8s-w02)
2025-10-25 17:36:27,761 p=25294 u=root n=ansible | ok: [k8s-m1 -> k8s-w03(10.0.6.16)] => (item=k8s-w03)
2025-10-25 17:36:28,012 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Download kubeconfig to ansible localhost] ******************
2025-10-25 17:36:28,065 p=25294 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/download_kubeconfig.yaml for k8s-m1
2025-10-25 17:36:28,611 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 kubeconfig to localhost] *********************
2025-10-25 17:36:28,612 p=25294 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:36:28,725 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Replace loopback IP by master server IP] *******************
2025-10-25 17:36:28,726 p=25294 u=root n=ansible | fatal: [k8s-m1 -> localhost]: FAILED! => 
  msg: Failed to import the required Python library (netaddr) on vnpt.vn's Python /usr/bin/python3.12. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter
2025-10-25 17:36:28,782 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Prepare and join remaining nodes of the cluster] ***********
2025-10-25 17:36:28,887 p=25294 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/remaining_nodes.yml for k8s-w01, k8s-w02, k8s-w03
2025-10-25 17:36:29,377 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Create the RKE2 config dir] ********************************
2025-10-25 17:36:29,378 p=25294 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:36:29,394 p=25294 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:36:29,405 p=25294 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:36:29,555 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Set agent taints] ******************************************
2025-10-25 17:36:29,556 p=25294 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:36:29,556 p=25294 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:36:29,576 p=25294 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:36:29,883 p=25294 u=root n=ansible | TASK [rke2-1.49.0 : Copy RKE2 config] ******************************************
2025-10-25 17:36:29,884 p=25294 u=root n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleFilterError: Failed to import the required Python library (netaddr) on vnpt.vn's Python /usr/bin/python3.12. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter
2025-10-25 17:36:29,885 p=25294 u=root n=ansible | fatal: [k8s-w02]: FAILED! => changed=false 
  msg: 'AnsibleFilterError: Failed to import the required Python library (netaddr) on vnpt.vn''s Python /usr/bin/python3.12. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter'
2025-10-25 17:36:29,887 p=25294 u=root n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleFilterError: Failed to import the required Python library (netaddr) on vnpt.vn's Python /usr/bin/python3.12. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter
2025-10-25 17:36:29,888 p=25294 u=root n=ansible | fatal: [k8s-w03]: FAILED! => changed=false 
  msg: 'AnsibleFilterError: Failed to import the required Python library (netaddr) on vnpt.vn''s Python /usr/bin/python3.12. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter'
2025-10-25 17:36:29,888 p=25294 u=root n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleFilterError: Failed to import the required Python library (netaddr) on vnpt.vn's Python /usr/bin/python3.12. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter
2025-10-25 17:36:29,889 p=25294 u=root n=ansible | fatal: [k8s-w01]: FAILED! => changed=false 
  msg: 'AnsibleFilterError: Failed to import the required Python library (netaddr) on vnpt.vn''s Python /usr/bin/python3.12. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter'
2025-10-25 17:36:29,891 p=25294 u=root n=ansible | PLAY RECAP *********************************************************************
2025-10-25 17:36:29,892 p=25294 u=root n=ansible | k8s-m1                     : ok=37   changed=16   unreachable=0    failed=1    skipped=45   rescued=0    ignored=0   
2025-10-25 17:36:29,892 p=25294 u=root n=ansible | k8s-w01                    : ok=29   changed=11   unreachable=0    failed=1    skipped=36   rescued=0    ignored=0   
2025-10-25 17:36:29,892 p=25294 u=root n=ansible | k8s-w02                    : ok=29   changed=11   unreachable=0    failed=1    skipped=36   rescued=0    ignored=0   
2025-10-25 17:36:29,892 p=25294 u=root n=ansible | k8s-w03                    : ok=29   changed=11   unreachable=0    failed=1    skipped=36   rescued=0    ignored=0   
2025-10-25 17:43:42,578 p=55263 u=root n=ansible | ansible-playbook [core 2.16.3]
  config file = /opt/git/ansible/ansible.cfg
  configured module search path = ['/root/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']
  ansible python module location = /usr/lib/python3.12/site-packages/ansible
  ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections
  executable location = /usr/bin/ansible-playbook
  python version = 3.12.11 (main, Aug 15 2025, 13:38:57) [GCC 8.5.0 20210514 (Red Hat 8.5.0-28)] (/usr/bin/python3.12)
  jinja version = 3.1.2
  libyaml = True
2025-10-25 17:43:42,579 p=55263 u=root n=ansible | Using /opt/git/ansible/ansible.cfg as config file
2025-10-25 17:43:42,758 p=55263 u=root n=ansible | host_list declined parsing /opt/git/ansible/inventories/hosts as it did not pass its verify_file() method
2025-10-25 17:43:42,758 p=55263 u=root n=ansible | script declined parsing /opt/git/ansible/inventories/hosts as it did not pass its verify_file() method
2025-10-25 17:43:42,758 p=55263 u=root n=ansible | auto declined parsing /opt/git/ansible/inventories/hosts as it did not pass its verify_file() method
2025-10-25 17:43:42,763 p=55263 u=root n=ansible | Parsed /opt/git/ansible/inventories/hosts inventory source with ini plugin
2025-10-25 17:43:42,987 p=55263 u=root n=ansible | redirecting (type: callback) ansible.builtin.yaml to community.general.yaml
2025-10-25 17:43:42,988 p=55263 u=root n=ansible | redirecting (type: callback) ansible.builtin.yaml to community.general.yaml
2025-10-25 17:43:43,015 p=55263 u=root n=ansible | Skipping callback 'default', as we already have a stdout callback.
2025-10-25 17:43:43,015 p=55263 u=root n=ansible | Skipping callback 'minimal', as we already have a stdout callback.
2025-10-25 17:43:43,015 p=55263 u=root n=ansible | Skipping callback 'oneline', as we already have a stdout callback.
2025-10-25 17:43:43,015 p=55263 u=root n=ansible | PLAYBOOK: rke2-airgap-install.yml ****************************************************
2025-10-25 17:43:43,015 p=55263 u=root n=ansible | 1 plays in playbooks/rke2-airgap-install.yml
2025-10-25 17:43:43,018 p=55263 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] **********
2025-10-25 17:43:43,390 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/setup.py
2025-10-25 17:43:43,391 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:43:43,392 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:43,392 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=cdyuxorfotbgykhqipnbewvdrqzagcib] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-cdyuxorfotbgykhqipnbewvdrqzagcib ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:43:44,360 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:43:46,880 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"ansible_facts": {"ansible_user_id": "root", "ansible_user_uid": 0, "ansible_user_gid": 0, "ansible_user_gecos": "root", "ansible_user_dir": "/root", "ansible_user_shell": "/bin/bash", "ansible_real_user_id": 0, "ansible_effective_user_id": 0, "ansible_real_group_id": 0, "ansible_effective_group_id": 0, "ansible_fips": false, "ansible_virtualization_role": "guest", "ansible_virtualization_type": "VMware", "ansible_virtualization_tech_guest": ["VMware"], "ansible_virtualization_tech_host": [], "ansible_system": "Linux", "ansible_kernel": "6.8.0-86-generic", "ansible_kernel_version": "#87-Ubuntu SMP PREEMPT_DYNAMIC Mon Sep 22 18:03:36 UTC 2025", "ansible_machine": "x86_64", "ansible_python_version": "3.12.3", "ansible_fqdn": "k8s-m1", "ansible_hostname": "k8s-m1", "ansible_nodename": "k8s-m1", "ansible_domain": "", "ansible_userspace_bits": "64", "ansible_architecture": "x86_64", "ansible_userspace_architecture": "x86_64", "ansible_machine_id": "b79012181a3f45208e1e5ef6d1d0cddb", "ansible_local": {}, "ansible_ssh_host_key_rsa_public": "AAAAB3NzaC1yc2EAAAADAQABAAABgQC0mbgptCs2BtsrCFs75+XBiC5aKGapf0XNWfK9alLcjExAyy+fYg4vA0KpKvnW/Nsug+12qHb0p52KZZnTI7cxrCjEdEg2Gk56+V/J4VUCor6nqntJUvwK/NXHwkABtRmOfznP6k9pqSR/6TENhORGazWL0ce6GUYpp+RXPnrWydyXvXN93wIdeJs3ZBLnQW2Ckv7jR/f2cNKTAiIlkTizGwMplGhw4zs/NzO3uZ9sq7UyOWk5LqZ8j6ioIiPwKe5JXip5QK8jOiQZFfY3OqV+2wMJ5LLJHsVFhukUT5R1ScOTGLKRWqzHMQhAAYd8BaWAQcOs50VGS1RStrCcXr67+E0FiOlgQPYwOAWEKweI5k6/jp0gfpgEXX4uoyaDnf7xkASdWU5czk33ltz81gtIS6qj9FucwIpNL1jw167rdbW0jB0qRNoyWADQ41D/84qSa5pX3J+BBshq/vYTcJJ8VpLuEHmf9A0Ewh+9akilu+8qYoo8UORB9DlGXqqR7Mc=", "ansible_ssh_host_key_rsa_public_keytype": "ssh-rsa", "ansible_ssh_host_key_ecdsa_public": "AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBDdbnZojwqe5YmI1uxG+x0v+rds0x0uKfFCwlhtAKmHy7CZd7kKYDtWkpX5KOhTMWwKJgYhBsP5ZIL+QP8YnZOE=", "ansible_ssh_host_key_ecdsa_public_keytype": "ecdsa-sha2-nistp256", "ansible_ssh_host_key_ed25519_public": "AAAAC3NzaC1lZDI1NTE5AAAAIPJHl29MLVghWbM4yw2gfJiXXKZgxpI7/CPNzL7/x/an", "ansible_ssh_host_key_ed25519_public_keytype": "ssh-ed25519", "ansible_distribution": "Ubuntu", "ansible_distribution_release": "noble", "ansible_distribution_version": "24.04", "ansible_distribution_major_version": "24", "ansible_distribution_file_path": "/etc/os-release", "ansible_distribution_file_variety": "Debian", "ansible_distribution_file_parsed": true, "ansible_os_family": "Debian", "ansible_is_chroot": false, "ansible_apparmor": {"status": "enabled"}, "ansible_env": {"SUDO_GID": "1000", "MAIL": "/var/mail/root", "USER": "root", "HOME": "/root", "SUDO_UID": "1000", "LOGNAME": "root", "TERM": "unknown", "PATH": "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin", "LANG": "en_US.UTF-8", "SUDO_COMMAND": "/bin/sh -c echo BECOME-SUCCESS-cdyuxorfotbgykhqipnbewvdrqzagcib ; /usr/bin/python3", "SHELL": "/bin/bash", "SUDO_USER": "ubuntu", "PWD": "/home/ubuntu"}, "ansible_selinux_python_present": true, "ansible_selinux": {"status": "disabled"}, "ansible_system_capabilities_enforced": "False", "ansible_system_capabilities": [], "ansible_fibre_channel_wwn": [], "ansible_loadavg": {"1m": 0.8681640625, "5m": 0.8994140625, "15m": 0.6787109375}, "ansible_cmdline": {"BOOT_IMAGE": "/vmlinuz-6.8.0-86-generic", "root": "/dev/mapper/ubuntu--vg-ubuntu--lv", "ro": true}, "ansible_proc_cmdline": {"BOOT_IMAGE": "/vmlinuz-6.8.0-86-generic", "root": "/dev/mapper/ubuntu--vg-ubuntu--lv", "ro": true}, "ansible_python": {"version": {"major": 3, "minor": 12, "micro": 3, "releaselevel": "final", "serial": 0}, "version_info": [3, 12, 3, "final", 0], "executable": "/usr/bin/python3", "has_sslcontext": true, "type": "cpython"}, "ansible_pkg_mgr": "apt", "ansible_iscsi_iqn": "iqn.2004-10.com.ubuntu:01:dfbae5d5031", "ansible_lsb": {"id": "Ubuntu", "description": "Ubuntu 24.04.3 LTS", "release": "24.04", "codename": "noble", "major_release": "24"}, "ansible_date_time": {"year": "2025", "month": "10", "weekday": "Saturday", "weekday_number": "6", "weeknumber": "42", "day": "25", "hour": "10", "minute": "43", "second": "45", "epoch": "1761389025", "epoch_int": "1761389025", "date": "2025-10-25", "time": "10:43:45", "iso8601_micro": "2025-10-25T10:43:45.240201Z", "iso8601": "2025-10-25T10:43:45Z", "iso8601_basic": "20251025T104345240201", "iso8601_basic_short": "20251025T104345", "tz": "UTC", "tz_dst": "UTC", "tz_offset": "+0000"}, "ansible_processor": ["0", "GenuineIntel", "Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz", "1", "GenuineIntel", "Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz", "2", "GenuineIntel", "Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz", "3", "GenuineIntel", "Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz"], "ansible_processor_count": 4, "ansible_processor_cores": 1, "ansible_processor_threads_per_core": 1, "ansible_processor_vcpus": 4, "ansible_processor_nproc": 4, "ansible_memtotal_mb": 3868, "ansible_memfree_mb": 141, "ansible_swaptotal_mb": 3867, "ansible_swapfree_mb": 3867, "ansible_memory_mb": {"real": {"total": 3868, "used": 3727, "free": 141}, "nocache": {"free": 2731, "used": 1137}, "swap": {"total": 3867, "free": 3867, "used": 0, "cached": 0}}, "ansible_bios_date": "07/22/2020", "ansible_bios_vendor": "Phoenix Technologies LTD", "ansible_bios_version": "6.00", "ansible_board_asset_tag": "NA", "ansible_board_name": "440BX Desktop Reference Platform", "ansible_board_serial": "None", "ansible_board_vendor": "Intel Corporation", "ansible_board_version": "None", "ansible_chassis_asset_tag": "No Asset Tag", "ansible_chassis_serial": "None", "ansible_chassis_vendor": "No Enclosure", "ansible_chassis_version": "N/A", "ansible_form_factor": "Other", "ansible_product_name": "VMware Virtual Platform", "ansible_product_serial": "VMware-56 4d 94 da 92 c1 92 4f-58 39 a6 66 8a d7 57 c4", "ansible_product_uuid": "da944d56-c192-4f92-5839-a6668ad757c4", "ansible_product_version": "None", "ansible_system_vendor": "VMware, Inc.", "ansible_devices": {"loop1": {"virtual": 1, "links": {"ids": [], "uuids": [], "labels": [], "masters": []}, "vendor": null, "model": null, "sas_address": null, "sas_device_handle": null, "removable": "0", "support_discard": "512", "partitions": {}, "rotational": "1", "scheduler_mode": "none", "sectors": "0", "sectorsize": "512", "size": "0.00 Bytes", "host": "", "holders": []}, "loop6": {"virtual": 1, "links": {"ids": [], "uuids": [], "labels": [], "masters": []}, "vendor": null, "model": null, "sas_address": null, "sas_device_handle": null, "removable": "0", "support_discard": "512", "partitions": {}, "rotational": "1", "scheduler_mode": "none", "sectors": "0", "sectorsize": "512", "size": "0.00 Bytes", "host": "", "holders": []}, "loop4": {"virtual": 1, "links": {"ids": [], "uuids": [], "labels": [], "masters": []}, "vendor": null, "model": null, "sas_address": null, "sas_device_handle": null, "removable": "0", "support_discard": "512", "partitions": {}, "rotational": "1", "scheduler_mode": "none", "sectors": "0", "sectorsize": "512", "size": "0.00 Bytes", "host": "", "holders": []}, "sr0": {"virtual": 1, "links": {"ids": ["ata-VMware_Virtual_SATA_CDRW_Drive_01000000000000000001"], "uuids": ["2025-08-05-23-54-07-00"], "labels": ["Ubuntu-Server\\\\x2024.04.3\\\\x20LTS\\\\x20amd64"], "masters": []}, "vendor": "NECVMWar", "model": "VMware SATA CD01", "sas_address": null, "sas_device_handle": null, "removable": "1", "support_discard": "2048", "partitions": {}, "rotational": "1", "scheduler_mode": "mq-deadline", "sectors": "6452040", "sectorsize": "2048", "size": "3.08 GB", "host": "SATA controller: VMware SATA AHCI controller", "holders": []}, "loop2": {"virtual": 1, "links": {"ids": [], "uuids": [], "labels": [], "masters": []}, "vendor": null, "model": null, "sas_address": null, "sas_device_handle": null, "removable": "0", "support_discard": "512", "partitions": {}, "rotational": "1", "scheduler_mode": "none", "sectors": "0", "sectorsize": "512", "size": "0.00 Bytes", "host": "", "holders": []}, "loop0": {"virtual": 1, "links": {"ids": [], "uuids": [], "labels": [], "masters": []}, "vendor": null, "model": null, "sas_address": null, "sas_device_handle": null, "removable": "0", "support_discard": "4096", "partitions": {}, "rotational": "1", "scheduler_mode": "none", "sectors": "0", "sectorsize": "512", "size": "0.00 Bytes", "host": "", "holders": []}, "dm-0": {"virtual": 1, "links": {"ids": ["dm-name-ubuntu--vg-ubuntu--lv", "dm-uuid-LVM-f9VgWcPQLHhtrEu8kszKQePSXLGc4VJASpjyd8GK3Nf04xrlLoNrS1mZa6e49Lid"], "uuids": ["f6c0de64-0341-4fcb-8a76-ac1af3fd5873"], "labels": [], "masters": []}, "vendor": null, "model": null, "sas_address": null, "sas_device_handle": null, "removable": "0", "support_discard": "0", "partitions": {}, "rotational": "1", "scheduler_mode": "", "sectors": "121626624", "sectorsize": "512", "size": "58.00 GB", "host": "", "holders": []}, "loop7": {"virtual": 1, "links": {"ids": [], "uuids": [], "labels": [], "masters": []}, "vendor": null, "model": null, "sas_address": null, "sas_device_handle": null, "removable": "0", "support_discard": "512", "partitions": {}, "rotational": "1", "scheduler_mode": "none", "sectors": "0", "sectorsize": "512", "size": "0.00 Bytes", "host": "", "holders": []}, "sda": {"virtual": 1, "links": {"ids": [], "uuids": [], "labels": [], "masters": []}, "vendor": "VMware,", "model": "VMware Virtual S", "sas_address": null, "sas_device_handle": null, "removable": "0", "support_discard": "512", "partitions": {"sda2": {"links": {"ids": [], "uuids": ["9bb05da7-85f0-4338-9b8f-06ca0d639ac6"], "labels": [], "masters": []}, "start": "4096", "sectors": "4194304", "sectorsize": 512, "size": "2.00 GB", "uuid": "9bb05da7-85f0-4338-9b8f-06ca0d639ac6", "holders": []}, "sda3": {"links": {"ids": ["lvm-pv-uuid-Je2tSH-vFBU-A91D-06CE-p6FP-Ge9R-IkhbSF"], "uuids": [], "labels": [], "masters": ["dm-0"]}, "start": "4198400", "sectors": "121628672", "sectorsize": 512, "size": "58.00 GB", "uuid": null, "holders": ["ubuntu--vg-ubuntu--lv"]}, "sda1": {"links": {"ids": [], "uuids": [], "labels": [], "masters": []}, "start": "2048", "sectors": "2048", "sectorsize": 512, "size": "1.00 MB", "uuid": null, "holders": []}}, "rotational": "1", "scheduler_mode": "mq-deadline", "sectors": "125829120", "sectorsize": "512", "size": "60.00 GB", "host": "SCSI storage controller: Broadcom / LSI 53c1030 PCI-X Fusion-MPT Dual Ultra320 SCSI (rev 01)", "holders": []}, "loop5": {"virtual": 1, "links": {"ids": [], "uuids": [], "labels": [], "masters": []}, "vendor": null, "model": null, "sas_address": null, "sas_device_handle": null, "removable": "0", "support_discard": "512", "partitions": {}, "rotational": "1", "scheduler_mode": "none", "sectors": "0", "sectorsize": "512", "size": "0.00 Bytes", "host": "", "holders": []}, "loop3": {"virtual": 1, "links": {"ids": [], "uuids": [], "labels": [], "masters": []}, "vendor": null, "model": null, "sas_address": null, "sas_device_handle": null, "removable": "0", "support_discard": "512", "partitions": {}, "rotational": "1", "scheduler_mode": "none", "sectors": "0", "sectorsize": "512", "size": "0.00 Bytes", "host": "", "holders": []}}, "ansible_device_links": {"ids": {"sr0": ["ata-VMware_Virtual_SATA_CDRW_Drive_01000000000000000001"], "dm-0": ["dm-name-ubuntu--vg-ubuntu--lv", "dm-uuid-LVM-f9VgWcPQLHhtrEu8kszKQePSXLGc4VJASpjyd8GK3Nf04xrlLoNrS1mZa6e49Lid"], "sda3": ["lvm-pv-uuid-Je2tSH-vFBU-A91D-06CE-p6FP-Ge9R-IkhbSF"]}, "uuids": {"sr0": ["2025-08-05-23-54-07-00"], "dm-0": ["f6c0de64-0341-4fcb-8a76-ac1af3fd5873"], "sda2": ["9bb05da7-85f0-4338-9b8f-06ca0d639ac6"]}, "labels": {"sr0": ["Ubuntu-Server\\\\x2024.04.3\\\\x20LTS\\\\x20amd64"]}, "masters": {"sda3": ["dm-0"]}}, "ansible_uptime_seconds": 7744, "ansible_lvm": {"lvs": {"ubuntu-lv": {"size_g": "58.00", "vg": "ubuntu-vg"}}, "vgs": {"ubuntu-vg": {"size_g": "58.00", "free_g": "0", "num_lvs": "1", "num_pvs": "1"}}, "pvs": {"/dev/sda3": {"size_g": "58.00", "free_g": "0", "vg": "ubuntu-vg"}}}, "ansible_mounts": [{"mount": "/", "device": "/dev/mapper/ubuntu--vg-ubuntu--lv", "fstype": "xfs", "options": "rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota", "size_total": 62205722624, "size_available": 39221329920, "block_size": 4096, "block_total": 15186944, "block_available": 9575520, "block_used": 5611424, "inode_total": 30406656, "inode_available": 30297054, "inode_used": 109602, "uuid": "f6c0de64-0341-4fcb-8a76-ac1af3fd5873"}, {"mount": "/boot", "device": "/dev/sda2", "fstype": "ext4", "options": "rw,relatime", "size_total": 2040373248, "size_available": 1811329024, "block_size": 4096, "block_total": 498138, "block_available": 442219, "block_used": 55919, "inode_total": 131072, "inode_available": 130755, "inode_used": 317, "uuid": "9bb05da7-85f0-4338-9b8f-06ca0d639ac6"}], "ansible_dns": {"nameservers": ["127.0.0.53"], "options": {"edns0": true, "trust-ad": true}, "search": ["."]}, "ansible_interfaces": ["flannel.1", "lo", "ens34", "cali034194dbf84"], "ansible_cali034194dbf84": {"device": "cali034194dbf84", "macaddress": "ee:ee:ee:ee:ee:ee", "mtu": 1450, "active": true, "type": "ether", "speed": 10000, "promisc": false, "ipv6": [{"address": "fe80::ecee:eeff:feee:eeee", "prefix": "64", "scope": "link"}], "features": {"rx_checksumming": "on", "tx_checksumming": "on", "tx_checksum_ipv4": "off [fixed]", "tx_checksum_ip_generic": "on", "tx_checksum_ipv6": "off [fixed]", "tx_checksum_fcoe_crc": "off [fixed]", "tx_checksum_sctp": "on", "scatter_gather": "on", "tx_scatter_gather": "on", "tx_scatter_gather_fraglist": "on", "tcp_segmentation_offload": "on", "tx_tcp_segmentation": "on", "tx_tcp_ecn_segmentation": "on", "tx_tcp_mangleid_segmentation": "on", "tx_tcp6_segmentation": "on", "generic_segmentation_offload": "on", "generic_receive_offload": "off", "large_receive_offload": "off [fixed]", "rx_vlan_offload": "on", "tx_vlan_offload": "on", "ntuple_filters": "off [fixed]", "receive_hashing": "off [fixed]", "highdma": "on", "rx_vlan_filter": "off [fixed]", "vlan_challenged": "off [fixed]", "tx_lockless": "on [fixed]", "netns_local": "off [fixed]", "tx_gso_robust": "off [fixed]", "tx_fcoe_segmentation": "off [fixed]", "tx_gre_segmentation": "on", "tx_gre_csum_segmentation": "on", "tx_ipxip4_segmentation": "on", "tx_ipxip6_segmentation": "on", "tx_udp_tnl_segmentation": "on", "tx_udp_tnl_csum_segmentation": "on", "tx_gso_partial": "off [fixed]", "tx_tunnel_remcsum_segmentation": "off [fixed]", "tx_sctp_segmentation": "on", "tx_esp_segmentation": "off [fixed]", "tx_udp_segmentation": "on", "tx_gso_list": "on", "fcoe_mtu": "off [fixed]", "tx_nocache_copy": "off", "loopback": "off [fixed]", "rx_fcs": "off [fixed]", "rx_all": "off [fixed]", "tx_vlan_stag_hw_insert": "on", "rx_vlan_stag_hw_parse": "on", "rx_vlan_stag_filter": "off [fixed]", "l2_fwd_offload": "off [fixed]", "hw_tc_offload": "off [fixed]", "esp_hw_offload": "off [fixed]", "esp_tx_csum_hw_offload": "off [fixed]", "rx_udp_tunnel_port_offload": "off [fixed]", "tls_hw_tx_offload": "off [fixed]", "tls_hw_rx_offload": "off [fixed]", "rx_gro_hw": "off [fixed]", "tls_hw_record": "off [fixed]", "rx_gro_list": "off", "macsec_hw_offload": "off [fixed]", "rx_udp_gro_forwarding": "off", "hsr_tag_ins_offload": "off [fixed]", "hsr_tag_rm_offload": "off [fixed]", "hsr_fwd_offload": "off [fixed]", "hsr_dup_offload": "off [fixed]"}, "timestamping": [], "hw_timestamp_filters": []}, "ansible_ens34": {"device": "ens34", "macaddress": "00:0c:29:d7:57:ce", "mtu": 1500, "active": true, "module": "e1000", "type": "ether", "pciid": "0000:02:02.0", "speed": 1000, "promisc": false, "ipv4": {"address": "10.0.6.11", "broadcast": "10.0.6.255", "netmask": "255.255.255.0", "network": "10.0.6.0", "prefix": "24"}, "ipv6": [{"address": "fe80::20c:29ff:fed7:57ce", "prefix": "64", "scope": "link"}], "features": {"rx_checksumming": "off", "tx_checksumming": "on", "tx_checksum_ipv4": "off [fixed]", "tx_checksum_ip_generic": "on", "tx_checksum_ipv6": "off [fixed]", "tx_checksum_fcoe_crc": "off [fixed]", "tx_checksum_sctp": "off [fixed]", "scatter_gather": "on", "tx_scatter_gather": "on", "tx_scatter_gather_fraglist": "off [fixed]", "tcp_segmentation_offload": "on", "tx_tcp_segmentation": "on", "tx_tcp_ecn_segmentation": "off [fixed]", "tx_tcp_mangleid_segmentation": "off", "tx_tcp6_segmentation": "off [fixed]", "generic_segmentation_offload": "on", "generic_receive_offload": "on", "large_receive_offload": "off [fixed]", "rx_vlan_offload": "on", "tx_vlan_offload": "on [fixed]", "ntuple_filters": "off [fixed]", "receive_hashing": "off [fixed]", "highdma": "off [fixed]", "rx_vlan_filter": "on [fixed]", "vlan_challenged": "off [fixed]", "tx_lockless": "off [fixed]", "netns_local": "off [fixed]", "tx_gso_robust": "off [fixed]", "tx_fcoe_segmentation": "off [fixed]", "tx_gre_segmentation": "off [fixed]", "tx_gre_csum_segmentation": "off [fixed]", "tx_ipxip4_segmentation": "off [fixed]", "tx_ipxip6_segmentation": "off [fixed]", "tx_udp_tnl_segmentation": "off [fixed]", "tx_udp_tnl_csum_segmentation": "off [fixed]", "tx_gso_partial": "off [fixed]", "tx_tunnel_remcsum_segmentation": "off [fixed]", "tx_sctp_segmentation": "off [fixed]", "tx_esp_segmentation": "off [fixed]", "tx_udp_segmentation": "off [fixed]", "tx_gso_list": "off [fixed]", "fcoe_mtu": "off [fixed]", "tx_nocache_copy": "off", "loopback": "off [fixed]", "rx_fcs": "off", "rx_all": "off", "tx_vlan_stag_hw_insert": "off [fixed]", "rx_vlan_stag_hw_parse": "off [fixed]", "rx_vlan_stag_filter": "off [fixed]", "l2_fwd_offload": "off [fixed]", "hw_tc_offload": "off [fixed]", "esp_hw_offload": "off [fixed]", "esp_tx_csum_hw_offload": "off [fixed]", "rx_udp_tunnel_port_offload": "off [fixed]", "tls_hw_tx_offload": "off [fixed]", "tls_hw_rx_offload": "off [fixed]", "rx_gro_hw": "off [fixed]", "tls_hw_record": "off [fixed]", "rx_gro_list": "off", "macsec_hw_offload": "off [fixed]", "rx_udp_gro_forwarding": "off", "hsr_tag_ins_offload": "off [fixed]", "hsr_tag_rm_offload": "off [fixed]", "hsr_fwd_offload": "off [fixed]", "hsr_dup_offload": "off [fixed]"}, "timestamping": [], "hw_timestamp_filters": []}, "ansible_lo": {"device": "lo", "mtu": 65536, "active": true, "type": "loopback", "promisc": false, "ipv4": {"address": "127.0.0.1", "broadcast": "", "netmask": "255.0.0.0", "network": "127.0.0.0", "prefix": "8"}, "ipv6": [{"address": "::1", "prefix": "128", "scope": "host"}], "features": {"rx_checksumming": "on [fixed]", "tx_checksumming": "on", "tx_checksum_ipv4": "off [fixed]", "tx_checksum_ip_generic": "on [fixed]", "tx_checksum_ipv6": "off [fixed]", "tx_checksum_fcoe_crc": "off [fixed]", "tx_checksum_sctp": "on [fixed]", "scatter_gather": "on", "tx_scatter_gather": "on [fixed]", "tx_scatter_gather_fraglist": "on [fixed]", "tcp_segmentation_offload": "on", "tx_tcp_segmentation": "on", "tx_tcp_ecn_segmentation": "on", "tx_tcp_mangleid_segmentation": "on", "tx_tcp6_segmentation": "on", "generic_segmentation_offload": "on", "generic_receive_offload": "on", "large_receive_offload": "off [fixed]", "rx_vlan_offload": "off [fixed]", "tx_vlan_offload": "off [fixed]", "ntuple_filters": "off [fixed]", "receive_hashing": "off [fixed]", "highdma": "on [fixed]", "rx_vlan_filter": "off [fixed]", "vlan_challenged": "on [fixed]", "tx_lockless": "on [fixed]", "netns_local": "on [fixed]", "tx_gso_robust": "off [fixed]", "tx_fcoe_segmentation": "off [fixed]", "tx_gre_segmentation": "off [fixed]", "tx_gre_csum_segmentation": "off [fixed]", "tx_ipxip4_segmentation": "off [fixed]", "tx_ipxip6_segmentation": "off [fixed]", "tx_udp_tnl_segmentation": "off [fixed]", "tx_udp_tnl_csum_segmentation": "off [fixed]", "tx_gso_partial": "off [fixed]", "tx_tunnel_remcsum_segmentation": "off [fixed]", "tx_sctp_segmentation": "on", "tx_esp_segmentation": "off [fixed]", "tx_udp_segmentation": "on", "tx_gso_list": "on", "fcoe_mtu": "off [fixed]", "tx_nocache_copy": "off [fixed]", "loopback": "on [fixed]", "rx_fcs": "off [fixed]", "rx_all": "off [fixed]", "tx_vlan_stag_hw_insert": "off [fixed]", "rx_vlan_stag_hw_parse": "off [fixed]", "rx_vlan_stag_filter": "off [fixed]", "l2_fwd_offload": "off [fixed]", "hw_tc_offload": "off [fixed]", "esp_hw_offload": "off [fixed]", "esp_tx_csum_hw_offload": "off [fixed]", "rx_udp_tunnel_port_offload": "off [fixed]", "tls_hw_tx_offload": "off [fixed]", "tls_hw_rx_offload": "off [fixed]", "rx_gro_hw": "off [fixed]", "tls_hw_record": "off [fixed]", "rx_gro_list": "off", "macsec_hw_offload": "off [fixed]", "rx_udp_gro_forwarding": "off", "hsr_tag_ins_offload": "off [fixed]", "hsr_tag_rm_offload": "off [fixed]", "hsr_fwd_offload": "off [fixed]", "hsr_dup_offload": "off [fixed]"}, "timestamping": [], "hw_timestamp_filters": []}, "ansible_flannel.1": {"device": "flannel.1", "macaddress": "f2:fd:15:28:cc:c9", "mtu": 1450, "active": true, "type": "ether", "speed": 1000, "promisc": false, "ipv4": {"address": "10.42.0.0", "broadcast": "", "netmask": "255.255.255.255", "network": "10.42.0.0", "prefix": "32"}, "ipv6": [{"address": "fe80::f0fd:15ff:fe28:ccc9", "prefix": "64", "scope": "link"}], "features": {"rx_checksumming": "on", "tx_checksumming": "on", "tx_checksum_ipv4": "off [fixed]", "tx_checksum_ip_generic": "on", "tx_checksum_ipv6": "off [fixed]", "tx_checksum_fcoe_crc": "off [fixed]", "tx_checksum_sctp": "off [fixed]", "scatter_gather": "on", "tx_scatter_gather": "on", "tx_scatter_gather_fraglist": "on", "tcp_segmentation_offload": "on", "tx_tcp_segmentation": "on", "tx_tcp_ecn_segmentation": "on", "tx_tcp_mangleid_segmentation": "on", "tx_tcp6_segmentation": "on", "generic_segmentation_offload": "on", "generic_receive_offload": "on", "large_receive_offload": "off [fixed]", "rx_vlan_offload": "off [fixed]", "tx_vlan_offload": "off [fixed]", "ntuple_filters": "off [fixed]", "receive_hashing": "off [fixed]", "highdma": "off [fixed]", "rx_vlan_filter": "off [fixed]", "vlan_challenged": "off [fixed]", "tx_lockless": "on [fixed]", "netns_local": "off [fixed]", "tx_gso_robust": "off [fixed]", "tx_fcoe_segmentation": "off [fixed]", "tx_gre_segmentation": "off [fixed]", "tx_gre_csum_segmentation": "off [fixed]", "tx_ipxip4_segmentation": "off [fixed]", "tx_ipxip6_segmentation": "off [fixed]", "tx_udp_tnl_segmentation": "off [fixed]", "tx_udp_tnl_csum_segmentation": "off [fixed]", "tx_gso_partial": "off [fixed]", "tx_tunnel_remcsum_segmentation": "off [fixed]", "tx_sctp_segmentation": "on", "tx_esp_segmentation": "off [fixed]", "tx_udp_segmentation": "on", "tx_gso_list": "on", "fcoe_mtu": "off [fixed]", "tx_nocache_copy": "off", "loopback": "off [fixed]", "rx_fcs": "off [fixed]", "rx_all": "off [fixed]", "tx_vlan_stag_hw_insert": "off [fixed]", "rx_vlan_stag_hw_parse": "off [fixed]", "rx_vlan_stag_filter": "off [fixed]", "l2_fwd_offload": "off [fixed]", "hw_tc_offload": "off [fixed]", "esp_hw_offload": "off [fixed]", "esp_tx_csum_hw_offload": "off [fixed]", "rx_udp_tunnel_port_offload": "off [fixed]", "tls_hw_tx_offload": "off [fixed]", "tls_hw_rx_offload": "off [fixed]", "rx_gro_hw": "off [fixed]", "tls_hw_record": "off [fixed]", "rx_gro_list": "off", "macsec_hw_offload": "off [fixed]", "rx_udp_gro_forwarding": "off", "hsr_tag_ins_offload": "off [fixed]", "hsr_tag_rm_offload": "off [fixed]", "hsr_fwd_offload": "off [fixed]", "hsr_dup_offload": "off [fixed]"}, "timestamping": [], "hw_timestamp_filters": []}, "ansible_default_ipv4": {"gateway": "10.0.6.1", "interface": "ens34", "address": "10.0.6.11", "broadcast": "10.0.6.255", "netmask": "255.255.255.0", "network": "10.0.6.0", "prefix": "24", "macaddress": "00:0c:29:d7:57:ce", "mtu": 1500, "type": "ether", "alias": "ens34"}, "ansible_default_ipv6": {}, "ansible_all_ipv4_addresses": ["10.0.6.11", "10.42.0.0"], "ansible_all_ipv6_addresses": ["fe80::ecee:eeff:feee:eeee", "fe80::20c:29ff:fed7:57ce", "fe80::f0fd:15ff:fe28:ccc9"], "ansible_locally_reachable_ips": {"ipv4": ["10.0.6.11", "10.42.0.0", "127.0.0.0/8", "127.0.0.1"], "ipv6": ["::1", "fe80::20c:29ff:fed7:57ce", "fe80::ecee:eeff:feee:eeee", "fe80::f0fd:15ff:fe28:ccc9"]}, "ansible_hostnqn": "", "ansible_service_mgr": "systemd", "gather_subset": ["all"], "module_setup": true}, "invocation": {"module_args": {"gather_subset": ["all"], "gather_timeout": 10, "filter": [], "fact_path": "/etc/ansible/facts.d"}}}\n', b'')
2025-10-25 17:43:46,912 p=55263 u=root n=ansible | TASK [Gathering Facts] ***************************************************************
2025-10-25 17:43:46,913 p=55263 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:43:47,031 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 17:43:47,032 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: The arg spec validation passed
  validate_args_context:
    argument_spec_name: main
    name: rke2-1.49.0
    path: /opt/git/ansible/roles/rke2-1.49.0
    type: role
2025-10-25 17:43:47,387 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/hostname.py
2025-10-25 17:43:47,388 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:43:47,388 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:47,388 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=yheodbbiqozzgvsuyxxqdnlkkxvzehny] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-yheodbbiqozzgvsuyxxqdnlkkxvzehny ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:43:47,431 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:43:49,438 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "name": "k8s-m1", "ansible_facts": {"ansible_hostname": "k8s-m1", "ansible_nodename": "k8s-m1", "ansible_fqdn": "k8s-m1", "ansible_domain": ""}, "invocation": {"module_args": {"name": "k8s-m1", "use": null}}}\n', b'')
2025-10-25 17:43:49,447 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] ****************************
2025-10-25 17:43:49,448 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  ansible_facts:
    ansible_domain: ''
    ansible_fqdn: k8s-m1
    ansible_hostname: k8s-m1
    ansible_nodename: k8s-m1
  invocation:
    module_args:
      name: k8s-m1
      use: null
  name: k8s-m1
2025-10-25 17:43:49,561 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] ************************
2025-10-25 17:43:49,603 p=55263 u=root n=ansible | redirecting (type: modules) ansible.builtin.firewalld to ansible.posix.firewalld
2025-10-25 17:43:49,607 p=55263 u=root n=ansible | redirecting (type: modules) ansible.builtin.firewalld to ansible.posix.firewalld
2025-10-25 17:43:49,608 p=55263 u=root n=ansible | redirecting (type: modules) ansible.builtin.firewalld to ansible.posix.firewalld
2025-10-25 17:43:49,634 p=55263 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1
2025-10-25 17:43:49,675 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:49,676 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'echo ~ubuntu && sleep 0'"'"''
2025-10-25 17:43:49,697 p=55263 u=root n=ansible | <10.0.6.11> (0, b'/home/ubuntu\n', b'')
2025-10-25 17:43:49,697 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:49,697 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'( umask 77 && mkdir -p "` echo /home/ubuntu/.ansible/tmp `"&& mkdir "` echo /home/ubuntu/.ansible/tmp/ansible-tmp-1761389029.6969893-55517-102907284854451 `" && echo ansible-tmp-1761389029.6969893-55517-102907284854451="` echo /home/ubuntu/.ansible/tmp/ansible-tmp-1761389029.6969893-55517-102907284854451 `" ) && sleep 0'"'"''
2025-10-25 17:43:49,725 p=55263 u=root n=ansible | <10.0.6.11> (0, b'ansible-tmp-1761389029.6969893-55517-102907284854451=/home/ubuntu/.ansible/tmp/ansible-tmp-1761389029.6969893-55517-102907284854451\n', b'')
2025-10-25 17:43:49,871 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/stat.py
2025-10-25 17:43:49,871 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:43:49,872 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:49,873 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=kkviwogoapqrbnytszzjsojuvhstjefw] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-kkviwogoapqrbnytszzjsojuvhstjefw ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:43:49,919 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:43:50,257 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "stat": {"exists": true, "path": "/etc/modules-load.d/k8s.conf", "mode": "0644", "isdir": false, "ischr": false, "isblk": false, "isreg": true, "isfifo": false, "islnk": false, "issock": false, "uid": 0, "gid": 0, "size": 21, "inode": 67513941, "dev": 64512, "nlink": 1, "atime": 1761387922.0330312, "mtime": 1761387921.5930312, "ctime": 1761387922.0330312, "wusr": true, "rusr": true, "xusr": false, "wgrp": false, "rgrp": true, "xgrp": false, "woth": false, "roth": true, "xoth": false, "isuid": false, "isgid": false, "blocks": 8, "block_size": 4096, "device_type": 0, "readable": true, "writeable": true, "executable": false, "pw_name": "root", "gr_name": "root", "checksum": "4e930f0397e1b134b08ec046c1abde0ed190bb08", "mimetype": "text/plain", "charset": "us-ascii", "version": "3797181603", "attributes": [], "attr_flags": ""}, "invocation": {"module_args": {"path": "/etc/modules-load.d/k8s.conf", "follow": false, "get_checksum": true, "checksum_algorithm": "sha1", "get_mime": true, "get_attributes": true}}}\n', b'')
2025-10-25 17:43:50,409 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/file.py
2025-10-25 17:43:50,410 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:43:50,410 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:50,410 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=wzhfzggnyltxudkdyayessoacoexoiyq] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-wzhfzggnyltxudkdyayessoacoexoiyq ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:43:50,458 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:43:50,741 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"path": "/etc/modules-load.d/k8s.conf", "changed": false, "diff": {"before": {"path": "/etc/modules-load.d/k8s.conf"}, "after": {"path": "/etc/modules-load.d/k8s.conf"}}, "uid": 0, "gid": 0, "owner": "root", "group": "root", "mode": "0644", "state": "file", "size": 21, "invocation": {"module_args": {"mode": "0644", "dest": "/etc/modules-load.d/k8s.conf", "_original_basename": "tmpzxihyamy", "recurse": false, "state": "file", "path": "/etc/modules-load.d/k8s.conf", "force": false, "follow": true, "modification_time_format": "%Y%m%d%H%M.%S", "access_time_format": "%Y%m%d%H%M.%S", "unsafe_writes": false, "_diff_peek": null, "src": null, "modification_time": null, "access_time": null, "owner": null, "group": null, "seuser": null, "serole": null, "selevel": null, "setype": null, "attributes": null}}}\n', b'')
2025-10-25 17:43:50,742 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:50,743 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'rm -f -r /home/ubuntu/.ansible/tmp/ansible-tmp-1761389029.6969893-55517-102907284854451/ > /dev/null 2>&1 && sleep 0'"'"''
2025-10-25 17:43:50,769 p=55263 u=root n=ansible | <10.0.6.11> (0, b'', b'')
2025-10-25 17:43:50,772 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Configure kernel modules] ****************************************
2025-10-25 17:43:50,773 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  checksum: 4e930f0397e1b134b08ec046c1abde0ed190bb08
  dest: /etc/modules-load.d/k8s.conf
  diff:
    after:
      path: /etc/modules-load.d/k8s.conf
    before:
      path: /etc/modules-load.d/k8s.conf
  gid: 0
  group: root
  invocation:
    module_args:
      _diff_peek: null
      _original_basename: tmpzxihyamy
      access_time: null
      access_time_format: '%Y%m%d%H%M.%S'
      attributes: null
      dest: /etc/modules-load.d/k8s.conf
      follow: true
      force: false
      group: null
      mode: '0644'
      modification_time: null
      modification_time_format: '%Y%m%d%H%M.%S'
      owner: null
      path: /etc/modules-load.d/k8s.conf
      recurse: false
      selevel: null
      serole: null
      setype: null
      seuser: null
      src: null
      state: file
      unsafe_writes: false
  mode: '0644'
  owner: root
  path: /etc/modules-load.d/k8s.conf
  size: 21
  state: file
  uid: 0
2025-10-25 17:43:50,809 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/stat.py
2025-10-25 17:43:50,810 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:43:50,810 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:50,811 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=nownxbriyfnanicyxzpcrvbdvltnoidu] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-nownxbriyfnanicyxzpcrvbdvltnoidu ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:43:50,855 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:43:51,138 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "stat": {"exists": false}, "invocation": {"module_args": {"path": "/etc/NetworkManager", "follow": false, "get_checksum": true, "get_mime": true, "get_attributes": true, "checksum_algorithm": "sha1"}}}\n', b'')
2025-10-25 17:43:51,141 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Detect NetworkManager] *******************************************
2025-10-25 17:43:51,142 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  invocation:
    module_args:
      checksum_algorithm: sha1
      follow: false
      get_attributes: true
      get_checksum: true
      get_mime: true
      path: /etc/NetworkManager
  stat:
    exists: false
2025-10-25 17:43:51,247 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:51,248 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'echo ~ubuntu && sleep 0'"'"''
2025-10-25 17:43:51,273 p=55263 u=root n=ansible | <10.0.6.11> (0, b'/home/ubuntu\n', b'')
2025-10-25 17:43:51,274 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:51,275 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'( umask 77 && mkdir -p "` echo /home/ubuntu/.ansible/tmp `"&& mkdir "` echo /home/ubuntu/.ansible/tmp/ansible-tmp-1761389031.2735758-55562-62434576348945 `" && echo ansible-tmp-1761389031.2735758-55562-62434576348945="` echo /home/ubuntu/.ansible/tmp/ansible-tmp-1761389031.2735758-55562-62434576348945 `" ) && sleep 0'"'"''
2025-10-25 17:43:51,303 p=55263 u=root n=ansible | <10.0.6.11> (0, b'ansible-tmp-1761389031.2735758-55562-62434576348945=/home/ubuntu/.ansible/tmp/ansible-tmp-1761389031.2735758-55562-62434576348945\n', b'')
2025-10-25 17:43:51,306 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/stat.py
2025-10-25 17:43:51,306 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:43:51,307 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:51,308 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=nrsyduczbeqgfxkafjwvsgqtatgzwzrf] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-nrsyduczbeqgfxkafjwvsgqtatgzwzrf ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:43:51,353 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:43:51,623 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "stat": {"exists": true, "path": "/etc/sysctl.d/90-rke2.conf", "mode": "0644", "isdir": false, "ischr": false, "isblk": false, "isreg": true, "isfifo": false, "islnk": false, "issock": false, "uid": 0, "gid": 0, "size": 800, "inode": 67513947, "dev": 64512, "nlink": 1, "atime": 1761387924.3810313, "mtime": 1761387923.2960312, "ctime": 1761387923.6030312, "wusr": true, "rusr": true, "xusr": false, "wgrp": false, "rgrp": true, "xgrp": false, "woth": false, "roth": true, "xoth": false, "isuid": false, "isgid": false, "blocks": 8, "block_size": 4096, "device_type": 0, "readable": true, "writeable": true, "executable": false, "pw_name": "root", "gr_name": "root", "checksum": "b9fd357c8cb9f32c381f0c384eb88b50b25a9bf4", "mimetype": "text/plain", "charset": "us-ascii", "version": "3712808766", "attributes": [], "attr_flags": ""}, "invocation": {"module_args": {"path": "/etc/sysctl.d/90-rke2.conf", "follow": false, "get_checksum": true, "checksum_algorithm": "sha1", "get_mime": true, "get_attributes": true}}}\n', b'')
2025-10-25 17:43:51,624 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/file.py
2025-10-25 17:43:51,625 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:43:51,625 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:51,625 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=sdscypzaznseziqkdbyhwkguxpqfjwqr] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-sdscypzaznseziqkdbyhwkguxpqfjwqr ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:43:51,662 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:43:51,972 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"path": "/etc/sysctl.d/90-rke2.conf", "changed": false, "diff": {"before": {"path": "/etc/sysctl.d/90-rke2.conf"}, "after": {"path": "/etc/sysctl.d/90-rke2.conf"}}, "uid": 0, "gid": 0, "owner": "root", "group": "root", "mode": "0644", "state": "file", "size": 800, "invocation": {"module_args": {"mode": "0644", "dest": "/etc/sysctl.d/90-rke2.conf", "_original_basename": "tmp_g54pmde", "recurse": false, "state": "file", "path": "/etc/sysctl.d/90-rke2.conf", "force": false, "follow": true, "modification_time_format": "%Y%m%d%H%M.%S", "access_time_format": "%Y%m%d%H%M.%S", "unsafe_writes": false, "_diff_peek": null, "src": null, "modification_time": null, "access_time": null, "owner": null, "group": null, "seuser": null, "serole": null, "selevel": null, "setype": null, "attributes": null}}}\n', b'')
2025-10-25 17:43:51,973 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:51,974 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'rm -f -r /home/ubuntu/.ansible/tmp/ansible-tmp-1761389031.2735758-55562-62434576348945/ > /dev/null 2>&1 && sleep 0'"'"''
2025-10-25 17:43:52,001 p=55263 u=root n=ansible | <10.0.6.11> (0, b'', b'')
2025-10-25 17:43:52,004 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Configure sysctl parameters] *************************************
2025-10-25 17:43:52,006 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  checksum: b9fd357c8cb9f32c381f0c384eb88b50b25a9bf4
  dest: /etc/sysctl.d/90-rke2.conf
  diff:
    after:
      path: /etc/sysctl.d/90-rke2.conf
    before:
      path: /etc/sysctl.d/90-rke2.conf
  gid: 0
  group: root
  invocation:
    module_args:
      _diff_peek: null
      _original_basename: tmp_g54pmde
      access_time: null
      access_time_format: '%Y%m%d%H%M.%S'
      attributes: null
      dest: /etc/sysctl.d/90-rke2.conf
      follow: true
      force: false
      group: null
      mode: '0644'
      modification_time: null
      modification_time_format: '%Y%m%d%H%M.%S'
      owner: null
      path: /etc/sysctl.d/90-rke2.conf
      recurse: false
      selevel: null
      serole: null
      setype: null
      seuser: null
      src: null
      state: file
      unsafe_writes: false
  mode: '0644'
  owner: root
  path: /etc/sysctl.d/90-rke2.conf
  size: 800
  state: file
  uid: 0
2025-10-25 17:43:52,224 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:43:52,224 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:43:52,224 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:52,225 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=gasyrujrkvpqgqlirmxlrjaxhetvosqp] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-gasyrujrkvpqgqlirmxlrjaxhetvosqp ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:43:52,283 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:43:52,670 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "* Applying /usr/lib/sysctl.d/10-apparmor.conf ...\\n* Applying /etc/sysctl.d/10-bufferbloat.conf ...\\n* Applying /etc/sysctl.d/10-console-messages.conf ...\\n* Applying /etc/sysctl.d/10-ipv6-privacy.conf ...\\n* Applying /etc/sysctl.d/10-kernel-hardening.conf ...\\n* Applying /etc/sysctl.d/10-magic-sysrq.conf ...\\n* Applying /etc/sysctl.d/10-map-count.conf ...\\n* Applying /etc/sysctl.d/10-network-security.conf ...\\n* Applying /etc/sysctl.d/10-ptrace.conf ...\\n* Applying /etc/sysctl.d/10-zeropage.conf ...\\n* Applying /usr/lib/sysctl.d/50-pid-max.conf ...\\n* Applying /etc/sysctl.d/90-rke2.conf ...\\n* Applying /usr/lib/sysctl.d/99-protect-links.conf ...\\n* Applying /etc/sysctl.d/99-sysctl.conf ...\\n* Applying /etc/sysctl.conf ...\\nkernel.apparmor_restrict_unprivileged_userns = 1\\nnet.core.default_qdisc = fq_codel\\nkernel.printk = 4 4 1 7\\nnet.ipv6.conf.all.use_tempaddr = 2\\nnet.ipv6.conf.default.use_tempaddr = 2\\nkernel.kptr_restrict = 1\\nkernel.sysrq = 176\\nvm.max_map_count = 1048576\\nnet.ipv4.conf.default.rp_filter = 2\\nnet.ipv4.conf.all.rp_filter = 2\\nkernel.yama.ptrace_scope = 1\\nvm.mmap_min_addr = 65536\\nkernel.pid_max = 4194304\\nnet.ipv4.ip_forward = 1\\nnet.ipv4.conf.all.send_redirects = 0\\nnet.ipv4.conf.default.send_redirects = 0\\nnet.ipv4.conf.default.accept_source_route = 0\\nnet.ipv4.conf.all.accept_redirects = 0\\nnet.ipv4.conf.default.accept_redirects = 0\\nnet.ipv4.conf.all.log_martians = 1\\nnet.ipv4.conf.default.log_martians = 1\\nnet.ipv4.conf.all.rp_filter = 0\\nnet.ipv4.conf.default.rp_filter = 0\\nnet.ipv6.conf.all.accept_ra = 0\\nnet.ipv6.conf.default.accept_ra = 0\\nnet.ipv6.conf.all.accept_redirects = 0\\nnet.ipv6.conf.default.accept_redirects = 0\\nkernel.keys.root_maxbytes = 25000000\\nkernel.keys.root_maxkeys = 1000000\\nkernel.panic = 10\\nkernel.panic_on_oops = 1\\nvm.overcommit_memory = 1\\nvm.panic_on_oom = 0\\nnet.ipv4.ip_local_reserved_ports = 30000-32767\\nnet.bridge.bridge-nf-call-iptables = 1\\nnet.bridge.bridge-nf-call-arptables = 1\\nnet.bridge.bridge-nf-call-ip6tables = 1\\nfs.protected_fifos = 1\\nfs.protected_hardlinks = 1\\nfs.protected_regular = 2\\nfs.protected_symlinks = 1", "stderr": "", "rc": 0, "cmd": "modprobe br_netfilter\\nmodprobe overlay\\nsysctl --system\\n", "start": "2025-10-25 10:43:52.786844", "end": "2025-10-25 10:43:52.886889", "delta": "0:00:00.100045", "msg": "", "invocation": {"module_args": {"_raw_params": "modprobe br_netfilter\\nmodprobe overlay\\nsysctl --system\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "executable": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:43:52,673 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Apply system configurations] *************************************
2025-10-25 17:43:52,676 p=55263 u=root n=ansible | changed: [k8s-m1] => changed=true 
  cmd: |-
    modprobe br_netfilter
    modprobe overlay
    sysctl --system
  delta: '0:00:00.100045'
  end: '2025-10-25 10:43:52.886889'
  invocation:
    module_args:
      _raw_params: |-
        modprobe br_netfilter
        modprobe overlay
        sysctl --system
      _uses_shell: true
      argv: null
      chdir: null
      creates: null
      executable: null
      expand_argument_vars: true
      removes: null
      stdin: null
      stdin_add_newline: true
      strip_empty_ends: true
  msg: ''
  rc: 0
  start: '2025-10-25 10:43:52.786844'
  stderr: ''
  stderr_lines: <omitted>
  stdout: |-
    * Applying /usr/lib/sysctl.d/10-apparmor.conf ...
    * Applying /etc/sysctl.d/10-bufferbloat.conf ...
    * Applying /etc/sysctl.d/10-console-messages.conf ...
    * Applying /etc/sysctl.d/10-ipv6-privacy.conf ...
    * Applying /etc/sysctl.d/10-kernel-hardening.conf ...
    * Applying /etc/sysctl.d/10-magic-sysrq.conf ...
    * Applying /etc/sysctl.d/10-map-count.conf ...
    * Applying /etc/sysctl.d/10-network-security.conf ...
    * Applying /etc/sysctl.d/10-ptrace.conf ...
    * Applying /etc/sysctl.d/10-zeropage.conf ...
    * Applying /usr/lib/sysctl.d/50-pid-max.conf ...
    * Applying /etc/sysctl.d/90-rke2.conf ...
    * Applying /usr/lib/sysctl.d/99-protect-links.conf ...
    * Applying /etc/sysctl.d/99-sysctl.conf ...
    * Applying /etc/sysctl.conf ...
    kernel.apparmor_restrict_unprivileged_userns = 1
    net.core.default_qdisc = fq_codel
    kernel.printk = 4 4 1 7
    net.ipv6.conf.all.use_tempaddr = 2
    net.ipv6.conf.default.use_tempaddr = 2
    kernel.kptr_restrict = 1
    kernel.sysrq = 176
    vm.max_map_count = 1048576
    net.ipv4.conf.default.rp_filter = 2
    net.ipv4.conf.all.rp_filter = 2
    kernel.yama.ptrace_scope = 1
    vm.mmap_min_addr = 65536
    kernel.pid_max = 4194304
    net.ipv4.ip_forward = 1
    net.ipv4.conf.all.send_redirects = 0
    net.ipv4.conf.default.send_redirects = 0
    net.ipv4.conf.default.accept_source_route = 0
    net.ipv4.conf.all.accept_redirects = 0
    net.ipv4.conf.default.accept_redirects = 0
    net.ipv4.conf.all.log_martians = 1
    net.ipv4.conf.default.log_martians = 1
    net.ipv4.conf.all.rp_filter = 0
    net.ipv4.conf.default.rp_filter = 0
    net.ipv6.conf.all.accept_ra = 0
    net.ipv6.conf.default.accept_ra = 0
    net.ipv6.conf.all.accept_redirects = 0
    net.ipv6.conf.default.accept_redirects = 0
    kernel.keys.root_maxbytes = 25000000
    kernel.keys.root_maxkeys = 1000000
    kernel.panic = 10
    kernel.panic_on_oops = 1
    vm.overcommit_memory = 1
    vm.panic_on_oom = 0
    net.ipv4.ip_local_reserved_ports = 30000-32767
    net.bridge.bridge-nf-call-iptables = 1
    net.bridge.bridge-nf-call-arptables = 1
    net.bridge.bridge-nf-call-ip6tables = 1
    fs.protected_fifos = 1
    fs.protected_hardlinks = 1
    fs.protected_regular = 2
    fs.protected_symlinks = 1
  stdout_lines: <omitted>
2025-10-25 17:43:53,399 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/apt.py
2025-10-25 17:43:53,400 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:43:53,401 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:53,402 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=frbkdluoeoibeokqhnhjjqeporekuhos] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-frbkdluoeoibeokqhnhjjqeporekuhos ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:43:53,449 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:43:55,174 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "cache_updated": false, "cache_update_time": 1761299972, "invocation": {"module_args": {"name": "ufw", "state": "present", "package": ["ufw"], "update_cache_retries": 5, "update_cache_retry_max_delay": 12, "cache_valid_time": 0, "purge": false, "force": false, "upgrade": null, "dpkg_options": "force-confdef,force-confold", "autoremove": false, "autoclean": false, "fail_on_autoremove": false, "only_upgrade": false, "force_apt_get": false, "clean": false, "allow_unauthenticated": false, "allow_downgrade": false, "allow_change_held_packages": false, "lock_timeout": 60, "update_cache": null, "deb": null, "default_release": null, "install_recommends": null, "policy_rc_d": null}}}\n', b'')
2025-10-25 17:43:55,178 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Ensure UFW is installed] *****************************************
2025-10-25 17:43:55,179 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  cache_update_time: 1761299972
  cache_updated: false
  invocation:
    module_args:
      allow_change_held_packages: false
      allow_downgrade: false
      allow_unauthenticated: false
      autoclean: false
      autoremove: false
      cache_valid_time: 0
      clean: false
      deb: null
      default_release: null
      dpkg_options: force-confdef,force-confold
      fail_on_autoremove: false
      force: false
      force_apt_get: false
      install_recommends: null
      lock_timeout: 60
      name: ufw
      only_upgrade: false
      package:
      - ufw
      policy_rc_d: null
      purge: false
      state: present
      update_cache: null
      update_cache_retries: 5
      update_cache_retry_max_delay: 12
      upgrade: null
2025-10-25 17:43:55,353 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:43:55,354 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:43:55,355 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:55,355 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=bzzlrdpqorabfcimjhujcraqjjfsmyzj] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-bzzlrdpqorabfcimjhujcraqjjfsmyzj ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:43:55,400 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:43:56,585 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw -f enable", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"state": "enabled", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "default": null, "logging": null, "direction": null, "insert": null, "rule": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "to_port": null, "proto": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:43:56,587 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Enable UFW] ******************************************************
2025-10-25 17:43:56,589 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw -f enable
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      proto: null
      route: false
      rule: null
      state: enabled
      to_ip: any
      to_port: null
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:43:56,631 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:43:56,632 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:43:56,632 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:56,633 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=biwfdnwjzxviryqmpcoqtqrykbvwqujr] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-biwfdnwjzxviryqmpcoqtqrykbvwqujr ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:43:56,682 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:43:59,533 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw default deny incoming", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"direction": "incoming", "policy": "deny", "default": "deny", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "logging": null, "insert": null, "rule": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "to_port": null, "proto": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:43:59,536 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Set default incoming policy to deny] *****************************
2025-10-25 17:43:59,538 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw default deny incoming
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: deny
      delete: false
      direction: incoming
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      policy: deny
      proto: null
      route: false
      rule: null
      state: null
      to_ip: any
      to_port: null
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:43:59,585 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:43:59,585 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:43:59,586 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:43:59,586 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=fixdjvcxkypnzoezznlvuucpusksqdry] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-fixdjvcxkypnzoezznlvuucpusksqdry ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:43:59,626 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:02,393 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw default allow outgoing", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"direction": "outgoing", "policy": "allow", "default": "allow", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "logging": null, "insert": null, "rule": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "to_port": null, "proto": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:02,397 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Set default outgoing policy to allow] ****************************
2025-10-25 17:44:02,400 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw default allow outgoing
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: allow
      delete: false
      direction: outgoing
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      policy: allow
      proto: null
      route: false
      rule: null
      state: null
      to_ip: any
      to_port: null
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:02,447 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:02,447 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:02,448 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:02,448 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=qwtfkvzedbspawfsdsnehypdpxlbipbd] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-qwtfkvzedbspawfsdsnehypdpxlbipbd ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:02,486 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:03,346 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 22 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "22", "proto": "tcp", "to_port": "22", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:03,351 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Allow required ports] ********************************************
2025-10-25 17:44:03,356 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=22/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 22 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '22'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '22'
  item: 22/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:03,361 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:03,361 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:03,361 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:03,362 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=pmimczioaxkixrmkfextpgbgzhjecmyw] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-pmimczioaxkixrmkfextpgbgzhjecmyw ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:03,401 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:04,271 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 179 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "179", "proto": "tcp", "to_port": "179", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:04,275 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=179/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 179 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '179'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '179'
  item: 179/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:04,284 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:04,285 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:04,285 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:04,285 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=padibzuowgyahbrntvguvsdiofmvxlgz] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-padibzuowgyahbrntvguvsdiofmvxlgz ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:04,326 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:05,186 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 9345 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "9345", "proto": "tcp", "to_port": "9345", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:05,192 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=9345/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 9345 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '9345'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '9345'
  item: 9345/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:05,199 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:05,200 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:05,200 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:05,200 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=nlyxhptttnqxhnehrvmrliahppvghlwr] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-nlyxhptttnqxhnehrvmrliahppvghlwr ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:05,240 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:06,137 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 6443 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "6443", "proto": "tcp", "to_port": "6443", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:06,144 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=6443/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 6443 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '6443'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '6443'
  item: 6443/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:06,153 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:06,154 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:06,154 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:06,154 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=meeqferxvrojpngzckphlzhxmiocoxlv] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-meeqferxvrojpngzckphlzhxmiocoxlv ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:06,193 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:07,067 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 10250 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "10250", "proto": "tcp", "to_port": "10250", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:07,071 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=10250/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 10250 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '10250'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '10250'
  item: 10250/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:07,085 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:07,085 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:07,085 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:07,085 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=wfyhvtxdhxctrlumxsfasxaihvyknxub] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-wfyhvtxdhxctrlumxsfasxaihvyknxub ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:07,132 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:07,981 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 2379 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "2379", "proto": "tcp", "to_port": "2379", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:07,984 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=2379/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 2379 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '2379'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '2379'
  item: 2379/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:07,994 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:07,994 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:07,994 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:07,994 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=lidosbmzuictmasymcwvboedstdvzrey] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-lidosbmzuictmasymcwvboedstdvzrey ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:08,039 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:08,901 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 2380 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "2380", "proto": "tcp", "to_port": "2380", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:08,905 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=2380/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 2380 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '2380'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '2380'
  item: 2380/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:08,915 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:08,916 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:08,916 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:08,916 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=bwhfqtvmgngftsnhuicbjwivontftvlq] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-bwhfqtvmgngftsnhuicbjwivontftvlq ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:08,955 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:09,795 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 2381 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "2381", "proto": "tcp", "to_port": "2381", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:09,801 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=2381/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 2381 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '2381'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '2381'
  item: 2381/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:09,809 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:09,809 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:09,809 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:09,809 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=meyveyvwnapgvxsbbvisgjyjdjqonehl] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-meyveyvwnapgvxsbbvisgjyjdjqonehl ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:09,847 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:10,688 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 30000:32767 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "30000:32767", "proto": "tcp", "to_port": "30000:32767", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:10,691 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=30000:32767/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 30000:32767 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: 30000:32767
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: 30000:32767
  item: 30000:32767/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:10,701 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:10,701 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:10,701 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:10,701 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=kackxxuchtludksnnrluemmeslxlnufk] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-kackxxuchtludksnnrluemmeslxlnufk ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:10,747 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:11,580 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 8472 proto udp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "8472", "proto": "udp", "to_port": "8472", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:11,584 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=8472/udp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 8472 proto udp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '8472'
      proto: udp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '8472'
  item: 8472/udp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:11,593 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:11,593 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:11,593 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:11,594 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=ujotgxirkfylmvgkuqyhqqguzvcvhqng] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-ujotgxirkfylmvgkuqyhqqguzvcvhqng ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:11,634 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:13,516 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 6081 proto udp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "6081", "proto": "udp", "to_port": "6081", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:13,520 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=6081/udp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 6081 proto udp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '6081'
      proto: udp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '6081'
  item: 6081/udp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:13,530 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:13,530 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:13,531 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:13,532 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=fiayctdriaotrsgfyhyvnhfobrzhxpkl] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-fiayctdriaotrsgfyhyvnhfobrzhxpkl ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:13,572 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:14,405 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 4240 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "4240", "proto": "tcp", "to_port": "4240", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:14,410 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=4240/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 4240 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '4240'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '4240'
  item: 4240/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:14,429 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:14,430 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:14,430 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:14,431 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=tcsblrpozdvmgogqdaltqbawfwpeczuf] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-tcsblrpozdvmgogqdaltqbawfwpeczuf ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:14,478 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:15,316 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 4244 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "4244", "proto": "tcp", "to_port": "4244", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:15,320 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=4244/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 4244 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '4244'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '4244'
  item: 4244/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:15,335 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:15,335 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:15,336 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:15,336 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=ieyerebzdxvyjtyudlcojpjcsazbbhwm] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-ieyerebzdxvyjtyudlcojpjcsazbbhwm ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:15,379 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:16,201 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 4245 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "4245", "proto": "tcp", "to_port": "4245", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:16,205 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=4245/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 4245 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '4245'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '4245'
  item: 4245/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:16,216 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:16,216 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:16,216 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:16,217 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=qrvoxgrhvdfhjfgvvdicrxnnbwqikqqy] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-qrvoxgrhvdfhjfgvvdicrxnnbwqikqqy ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:16,260 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:17,129 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 4222 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "4222", "proto": "tcp", "to_port": "4222", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:17,132 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=4222/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 4222 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '4222'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '4222'
  item: 4222/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:17,141 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:17,141 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:17,141 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:17,141 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=lhmgdepovasccjgfrmkkjnajraxafclk] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-lhmgdepovasccjgfrmkkjnajraxafclk ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:17,195 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:18,093 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 9966 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "9966", "proto": "tcp", "to_port": "9966", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:18,097 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=9966/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 9966 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '9966'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '9966'
  item: 9966/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:18,108 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:18,109 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:18,109 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:18,109 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=lodxnmvxaxjcrimtzmamevvpmqnjtdzb] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-lodxnmvxaxjcrimtzmamevvpmqnjtdzb ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:18,150 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:19,075 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 4250:4251 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "4250:4251", "proto": "tcp", "to_port": "4250:4251", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:19,081 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=4250:4251/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 4250:4251 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: 4250:4251
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: 4250:4251
  item: 4250:4251/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:19,094 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:19,094 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:19,094 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:19,094 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=klvnqgzjlwrpuaejkgsunsugamjcdoaf] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-klvnqgzjlwrpuaejkgsunsugamjcdoaf ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:19,157 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:20,074 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 6060:6062 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "6060:6062", "proto": "tcp", "to_port": "6060:6062", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:20,079 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=6060:6062/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 6060:6062 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: 6060:6062
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: 6060:6062
  item: 6060:6062/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:20,088 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:20,088 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:20,088 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:20,089 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=xffkqnqvtedbivbolpxngaoqfnlsvjle] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-xffkqnqvtedbivbolpxngaoqfnlsvjle ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:20,132 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:21,004 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 9878:9879 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "9878:9879", "proto": "tcp", "to_port": "9878:9879", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:21,010 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=9878:9879/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 9878:9879 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: 9878:9879
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: 9878:9879
  item: 9878:9879/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:21,024 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:21,025 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:21,025 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:21,025 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=wmfcadxcfgqcdcjpgltkboexzxpbsxew] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-wmfcadxcfgqcdcjpgltkboexzxpbsxew ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:21,074 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:21,984 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 9890:9893 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "9890:9893", "proto": "tcp", "to_port": "9890:9893", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:21,989 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=9890:9893/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 9890:9893 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: 9890:9893
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: 9890:9893
  item: 9890:9893/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:21,999 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:22,000 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:22,000 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:22,001 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=voxabrwrrqlounbfmelllfbkbpurpvhl] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-voxabrwrrqlounbfmelllfbkbpurpvhl ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:22,043 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:22,912 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 9901 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "9901", "proto": "tcp", "to_port": "9901", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:22,917 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=9901/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 9901 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '9901'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '9901'
  item: 9901/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:22,927 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:22,927 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:22,928 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:22,928 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=tipbswfvgnubxskydxdgzoycsbatxckx] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-tipbswfvgnubxskydxdgzoycsbatxckx ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:22,968 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:23,848 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 9962:9964 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "9962:9964", "proto": "tcp", "to_port": "9962:9964", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:23,852 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=9962:9964/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 9962:9964 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: 9962:9964
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: 9962:9964
  item: 9962:9964/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:23,864 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:23,864 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:23,864 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:23,865 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=rucaqmopmkbmvfchiuoylloludpviudu] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-rucaqmopmkbmvfchiuoylloludpviudu ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:23,906 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:24,800 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 51871 proto udp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "51871", "proto": "udp", "to_port": "51871", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:24,804 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=51871/udp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 51871 proto udp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '51871'
      proto: udp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '51871'
  item: 51871/udp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:24,816 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:24,816 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:24,816 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:24,817 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=btvfpxoyqofpkfsedrprvashjpvmcfhk] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-btvfpxoyqofpkfsedrprvashjpvmcfhk ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:24,867 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:25,837 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 4567 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "4567", "proto": "tcp", "to_port": "4567", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:25,841 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=4567/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 4567 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '4567'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '4567'
  item: 4567/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:25,853 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:25,853 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:25,853 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:25,853 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=rulabcbvhylyevhzfoehzstynnjugjqm] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-rulabcbvhylyevhzfoehzstynnjugjqm ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:25,901 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:26,730 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 4567 proto udp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "4567", "proto": "udp", "to_port": "4567", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:26,736 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=4567/udp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 4567 proto udp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '4567'
      proto: udp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '4567'
  item: 4567/udp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:26,745 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:26,745 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:26,745 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:26,746 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=ufgkxotbbxkmxceewtjivdvmgeytmfla] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-ufgkxotbbxkmxceewtjivdvmgeytmfla ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:26,785 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:27,610 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 4568 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "4568", "proto": "tcp", "to_port": "4568", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:27,614 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=4568/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 4568 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '4568'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '4568'
  item: 4568/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:27,625 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:27,625 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:27,625 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:27,625 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=bobunwxqhnqjzntwbcexqelypekzdxsu] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-bobunwxqhnqjzntwbcexqelypekzdxsu ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:27,668 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:28,498 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 4444 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "4444", "proto": "tcp", "to_port": "4444", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:28,501 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=4444/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 4444 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '4444'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '4444'
  item: 4444/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:28,512 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:28,512 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:28,512 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:28,512 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=sdvjicfntokjomoccdqocbawcyeknqqm] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-sdvjicfntokjomoccdqocbawcyeknqqm ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:28,573 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:29,442 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 7443 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "7443", "proto": "tcp", "to_port": "7443", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:29,448 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=7443/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 7443 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '7443'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '7443'
  item: 7443/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:29,456 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:29,457 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:29,457 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:29,458 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=xfsceffcpbttxmsaledeesrkqutohedu] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-xfsceffcpbttxmsaledeesrkqutohedu ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:29,522 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:30,486 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 7345 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "7345", "proto": "tcp", "to_port": "7345", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:30,491 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=7345/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 7345 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '7345'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '7345'
  item: 7345/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:30,501 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:30,502 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:30,502 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:30,502 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=mvavnuzbvoviwkqwsypyvthfvwxoeelk] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-mvavnuzbvoviwkqwsypyvthfvwxoeelk ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:30,578 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:32,544 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 8080 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "8080", "proto": "tcp", "to_port": "8080", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:32,548 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=8080/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 8080 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '8080'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '8080'
  item: 8080/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:32,561 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:32,561 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:32,561 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:32,561 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=yhnaivyezmvnmtbfvvykypwlhtbwlzkt] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-yhnaivyezmvnmtbfvvykypwlhtbwlzkt ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:32,613 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:33,542 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 8443 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "8443", "proto": "tcp", "to_port": "8443", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:33,546 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=8443/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 8443 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '8443'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '8443'
  item: 8443/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:33,557 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:33,557 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:33,558 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:33,558 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=nmqrhjknpdzjqcwytehacrkqahjsrvgr] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-nmqrhjknpdzjqcwytehacrkqahjsrvgr ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:33,601 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:34,488 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 3306 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "3306", "proto": "tcp", "to_port": "3306", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:34,492 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=3306/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 3306 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '3306'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '3306'
  item: 3306/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:34,502 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:34,503 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:34,503 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:34,503 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=hcdenodqlqgvlmwpbeugyjoomlcjmjpt] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-hcdenodqlqgvlmwpbeugyjoomlcjmjpt ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:34,556 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:36,413 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 3307 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "3307", "proto": "tcp", "to_port": "3307", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:36,417 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=3307/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 3307 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '3307'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '3307'
  item: 3307/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:36,427 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:36,427 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:36,427 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:36,427 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=dgtlapojbsmdcwyjabezgnzhipqukayh] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-dgtlapojbsmdcwyjabezgnzhipqukayh ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:36,478 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:37,446 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 80 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "80", "proto": "tcp", "to_port": "80", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:37,451 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=80/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 80 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '80'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '80'
  item: 80/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:37,472 p=55263 u=root n=ansible | Using module file /root/.ansible/collections/ansible_collections/community/general/plugins/modules/ufw.py
2025-10-25 17:44:37,472 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:37,472 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:37,472 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=jrjiwdnamwywjnjpyzfxzybelkqxwfyh] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-jrjiwdnamwywjnjpyzfxzybelkqxwfyh ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:37,547 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:38,437 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "commands": ["/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules", "/usr/sbin/ufw --version", "/usr/sbin/ufw allow from any to any port 443 proto tcp", "/usr/sbin/ufw status verbose", "/usr/bin/grep -h \'^### tuple\' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules"], "msg": "Status: active\\nLogging: on (low)\\nDefault: deny (incoming), allow (outgoing), deny (routed)\\nNew profiles: skip\\n\\nTo                         Action      From\\n--                         ------      ----\\n22/tcp                     ALLOW IN    Anywhere                  \\n179/tcp                    ALLOW IN    Anywhere                  \\n9345/tcp                   ALLOW IN    Anywhere                  \\n6443/tcp                   ALLOW IN    Anywhere                  \\n10250/tcp                  ALLOW IN    Anywhere                  \\n2379/tcp                   ALLOW IN    Anywhere                  \\n2380/tcp                   ALLOW IN    Anywhere                  \\n2381/tcp                   ALLOW IN    Anywhere                  \\n30000:32767/tcp            ALLOW IN    Anywhere                  \\n8472/udp                   ALLOW IN    Anywhere                  \\n6081/udp                   ALLOW IN    Anywhere                  \\n4240/tcp                   ALLOW IN    Anywhere                  \\n4244/tcp                   ALLOW IN    Anywhere                  \\n4245/tcp                   ALLOW IN    Anywhere                  \\n4222/tcp                   ALLOW IN    Anywhere                  \\n9966/tcp                   ALLOW IN    Anywhere                  \\n4250:4251/tcp              ALLOW IN    Anywhere                  \\n6060:6062/tcp              ALLOW IN    Anywhere                  \\n9878:9879/tcp              ALLOW IN    Anywhere                  \\n9890:9893/tcp              ALLOW IN    Anywhere                  \\n9901/tcp                   ALLOW IN    Anywhere                  \\n9962:9964/tcp              ALLOW IN    Anywhere                  \\n51871/udp                  ALLOW IN    Anywhere                  \\n4567/tcp                   ALLOW IN    Anywhere                  \\n4567/udp                   ALLOW IN    Anywhere                  \\n4568/tcp                   ALLOW IN    Anywhere                  \\n4444/tcp                   ALLOW IN    Anywhere                  \\n7443/tcp                   ALLOW IN    Anywhere                  \\n7345/tcp                   ALLOW IN    Anywhere                  \\n8080/tcp                   ALLOW IN    Anywhere                  \\n8443/tcp                   ALLOW IN    Anywhere                  \\n3306/tcp                   ALLOW IN    Anywhere                  \\n3307/tcp                   ALLOW IN    Anywhere                  \\n80/tcp                     ALLOW IN    Anywhere                  \\n443/tcp                    ALLOW IN    Anywhere                  \\n22/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n179/tcp (v6)               ALLOW IN    Anywhere (v6)             \\n9345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n6443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n10250/tcp (v6)             ALLOW IN    Anywhere (v6)             \\n2379/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2380/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n2381/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)             \\n8472/udp (v6)              ALLOW IN    Anywhere (v6)             \\n6081/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4240/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4244/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4245/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4222/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9966/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n9901/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)             \\n51871/udp (v6)             ALLOW IN    Anywhere (v6)             \\n4567/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4567/udp (v6)              ALLOW IN    Anywhere (v6)             \\n4568/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n4444/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n7345/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8080/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n8443/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3306/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n3307/tcp (v6)              ALLOW IN    Anywhere (v6)             \\n80/tcp (v6)                ALLOW IN    Anywhere (v6)             \\n443/tcp (v6)               ALLOW IN    Anywhere (v6)", "invocation": {"module_args": {"rule": "allow", "port": "443", "proto": "tcp", "to_port": "443", "delete": false, "route": false, "insert_relative_to": "zero", "log": false, "from_ip": "any", "to_ip": "any", "state": null, "default": null, "logging": null, "direction": null, "insert": null, "interface": null, "interface_in": null, "interface_out": null, "from_port": null, "name": null, "comment": null}}}\n', b'')
2025-10-25 17:44:38,441 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=443/tcp) => changed=false 
  ansible_loop_var: item
  commands:
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  - /usr/sbin/ufw --version
  - /usr/sbin/ufw allow from any to any port 443 proto tcp
  - /usr/sbin/ufw status verbose
  - /usr/bin/grep -h '^### tuple' /lib/ufw/user.rules /lib/ufw/user6.rules /etc/ufw/user.rules /etc/ufw/user6.rules /var/lib/ufw/user.rules /var/lib/ufw/user6.rules
  invocation:
    module_args:
      comment: null
      default: null
      delete: false
      direction: null
      from_ip: any
      from_port: null
      insert: null
      insert_relative_to: zero
      interface: null
      interface_in: null
      interface_out: null
      log: false
      logging: null
      name: null
      port: '443'
      proto: tcp
      route: false
      rule: allow
      state: null
      to_ip: any
      to_port: '443'
  item: 443/tcp
  msg: |-
    Status: active
    Logging: on (low)
    Default: deny (incoming), allow (outgoing), deny (routed)
    New profiles: skip
  
    To                         Action      From
    --                         ------      ----
    22/tcp                     ALLOW IN    Anywhere
    179/tcp                    ALLOW IN    Anywhere
    9345/tcp                   ALLOW IN    Anywhere
    6443/tcp                   ALLOW IN    Anywhere
    10250/tcp                  ALLOW IN    Anywhere
    2379/tcp                   ALLOW IN    Anywhere
    2380/tcp                   ALLOW IN    Anywhere
    2381/tcp                   ALLOW IN    Anywhere
    30000:32767/tcp            ALLOW IN    Anywhere
    8472/udp                   ALLOW IN    Anywhere
    6081/udp                   ALLOW IN    Anywhere
    4240/tcp                   ALLOW IN    Anywhere
    4244/tcp                   ALLOW IN    Anywhere
    4245/tcp                   ALLOW IN    Anywhere
    4222/tcp                   ALLOW IN    Anywhere
    9966/tcp                   ALLOW IN    Anywhere
    4250:4251/tcp              ALLOW IN    Anywhere
    6060:6062/tcp              ALLOW IN    Anywhere
    9878:9879/tcp              ALLOW IN    Anywhere
    9890:9893/tcp              ALLOW IN    Anywhere
    9901/tcp                   ALLOW IN    Anywhere
    9962:9964/tcp              ALLOW IN    Anywhere
    51871/udp                  ALLOW IN    Anywhere
    4567/tcp                   ALLOW IN    Anywhere
    4567/udp                   ALLOW IN    Anywhere
    4568/tcp                   ALLOW IN    Anywhere
    4444/tcp                   ALLOW IN    Anywhere
    7443/tcp                   ALLOW IN    Anywhere
    7345/tcp                   ALLOW IN    Anywhere
    8080/tcp                   ALLOW IN    Anywhere
    8443/tcp                   ALLOW IN    Anywhere
    3306/tcp                   ALLOW IN    Anywhere
    3307/tcp                   ALLOW IN    Anywhere
    80/tcp                     ALLOW IN    Anywhere
    443/tcp                    ALLOW IN    Anywhere
    22/tcp (v6)                ALLOW IN    Anywhere (v6)
    179/tcp (v6)               ALLOW IN    Anywhere (v6)
    9345/tcp (v6)              ALLOW IN    Anywhere (v6)
    6443/tcp (v6)              ALLOW IN    Anywhere (v6)
    10250/tcp (v6)             ALLOW IN    Anywhere (v6)
    2379/tcp (v6)              ALLOW IN    Anywhere (v6)
    2380/tcp (v6)              ALLOW IN    Anywhere (v6)
    2381/tcp (v6)              ALLOW IN    Anywhere (v6)
    30000:32767/tcp (v6)       ALLOW IN    Anywhere (v6)
    8472/udp (v6)              ALLOW IN    Anywhere (v6)
    6081/udp (v6)              ALLOW IN    Anywhere (v6)
    4240/tcp (v6)              ALLOW IN    Anywhere (v6)
    4244/tcp (v6)              ALLOW IN    Anywhere (v6)
    4245/tcp (v6)              ALLOW IN    Anywhere (v6)
    4222/tcp (v6)              ALLOW IN    Anywhere (v6)
    9966/tcp (v6)              ALLOW IN    Anywhere (v6)
    4250:4251/tcp (v6)         ALLOW IN    Anywhere (v6)
    6060:6062/tcp (v6)         ALLOW IN    Anywhere (v6)
    9878:9879/tcp (v6)         ALLOW IN    Anywhere (v6)
    9890:9893/tcp (v6)         ALLOW IN    Anywhere (v6)
    9901/tcp (v6)              ALLOW IN    Anywhere (v6)
    9962:9964/tcp (v6)         ALLOW IN    Anywhere (v6)
    51871/udp (v6)             ALLOW IN    Anywhere (v6)
    4567/tcp (v6)              ALLOW IN    Anywhere (v6)
    4567/udp (v6)              ALLOW IN    Anywhere (v6)
    4568/tcp (v6)              ALLOW IN    Anywhere (v6)
    4444/tcp (v6)              ALLOW IN    Anywhere (v6)
    7443/tcp (v6)              ALLOW IN    Anywhere (v6)
    7345/tcp (v6)              ALLOW IN    Anywhere (v6)
    8080/tcp (v6)              ALLOW IN    Anywhere (v6)
    8443/tcp (v6)              ALLOW IN    Anywhere (v6)
    3306/tcp (v6)              ALLOW IN    Anywhere (v6)
    3307/tcp (v6)              ALLOW IN    Anywhere (v6)
    80/tcp (v6)                ALLOW IN    Anywhere (v6)
    443/tcp (v6)               ALLOW IN    Anywhere (v6)
2025-10-25 17:44:38,566 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/file.py
2025-10-25 17:44:38,566 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:38,567 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:38,567 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=jjzhrpzoflmvaflnpnhvqzkjkrrnsugk] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-jjzhrpzoflmvaflnpnhvqzkjkrrnsugk ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:38,610 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:38,874 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"path": "/rke2/artifact", "changed": false, "diff": {"before": {"path": "/rke2/artifact"}, "after": {"path": "/rke2/artifact"}}, "uid": 0, "gid": 0, "owner": "root", "group": "root", "mode": "0700", "state": "directory", "size": 4096, "invocation": {"module_args": {"path": "/rke2/artifact", "state": "directory", "mode": 448, "recurse": false, "force": false, "follow": true, "modification_time_format": "%Y%m%d%H%M.%S", "access_time_format": "%Y%m%d%H%M.%S", "unsafe_writes": false, "_original_basename": null, "_diff_peek": null, "src": null, "modification_time": null, "access_time": null, "owner": null, "group": null, "seuser": null, "serole": null, "selevel": null, "setype": null, "attributes": null}}}\n', b'')
2025-10-25 17:44:38,877 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Create RKE2 artifacts folder] ************************************
2025-10-25 17:44:38,878 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  diff:
    after:
      path: /rke2/artifact
    before:
      path: /rke2/artifact
  gid: 0
  group: root
  invocation:
    module_args:
      _diff_peek: null
      _original_basename: null
      access_time: null
      access_time_format: '%Y%m%d%H%M.%S'
      attributes: null
      follow: true
      force: false
      group: null
      mode: 448
      modification_time: null
      modification_time_format: '%Y%m%d%H%M.%S'
      owner: null
      path: /rke2/artifact
      recurse: false
      selevel: null
      serole: null
      setype: null
      seuser: null
      src: null
      state: directory
      unsafe_writes: false
  mode: '0700'
  owner: root
  path: /rke2/artifact
  size: 4096
  state: directory
  uid: 0
2025-10-25 17:44:39,082 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/get_url.py
2025-10-25 17:44:39,083 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:39,083 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:39,083 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=zcuknpvlezkexchvvoutjbxxizcsgdwm] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-zcuknpvlezkexchvvoutjbxxizcsgdwm ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:39,128 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:39,617 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"msg": "OK (4252 bytes)", "status_code": 200, "changed": false, "checksum_dest": "b11f1d86b1adc36d1f496eac0f272061b8c953ed", "checksum_src": "b11f1d86b1adc36d1f496eac0f272061b8c953ed", "dest": "/rke2/artifact/sha256sum-amd64.txt", "elapsed": 0, "url": "http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/sha256sum-amd64.txt", "src": "/root/.ansible/tmp/ansible-moduletmp-1761389079.7609546-jc4rz87i/tmpgzywo4tc", "md5sum": "e3066e778907170a0c5ee2509aac23de", "uid": 0, "gid": 0, "owner": "root", "group": "root", "mode": "0640", "state": "file", "size": 4252, "invocation": {"module_args": {"url": "http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/sha256sum-amd64.txt", "dest": "/rke2/artifact/sha256sum-amd64.txt", "force": true, "mode": 416, "timeout": 30, "http_agent": "ansible-httpget", "use_proxy": true, "validate_certs": true, "force_basic_auth": false, "use_gssapi": false, "backup": false, "checksum": "", "unredirected_headers": [], "decompress": true, "use_netrc": true, "unsafe_writes": false, "url_username": null, "url_password": null, "client_cert": null, "client_key": null, "headers": null, "tmp_dest": null, "ciphers": null, "owner": null, "group": null, "seuser": null, "serole": null, "selevel": null, "setype": null, "attributes": null}}}\n', b'')
2025-10-25 17:44:39,620 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Download sha256 checksum file ( airgap mode )] *******************
2025-10-25 17:44:39,622 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  checksum_dest: b11f1d86b1adc36d1f496eac0f272061b8c953ed
  checksum_src: b11f1d86b1adc36d1f496eac0f272061b8c953ed
  dest: /rke2/artifact/sha256sum-amd64.txt
  elapsed: 0
  gid: 0
  group: root
  invocation:
    module_args:
      attributes: null
      backup: false
      checksum: ''
      ciphers: null
      client_cert: null
      client_key: null
      decompress: true
      dest: /rke2/artifact/sha256sum-amd64.txt
      force: true
      force_basic_auth: false
      group: null
      headers: null
      http_agent: ansible-httpget
      mode: 416
      owner: null
      selevel: null
      serole: null
      setype: null
      seuser: null
      timeout: 30
      tmp_dest: null
      unredirected_headers: []
      unsafe_writes: false
      url: http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/sha256sum-amd64.txt
      url_password: null
      url_username: null
      use_gssapi: false
      use_netrc: true
      use_proxy: true
      validate_certs: true
  md5sum: e3066e778907170a0c5ee2509aac23de
  mode: '0640'
  msg: OK (4252 bytes)
  owner: root
  size: 4252
  src: /root/.ansible/tmp/ansible-moduletmp-1761389079.7609546-jc4rz87i/tmpgzywo4tc
  state: file
  status_code: 200
  uid: 0
  url: http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/sha256sum-amd64.txt
2025-10-25 17:44:39,698 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/get_url.py
2025-10-25 17:44:39,699 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:39,699 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:39,699 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=wbxeqplpupmuwtqfqtjzosbcuifykazv] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-wbxeqplpupmuwtqfqtjzosbcuifykazv ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:39,739 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:40,394 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"msg": "file already exists", "changed": false, "checksum_dest": null, "checksum_src": null, "dest": "/rke2/artifact/rke2.linux-amd64.tar.gz", "elapsed": 0, "url": "http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2.linux-amd64.tar.gz", "uid": 0, "gid": 0, "owner": "root", "group": "root", "mode": "0640", "state": "file", "size": 39700180, "invocation": {"module_args": {"url": "http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2.linux-amd64.tar.gz", "dest": "/rke2/artifact/rke2.linux-amd64.tar.gz", "mode": 416, "checksum": "sha256:http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/sha256sum-amd64.txt", "timeout": 30, "force": false, "http_agent": "ansible-httpget", "use_proxy": true, "validate_certs": true, "force_basic_auth": false, "use_gssapi": false, "backup": false, "unredirected_headers": [], "decompress": true, "use_netrc": true, "unsafe_writes": false, "url_username": null, "url_password": null, "client_cert": null, "client_key": null, "headers": null, "tmp_dest": null, "ciphers": null, "owner": null, "group": null, "seuser": null, "serole": null, "selevel": null, "setype": null, "attributes": null}}}\n', b'')
2025-10-25 17:44:40,397 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 artifacts and compare with checksums ( airgap mode )] ***
2025-10-25 17:44:40,399 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=rke2.linux-amd64.tar.gz) => changed=false 
  ansible_loop_var: item
  checksum_dest: null
  checksum_src: null
  dest: /rke2/artifact/rke2.linux-amd64.tar.gz
  elapsed: 0
  gid: 0
  group: root
  invocation:
    module_args:
      attributes: null
      backup: false
      checksum: sha256:http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/sha256sum-amd64.txt
      ciphers: null
      client_cert: null
      client_key: null
      decompress: true
      dest: /rke2/artifact/rke2.linux-amd64.tar.gz
      force: false
      force_basic_auth: false
      group: null
      headers: null
      http_agent: ansible-httpget
      mode: 416
      owner: null
      selevel: null
      serole: null
      setype: null
      seuser: null
      timeout: 30
      tmp_dest: null
      unredirected_headers: []
      unsafe_writes: false
      url: http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2.linux-amd64.tar.gz
      url_password: null
      url_username: null
      use_gssapi: false
      use_netrc: true
      use_proxy: true
      validate_certs: true
  item: rke2.linux-amd64.tar.gz
  mode: '0640'
  msg: file already exists
  owner: root
  size: 39700180
  state: file
  uid: 0
  url: http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2.linux-amd64.tar.gz
2025-10-25 17:44:40,410 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/get_url.py
2025-10-25 17:44:40,410 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:40,410 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:40,410 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=sejjyrslbfhygxwzshegenidlebhrsbc] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-sejjyrslbfhygxwzshegenidlebhrsbc ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:40,451 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:46,491 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"msg": "file already exists", "changed": false, "checksum_dest": null, "checksum_src": null, "dest": "/rke2/artifact/rke2-images.linux-amd64.tar.gz", "elapsed": 0, "url": "http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images.linux-amd64.tar.gz", "uid": 0, "gid": 0, "owner": "root", "group": "root", "mode": "0640", "state": "file", "size": 1043887568, "invocation": {"module_args": {"url": "http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images.linux-amd64.tar.gz", "dest": "/rke2/artifact/rke2-images.linux-amd64.tar.gz", "mode": 416, "checksum": "sha256:http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/sha256sum-amd64.txt", "timeout": 30, "force": false, "http_agent": "ansible-httpget", "use_proxy": true, "validate_certs": true, "force_basic_auth": false, "use_gssapi": false, "backup": false, "unredirected_headers": [], "decompress": true, "use_netrc": true, "unsafe_writes": false, "url_username": null, "url_password": null, "client_cert": null, "client_key": null, "headers": null, "tmp_dest": null, "ciphers": null, "owner": null, "group": null, "seuser": null, "serole": null, "selevel": null, "setype": null, "attributes": null}}}\n', b'')
2025-10-25 17:44:46,496 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=rke2-images.linux-amd64.tar.gz) => changed=false 
  ansible_loop_var: item
  checksum_dest: null
  checksum_src: null
  dest: /rke2/artifact/rke2-images.linux-amd64.tar.gz
  elapsed: 0
  gid: 0
  group: root
  invocation:
    module_args:
      attributes: null
      backup: false
      checksum: sha256:http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/sha256sum-amd64.txt
      ciphers: null
      client_cert: null
      client_key: null
      decompress: true
      dest: /rke2/artifact/rke2-images.linux-amd64.tar.gz
      force: false
      force_basic_auth: false
      group: null
      headers: null
      http_agent: ansible-httpget
      mode: 416
      owner: null
      selevel: null
      serole: null
      setype: null
      seuser: null
      timeout: 30
      tmp_dest: null
      unredirected_headers: []
      unsafe_writes: false
      url: http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images.linux-amd64.tar.gz
      url_password: null
      url_username: null
      use_gssapi: false
      use_netrc: true
      use_proxy: true
      validate_certs: true
  item: rke2-images.linux-amd64.tar.gz
  mode: '0640'
  msg: file already exists
  owner: root
  size: 1043887568
  state: file
  uid: 0
  url: http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images.linux-amd64.tar.gz
2025-10-25 17:44:46,507 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/get_url.py
2025-10-25 17:44:46,507 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:46,507 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:46,508 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=mspfdcupxidmymcfrhvwsblwiahawxcj] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-mspfdcupxidmymcfrhvwsblwiahawxcj ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:46,587 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:50,197 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"msg": "file already exists", "changed": false, "checksum_dest": null, "checksum_src": null, "dest": "/rke2/artifact/rke2-images-cilium.linux-amd64.tar.gz", "elapsed": 0, "url": "http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images-cilium.linux-amd64.tar.gz", "uid": 0, "gid": 0, "owner": "root", "group": "root", "mode": "0640", "state": "file", "size": 595270840, "invocation": {"module_args": {"url": "http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images-cilium.linux-amd64.tar.gz", "dest": "/rke2/artifact/rke2-images-cilium.linux-amd64.tar.gz", "mode": 416, "checksum": "sha256:http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/sha256sum-amd64.txt", "timeout": 30, "force": false, "http_agent": "ansible-httpget", "use_proxy": true, "validate_certs": true, "force_basic_auth": false, "use_gssapi": false, "backup": false, "unredirected_headers": [], "decompress": true, "use_netrc": true, "unsafe_writes": false, "url_username": null, "url_password": null, "client_cert": null, "client_key": null, "headers": null, "tmp_dest": null, "ciphers": null, "owner": null, "group": null, "seuser": null, "serole": null, "selevel": null, "setype": null, "attributes": null}}}\n', b'')
2025-10-25 17:44:50,202 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=rke2-images-cilium.linux-amd64.tar.gz) => changed=false 
  ansible_loop_var: item
  checksum_dest: null
  checksum_src: null
  dest: /rke2/artifact/rke2-images-cilium.linux-amd64.tar.gz
  elapsed: 0
  gid: 0
  group: root
  invocation:
    module_args:
      attributes: null
      backup: false
      checksum: sha256:http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/sha256sum-amd64.txt
      ciphers: null
      client_cert: null
      client_key: null
      decompress: true
      dest: /rke2/artifact/rke2-images-cilium.linux-amd64.tar.gz
      force: false
      force_basic_auth: false
      group: null
      headers: null
      http_agent: ansible-httpget
      mode: 416
      owner: null
      selevel: null
      serole: null
      setype: null
      seuser: null
      timeout: 30
      tmp_dest: null
      unredirected_headers: []
      unsafe_writes: false
      url: http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images-cilium.linux-amd64.tar.gz
      url_password: null
      url_username: null
      use_gssapi: false
      use_netrc: true
      use_proxy: true
      validate_certs: true
  item: rke2-images-cilium.linux-amd64.tar.gz
  mode: '0640'
  msg: file already exists
  owner: root
  size: 595270840
  state: file
  uid: 0
  url: http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images-cilium.linux-amd64.tar.gz
2025-10-25 17:44:50,213 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/get_url.py
2025-10-25 17:44:50,213 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:50,213 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:50,214 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=jjkxjvqjbthufmmmkighbggosrygmnqe] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-jjkxjvqjbthufmmmkighbggosrygmnqe ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:50,279 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:53,718 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"msg": "file already exists", "changed": false, "checksum_dest": null, "checksum_src": null, "dest": "/rke2/artifact/rke2-images-calico.linux-amd64.tar.gz", "elapsed": 0, "url": "http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images-calico.linux-amd64.tar.gz", "uid": 0, "gid": 0, "owner": "root", "group": "root", "mode": "0640", "state": "file", "size": 577853188, "invocation": {"module_args": {"url": "http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images-calico.linux-amd64.tar.gz", "dest": "/rke2/artifact/rke2-images-calico.linux-amd64.tar.gz", "mode": 416, "checksum": "sha256:http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/sha256sum-amd64.txt", "timeout": 30, "force": false, "http_agent": "ansible-httpget", "use_proxy": true, "validate_certs": true, "force_basic_auth": false, "use_gssapi": false, "backup": false, "unredirected_headers": [], "decompress": true, "use_netrc": true, "unsafe_writes": false, "url_username": null, "url_password": null, "client_cert": null, "client_key": null, "headers": null, "tmp_dest": null, "ciphers": null, "owner": null, "group": null, "seuser": null, "serole": null, "selevel": null, "setype": null, "attributes": null}}}\n', b'')
2025-10-25 17:44:53,724 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=rke2-images-calico.linux-amd64.tar.gz) => changed=false 
  ansible_loop_var: item
  checksum_dest: null
  checksum_src: null
  dest: /rke2/artifact/rke2-images-calico.linux-amd64.tar.gz
  elapsed: 0
  gid: 0
  group: root
  invocation:
    module_args:
      attributes: null
      backup: false
      checksum: sha256:http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/sha256sum-amd64.txt
      ciphers: null
      client_cert: null
      client_key: null
      decompress: true
      dest: /rke2/artifact/rke2-images-calico.linux-amd64.tar.gz
      force: false
      force_basic_auth: false
      group: null
      headers: null
      http_agent: ansible-httpget
      mode: 416
      owner: null
      selevel: null
      serole: null
      setype: null
      seuser: null
      timeout: 30
      tmp_dest: null
      unredirected_headers: []
      unsafe_writes: false
      url: http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images-calico.linux-amd64.tar.gz
      url_password: null
      url_username: null
      use_gssapi: false
      use_netrc: true
      use_proxy: true
      validate_certs: true
  item: rke2-images-calico.linux-amd64.tar.gz
  mode: '0640'
  msg: file already exists
  owner: root
  size: 577853188
  state: file
  uid: 0
  url: http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images-calico.linux-amd64.tar.gz
2025-10-25 17:44:53,740 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/get_url.py
2025-10-25 17:44:53,740 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:53,741 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:53,741 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=dazgldunvvhlrewosiqtwvgtdlsneuhw] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-dazgldunvvhlrewosiqtwvgtdlsneuhw ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:53,782 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:55,190 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"msg": "file already exists", "changed": false, "checksum_dest": null, "checksum_src": null, "dest": "/rke2/artifact/rke2-images-multus.linux-amd64.tar.gz", "elapsed": 0, "url": "http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images-multus.linux-amd64.tar.gz", "uid": 0, "gid": 0, "owner": "root", "group": "root", "mode": "0640", "state": "file", "size": 177177847, "invocation": {"module_args": {"url": "http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images-multus.linux-amd64.tar.gz", "dest": "/rke2/artifact/rke2-images-multus.linux-amd64.tar.gz", "mode": 416, "checksum": "sha256:http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/sha256sum-amd64.txt", "timeout": 30, "force": false, "http_agent": "ansible-httpget", "use_proxy": true, "validate_certs": true, "force_basic_auth": false, "use_gssapi": false, "backup": false, "unredirected_headers": [], "decompress": true, "use_netrc": true, "unsafe_writes": false, "url_username": null, "url_password": null, "client_cert": null, "client_key": null, "headers": null, "tmp_dest": null, "ciphers": null, "owner": null, "group": null, "seuser": null, "serole": null, "selevel": null, "setype": null, "attributes": null}}}\n', b'')
2025-10-25 17:44:55,195 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=rke2-images-multus.linux-amd64.tar.gz) => changed=false 
  ansible_loop_var: item
  checksum_dest: null
  checksum_src: null
  dest: /rke2/artifact/rke2-images-multus.linux-amd64.tar.gz
  elapsed: 0
  gid: 0
  group: root
  invocation:
    module_args:
      attributes: null
      backup: false
      checksum: sha256:http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/sha256sum-amd64.txt
      ciphers: null
      client_cert: null
      client_key: null
      decompress: true
      dest: /rke2/artifact/rke2-images-multus.linux-amd64.tar.gz
      force: false
      force_basic_auth: false
      group: null
      headers: null
      http_agent: ansible-httpget
      mode: 416
      owner: null
      selevel: null
      serole: null
      setype: null
      seuser: null
      timeout: 30
      tmp_dest: null
      unredirected_headers: []
      unsafe_writes: false
      url: http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images-multus.linux-amd64.tar.gz
      url_password: null
      url_username: null
      use_gssapi: false
      use_netrc: true
      use_proxy: true
      validate_certs: true
  item: rke2-images-multus.linux-amd64.tar.gz
  mode: '0640'
  msg: file already exists
  owner: root
  size: 177177847
  state: file
  uid: 0
  url: http://10.0.6.3:8081/repository/rke2-proxy/v1.34.1+rke2r1/rke2-images-multus.linux-amd64.tar.gz
2025-10-25 17:44:55,254 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/get_url.py
2025-10-25 17:44:55,255 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:55,257 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:55,257 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=digpiowqmokcfgwjaqyafslklaaigtoz] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-digpiowqmokcfgwjaqyafslklaaigtoz ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:55,300 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:44:55,914 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"url": "http://10.0.6.3:8081/repository/rke2-install-proxy/", "dest": "/var/tmp/rke2.sh", "changed": false, "msg": "HTTP Error 304: Not Modified", "status_code": 304, "elapsed": 0, "uid": 0, "gid": 0, "owner": "root", "group": "root", "mode": "0700", "state": "file", "size": 25288, "invocation": {"module_args": {"url": "http://10.0.6.3:8081/repository/rke2-install-proxy/", "dest": "/var/tmp/rke2.sh", "mode": 448, "timeout": 30, "force": false, "http_agent": "ansible-httpget", "use_proxy": true, "validate_certs": true, "force_basic_auth": false, "use_gssapi": false, "backup": false, "checksum": "", "unredirected_headers": [], "decompress": true, "use_netrc": true, "unsafe_writes": false, "url_username": null, "url_password": null, "client_cert": null, "client_key": null, "headers": null, "tmp_dest": null, "ciphers": null, "owner": null, "group": null, "seuser": null, "serole": null, "selevel": null, "setype": null, "attributes": null}}}\n', b'')
2025-10-25 17:44:55,916 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 install script ( airgap mode )] ********************
2025-10-25 17:44:55,918 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  dest: /var/tmp/rke2.sh
  elapsed: 0
  gid: 0
  group: root
  invocation:
    module_args:
      attributes: null
      backup: false
      checksum: ''
      ciphers: null
      client_cert: null
      client_key: null
      decompress: true
      dest: /var/tmp/rke2.sh
      force: false
      force_basic_auth: false
      group: null
      headers: null
      http_agent: ansible-httpget
      mode: 448
      owner: null
      selevel: null
      serole: null
      setype: null
      seuser: null
      timeout: 30
      tmp_dest: null
      unredirected_headers: []
      unsafe_writes: false
      url: http://10.0.6.3:8081/repository/rke2-install-proxy/
      url_password: null
      url_username: null
      use_gssapi: false
      use_netrc: true
      use_proxy: true
      validate_certs: true
  mode: '0700'
  msg: 'HTTP Error 304: Not Modified'
  owner: root
  size: 25288
  state: file
  status_code: 304
  uid: 0
  url: http://10.0.6.3:8081/repository/rke2-install-proxy/
2025-10-25 17:44:56,673 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/service_facts.py
2025-10-25 17:44:56,674 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:44:56,674 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:44:56,675 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=mhdjtqprgiwbgpuyuwjiaaixhirwxkun] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-mhdjtqprgiwbgpuyuwjiaaixhirwxkun ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:44:56,731 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:02,462 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"ansible_facts": {"services": {"apparmor": {"name": "apparmor", "state": "running", "source": "sysv"}, "apport": {"name": "apport", "state": "running", "source": "sysv"}, "console-setup.sh": {"name": "console-setup.sh", "state": "stopped", "source": "sysv"}, "cron": {"name": "cron", "state": "running", "source": "sysv"}, "cryptdisks": {"name": "cryptdisks", "state": "stopped", "source": "sysv"}, "cryptdisks-early": {"name": "cryptdisks-early", "state": "stopped", "source": "sysv"}, "dbus": {"name": "dbus", "state": "running", "source": "sysv"}, "grub-common": {"name": "grub-common", "state": "stopped", "source": "sysv"}, "iscsid": {"name": "iscsid", "state": "stopped", "source": "sysv"}, "keyboard-setup.sh": {"name": "keyboard-setup.sh", "state": "stopped", "source": "sysv"}, "kmod": {"name": "kmod", "state": "running", "source": "sysv"}, "open-iscsi": {"name": "open-iscsi", "state": "stopped", "source": "sysv"}, "open-vm-tools": {"name": "open-vm-tools", "state": "running", "source": "sysv"}, "plymouth": {"name": "plymouth", "state": "running", "source": "sysv"}, "plymouth-log": {"name": "plymouth-log", "state": "running", "source": "sysv"}, "procps": {"name": "procps", "state": "running", "source": "sysv"}, "rsync": {"name": "rsync", "state": "stopped", "source": "sysv"}, "screen-cleanup": {"name": "screen-cleanup", "state": "stopped", "source": "sysv"}, "ssh": {"name": "ssh", "state": "running", "source": "sysv"}, "sysstat": {"name": "sysstat", "state": "running", "source": "sysv"}, "ufw": {"name": "ufw", "state": "running", "source": "sysv"}, "unattended-upgrades": {"name": "unattended-upgrades", "state": "running", "source": "sysv"}, "uuidd": {"name": "uuidd", "state": "stopped", "source": "sysv"}, "apparmor.service": {"name": "apparmor.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "apport-autoreport.service": {"name": "apport-autoreport.service", "state": "stopped", "status": "static", "source": "systemd"}, "apport.service": {"name": "apport.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "apt-daily-upgrade.service": {"name": "apt-daily-upgrade.service", "state": "stopped", "status": "static", "source": "systemd"}, "apt-daily.service": {"name": "apt-daily.service", "state": "stopped", "status": "static", "source": "systemd"}, "auditd.service": {"name": "auditd.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "blk-availability.service": {"name": "blk-availability.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "cloud-init-local.service": {"name": "cloud-init-local.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "connman.service": {"name": "connman.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "console-screen.service": {"name": "console-screen.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "console-setup.service": {"name": "console-setup.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "cron.service": {"name": "cron.service", "state": "running", "status": "enabled", "source": "systemd"}, "dbus.service": {"name": "dbus.service", "state": "running", "status": "static", "source": "systemd"}, "display-manager.service": {"name": "display-manager.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "dm-event.service": {"name": "dm-event.service", "state": "stopped", "status": "static", "source": "systemd"}, "dmesg.service": {"name": "dmesg.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "dpkg-db-backup.service": {"name": "dpkg-db-backup.service", "state": "stopped", "status": "static", "source": "systemd"}, "e2scrub_all.service": {"name": "e2scrub_all.service", "state": "stopped", "status": "static", "source": "systemd"}, "e2scrub_reap.service": {"name": "e2scrub_reap.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "emergency.service": {"name": "emergency.service", "state": "stopped", "status": "static", "source": "systemd"}, "fcoe.service": {"name": "fcoe.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "finalrd.service": {"name": "finalrd.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "fstrim.service": {"name": "fstrim.service", "state": "stopped", "status": "static", "source": "systemd"}, "fwupd-refresh.service": {"name": "fwupd-refresh.service", "state": "stopped", "status": "failed", "source": "systemd"}, "fwupd.service": {"name": "fwupd.service", "state": "running", "status": "static", "source": "systemd"}, "getty-static.service": {"name": "getty-static.service", "state": "stopped", "status": "static", "source": "systemd"}, "getty@tty1.service": {"name": "getty@tty1.service", "state": "running", "status": "active", "source": "systemd"}, "grub-common.service": {"name": "grub-common.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "loaded": {"name": "loaded", "state": "stopped", "status": "failed", "source": "systemd"}, "hv_kvp_daemon.service": {"name": "hv_kvp_daemon.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "initrd-cleanup.service": {"name": "initrd-cleanup.service", "state": "stopped", "status": "static", "source": "systemd"}, "initrd-parse-etc.service": {"name": "initrd-parse-etc.service", "state": "stopped", "status": "static", "source": "systemd"}, "initrd-switch-root.service": {"name": "initrd-switch-root.service", "state": "stopped", "status": "static", "source": "systemd"}, "initrd-udevadm-cleanup-db.service": {"name": "initrd-udevadm-cleanup-db.service", "state": "stopped", "status": "static", "source": "systemd"}, "iscsi-shutdown.service": {"name": "iscsi-shutdown.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "iscsid.service": {"name": "iscsid.service", "state": "stopped", "status": "disabled", "source": "systemd"}, "kbd.service": {"name": "kbd.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "keyboard-setup.service": {"name": "keyboard-setup.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "kmod-static-nodes.service": {"name": "kmod-static-nodes.service", "state": "stopped", "status": "static", "source": "systemd"}, "ldconfig.service": {"name": "ldconfig.service", "state": "stopped", "status": "static", "source": "systemd"}, "logrotate.service": {"name": "logrotate.service", "state": "stopped", "status": "static", "source": "systemd"}, "lvm2-activation-early.service": {"name": "lvm2-activation-early.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "lvm2-lvmpolld.service": {"name": "lvm2-lvmpolld.service", "state": "stopped", "status": "static", "source": "systemd"}, "lvm2-monitor.service": {"name": "lvm2-monitor.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "man-db.service": {"name": "man-db.service", "state": "stopped", "status": "static", "source": "systemd"}, "ModemManager.service": {"name": "ModemManager.service", "state": "running", "status": "enabled", "source": "systemd"}, "modprobe@configfs.service": {"name": "modprobe@configfs.service", "state": "stopped", "status": "inactive", "source": "systemd"}, "modprobe@dm_mod.service": {"name": "modprobe@dm_mod.service", "state": "stopped", "status": "inactive", "source": "systemd"}, "modprobe@drm.service": {"name": "modprobe@drm.service", "state": "stopped", "status": "inactive", "source": "systemd"}, "modprobe@efi_pstore.service": {"name": "modprobe@efi_pstore.service", "state": "stopped", "status": "inactive", "source": "systemd"}, "modprobe@fuse.service": {"name": "modprobe@fuse.service", "state": "stopped", "status": "inactive", "source": "systemd"}, "modprobe@loop.service": {"name": "modprobe@loop.service", "state": "stopped", "status": "inactive", "source": "systemd"}, "motd-news.service": {"name": "motd-news.service", "state": "stopped", "status": "static", "source": "systemd"}, "multipathd.service": {"name": "multipathd.service", "state": "running", "status": "enabled", "source": "systemd"}, "netplan-ovs-cleanup.service": {"name": "netplan-ovs-cleanup.service", "state": "stopped", "status": "enabled-runtime", "source": "systemd"}, "networkd-dispatcher.service": {"name": "networkd-dispatcher.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "NetworkManager.service": {"name": "NetworkManager.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "open-iscsi.service": {"name": "open-iscsi.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "open-vm-tools.service": {"name": "open-vm-tools.service", "state": "running", "status": "enabled", "source": "systemd"}, "ovsdb-server.service": {"name": "ovsdb-server.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "plymouth-quit-wait.service": {"name": "plymouth-quit-wait.service", "state": "stopped", "status": "static", "source": "systemd"}, "plymouth-quit.service": {"name": "plymouth-quit.service", "state": "stopped", "status": "static", "source": "systemd"}, "plymouth-read-write.service": {"name": "plymouth-read-write.service", "state": "stopped", "status": "static", "source": "systemd"}, "plymouth-start.service": {"name": "plymouth-start.service", "state": "stopped", "status": "static", "source": "systemd"}, "plymouth-switch-root.service": {"name": "plymouth-switch-root.service", "state": "stopped", "status": "static", "source": "systemd"}, "polkit.service": {"name": "polkit.service", "state": "running", "status": "static", "source": "systemd"}, "pollinate.service": {"name": "pollinate.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "rbdmap.service": {"name": "rbdmap.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "rc-local.service": {"name": "rc-local.service", "state": "stopped", "status": "static", "source": "systemd"}, "rescue.service": {"name": "rescue.service", "state": "stopped", "status": "static", "source": "systemd"}, "rke2-agent.service": {"name": "rke2-agent.service", "state": "stopped", "status": "masked", "source": "systemd"}, "rke2-server.service": {"name": "rke2-server.service", "state": "running", "status": "enabled", "source": "systemd"}, "rsyslog.service": {"name": "rsyslog.service", "state": "running", "status": "enabled", "source": "systemd"}, "secureboot-db.service": {"name": "secureboot-db.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "setvtrgb.service": {"name": "setvtrgb.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "snapd.apparmor.service": {"name": "snapd.apparmor.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "snapd.autoimport.service": {"name": "snapd.autoimport.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "snapd.core-fixup.service": {"name": "snapd.core-fixup.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "snapd.failure.service": {"name": "snapd.failure.service", "state": "stopped", "status": "static", "source": "systemd"}, "snapd.recovery-chooser-trigger.service": {"name": "snapd.recovery-chooser-trigger.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "snapd.seeded.service": {"name": "snapd.seeded.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "snapd.service": {"name": "snapd.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "snapd.snap-repair.service": {"name": "snapd.snap-repair.service", "state": "stopped", "status": "static", "source": "systemd"}, "snapd.system-shutdown.service": {"name": "snapd.system-shutdown.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "ssh.service": {"name": "ssh.service", "state": "running", "status": "disabled", "source": "systemd"}, "sysstat-collect.service": {"name": "sysstat-collect.service", "state": "stopped", "status": "static", "source": "systemd"}, "sysstat-summary.service": {"name": "sysstat-summary.service", "state": "stopped", "status": "static", "source": "systemd"}, "sysstat.service": {"name": "sysstat.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "systemd-ask-password-console.service": {"name": "systemd-ask-password-console.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-ask-password-plymouth.service": {"name": "systemd-ask-password-plymouth.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-ask-password-wall.service": {"name": "systemd-ask-password-wall.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-battery-check.service": {"name": "systemd-battery-check.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-binfmt.service": {"name": "systemd-binfmt.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-bsod.service": {"name": "systemd-bsod.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-firstboot.service": {"name": "systemd-firstboot.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-fsck-root.service": {"name": "systemd-fsck-root.service", "state": "stopped", "status": "enabled-runtime", "source": "systemd"}, "systemd-fsck@dev-disk-by\\\\x2duuid-9bb05da7\\\\x2d85f0\\\\x2d4338\\\\x2d9b8f\\\\x2d06ca0d639ac6.service": {"name": "systemd-fsck@dev-disk-by\\\\x2duuid-9bb05da7\\\\x2d85f0\\\\x2d4338\\\\x2d9b8f\\\\x2d06ca0d639ac6.service", "state": "stopped", "status": "active", "source": "systemd"}, "systemd-fsckd.service": {"name": "systemd-fsckd.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-hibernate-resume.service": {"name": "systemd-hibernate-resume.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-hibernate.service": {"name": "systemd-hibernate.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-hwdb-update.service": {"name": "systemd-hwdb-update.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-hybrid-sleep.service": {"name": "systemd-hybrid-sleep.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-initctl.service": {"name": "systemd-initctl.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-journal-catalog-update.service": {"name": "systemd-journal-catalog-update.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-journal-flush.service": {"name": "systemd-journal-flush.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-journald.service": {"name": "systemd-journald.service", "state": "running", "status": "static", "source": "systemd"}, "systemd-logind.service": {"name": "systemd-logind.service", "state": "running", "status": "static", "source": "systemd"}, "systemd-machine-id-commit.service": {"name": "systemd-machine-id-commit.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-modules-load.service": {"name": "systemd-modules-load.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-networkd-wait-online.service": {"name": "systemd-networkd-wait-online.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "systemd-networkd.service": {"name": "systemd-networkd.service", "state": "running", "status": "enabled", "source": "systemd"}, "systemd-oomd.service": {"name": "systemd-oomd.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "systemd-pcrmachine.service": {"name": "systemd-pcrmachine.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-pcrphase-initrd.service": {"name": "systemd-pcrphase-initrd.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-pcrphase-sysinit.service": {"name": "systemd-pcrphase-sysinit.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-pcrphase.service": {"name": "systemd-pcrphase.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-pstore.service": {"name": "systemd-pstore.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "systemd-quotacheck.service": {"name": "systemd-quotacheck.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-random-seed.service": {"name": "systemd-random-seed.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-remount-fs.service": {"name": "systemd-remount-fs.service", "state": "stopped", "status": "enabled-runtime", "source": "systemd"}, "systemd-repart.service": {"name": "systemd-repart.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-resolved.service": {"name": "systemd-resolved.service", "state": "running", "status": "enabled", "source": "systemd"}, "systemd-rfkill.service": {"name": "systemd-rfkill.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-soft-reboot.service": {"name": "systemd-soft-reboot.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-suspend-then-hibernate.service": {"name": "systemd-suspend-then-hibernate.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-suspend.service": {"name": "systemd-suspend.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-sysctl.service": {"name": "systemd-sysctl.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-sysext.service": {"name": "systemd-sysext.service", "state": "stopped", "status": "disabled", "source": "systemd"}, "systemd-sysusers.service": {"name": "systemd-sysusers.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-timesyncd.service": {"name": "systemd-timesyncd.service", "state": "running", "status": "enabled", "source": "systemd"}, "systemd-tmpfiles-clean.service": {"name": "systemd-tmpfiles-clean.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-tmpfiles-setup-dev-early.service": {"name": "systemd-tmpfiles-setup-dev-early.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-tmpfiles-setup-dev.service": {"name": "systemd-tmpfiles-setup-dev.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-tmpfiles-setup.service": {"name": "systemd-tmpfiles-setup.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-tpm2-setup-early.service": {"name": "systemd-tpm2-setup-early.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-tpm2-setup.service": {"name": "systemd-tpm2-setup.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-udev-settle.service": {"name": "systemd-udev-settle.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-udev-trigger.service": {"name": "systemd-udev-trigger.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-udevd.service": {"name": "systemd-udevd.service", "state": "running", "status": "static", "source": "systemd"}, "systemd-update-done.service": {"name": "systemd-update-done.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-update-utmp-runlevel.service": {"name": "systemd-update-utmp-runlevel.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-update-utmp.service": {"name": "systemd-update-utmp.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-user-sessions.service": {"name": "systemd-user-sessions.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-vconsole-setup.service": {"name": "systemd-vconsole-setup.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "thermald.service": {"name": "thermald.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "tpm-udev.service": {"name": "tpm-udev.service", "state": "stopped", "status": "static", "source": "systemd"}, "ua-auto-attach.service": {"name": "ua-auto-attach.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "ua-reboot-cmds.service": {"name": "ua-reboot-cmds.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "ua-timer.service": {"name": "ua-timer.service", "state": "stopped", "status": "static", "source": "systemd"}, "ubuntu-advantage-cloud-id-shim.service": {"name": "ubuntu-advantage-cloud-id-shim.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "ubuntu-advantage.service": {"name": "ubuntu-advantage.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "udisks2.service": {"name": "udisks2.service", "state": "running", "status": "enabled", "source": "systemd"}, "ufw.service": {"name": "ufw.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "unattended-upgrades.service": {"name": "unattended-upgrades.service", "state": "running", "status": "enabled", "source": "systemd"}, "update-notifier-motd.service": {"name": "update-notifier-motd.service", "state": "stopped", "status": "static", "source": "systemd"}, "upower.service": {"name": "upower.service", "state": "running", "status": "disabled", "source": "systemd"}, "user-runtime-dir@1000.service": {"name": "user-runtime-dir@1000.service", "state": "stopped", "status": "active", "source": "systemd"}, "user@1000.service": {"name": "user@1000.service", "state": "running", "status": "active", "source": "systemd"}, "uuidd.service": {"name": "uuidd.service", "state": "stopped", "status": "indirect", "source": "systemd"}, "vgauth.service": {"name": "vgauth.service", "state": "running", "status": "enabled", "source": "systemd"}, "zfs-mount.service": {"name": "zfs-mount.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "apport-coredump-hook@.service": {"name": "apport-coredump-hook@.service", "state": "unknown", "status": "static", "source": "systemd"}, "apport-forward@.service": {"name": "apport-forward@.service", "state": "unknown", "status": "static", "source": "systemd"}, "apt-news.service": {"name": "apt-news.service", "state": "inactive", "status": "static", "source": "systemd"}, "autovt@.service": {"name": "autovt@.service", "state": "unknown", "status": "alias", "source": "systemd"}, "bolt.service": {"name": "bolt.service", "state": "inactive", "status": "static", "source": "systemd"}, "cloud-config.service": {"name": "cloud-config.service", "state": "inactive", "status": "enabled", "source": "systemd"}, "cloud-final.service": {"name": "cloud-final.service", "state": "inactive", "status": "enabled", "source": "systemd"}, "cloud-init-hotplugd.service": {"name": "cloud-init-hotplugd.service", "state": "inactive", "status": "static", "source": "systemd"}, "cloud-init.service": {"name": "cloud-init.service", "state": "inactive", "status": "enabled", "source": "systemd"}, "console-getty.service": {"name": "console-getty.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "container-getty@.service": {"name": "container-getty@.service", "state": "unknown", "status": "static", "source": "systemd"}, "cryptdisks-early.service": {"name": "cryptdisks-early.service", "state": "inactive", "status": "masked", "source": "systemd"}, "cryptdisks.service": {"name": "cryptdisks.service", "state": "inactive", "status": "masked", "source": "systemd"}, "dbus-org.freedesktop.hostname1.service": {"name": "dbus-org.freedesktop.hostname1.service", "state": "inactive", "status": "alias", "source": "systemd"}, "dbus-org.freedesktop.locale1.service": {"name": "dbus-org.freedesktop.locale1.service", "state": "inactive", "status": "alias", "source": "systemd"}, "dbus-org.freedesktop.login1.service": {"name": "dbus-org.freedesktop.login1.service", "state": "active", "status": "alias", "source": "systemd"}, "dbus-org.freedesktop.ModemManager1.service": {"name": "dbus-org.freedesktop.ModemManager1.service", "state": "active", "status": "alias", "source": "systemd"}, "dbus-org.freedesktop.resolve1.service": {"name": "dbus-org.freedesktop.resolve1.service", "state": "active", "status": "alias", "source": "systemd"}, "dbus-org.freedesktop.thermald.service": {"name": "dbus-org.freedesktop.thermald.service", "state": "inactive", "status": "alias", "source": "systemd"}, "dbus-org.freedesktop.timedate1.service": {"name": "dbus-org.freedesktop.timedate1.service", "state": "inactive", "status": "alias", "source": "systemd"}, "dbus-org.freedesktop.timesync1.service": {"name": "dbus-org.freedesktop.timesync1.service", "state": "active", "status": "alias", "source": "systemd"}, "debug-shell.service": {"name": "debug-shell.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "e2scrub@.service": {"name": "e2scrub@.service", "state": "unknown", "status": "static", "source": "systemd"}, "e2scrub_fail@.service": {"name": "e2scrub_fail@.service", "state": "unknown", "status": "static", "source": "systemd"}, "esm-cache.service": {"name": "esm-cache.service", "state": "inactive", "status": "static", "source": "systemd"}, "friendly-recovery.service": {"name": "friendly-recovery.service", "state": "inactive", "status": "static", "source": "systemd"}, "fwupd-offline-update.service": {"name": "fwupd-offline-update.service", "state": "inactive", "status": "static", "source": "systemd"}, "getty@.service": {"name": "getty@.service", "state": "unknown", "status": "enabled", "source": "systemd"}, "gpu-manager.service": {"name": "gpu-manager.service", "state": "inactive", "status": "enabled", "source": "systemd"}, "grub-initrd-fallback.service": {"name": "grub-initrd-fallback.service", "state": "inactive", "status": "enabled", "source": "systemd"}, "hwclock.service": {"name": "hwclock.service", "state": "inactive", "status": "masked", "source": "systemd"}, "iscsi.service": {"name": "iscsi.service", "state": "inactive", "status": "alias", "source": "systemd"}, "kmod.service": {"name": "kmod.service", "state": "active", "status": "alias", "source": "systemd"}, "lxd-agent.service": {"name": "lxd-agent.service", "state": "inactive", "status": "static", "source": "systemd"}, "lxd-installer@.service": {"name": "lxd-installer@.service", "state": "unknown", "status": "static", "source": "systemd"}, "mdadm-grow-continue@.service": {"name": "mdadm-grow-continue@.service", "state": "unknown", "status": "static", "source": "systemd"}, "mdadm-last-resort@.service": {"name": "mdadm-last-resort@.service", "state": "unknown", "status": "static", "source": "systemd"}, "mdcheck_continue.service": {"name": "mdcheck_continue.service", "state": "inactive", "status": "static", "source": "systemd"}, "mdcheck_start.service": {"name": "mdcheck_start.service", "state": "inactive", "status": "static", "source": "systemd"}, "mdmon@.service": {"name": "mdmon@.service", "state": "unknown", "status": "static", "source": "systemd"}, "mdmonitor-oneshot.service": {"name": "mdmonitor-oneshot.service", "state": "inactive", "status": "static", "source": "systemd"}, "mdmonitor.service": {"name": "mdmonitor.service", "state": "inactive", "status": "static", "source": "systemd"}, "modprobe@.service": {"name": "modprobe@.service", "state": "unknown", "status": "static", "source": "systemd"}, "multipath-tools-boot.service": {"name": "multipath-tools-boot.service", "state": "inactive", "status": "masked", "source": "systemd"}, "multipath-tools.service": {"name": "multipath-tools.service", "state": "active", "status": "alias", "source": "systemd"}, "nftables.service": {"name": "nftables.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "packagekit-offline-update.service": {"name": "packagekit-offline-update.service", "state": "inactive", "status": "static", "source": "systemd"}, "packagekit.service": {"name": "packagekit.service", "state": "inactive", "status": "static", "source": "systemd"}, "pam_namespace.service": {"name": "pam_namespace.service", "state": "inactive", "status": "static", "source": "systemd"}, "plymouth-halt.service": {"name": "plymouth-halt.service", "state": "inactive", "status": "static", "source": "systemd"}, "plymouth-kexec.service": {"name": "plymouth-kexec.service", "state": "inactive", "status": "static", "source": "systemd"}, "plymouth-log.service": {"name": "plymouth-log.service", "state": "active", "status": "alias", "source": "systemd"}, "plymouth-poweroff.service": {"name": "plymouth-poweroff.service", "state": "inactive", "status": "static", "source": "systemd"}, "plymouth-reboot.service": {"name": "plymouth-reboot.service", "state": "inactive", "status": "static", "source": "systemd"}, "plymouth-switch-root-initramfs.service": {"name": "plymouth-switch-root-initramfs.service", "state": "inactive", "status": "static", "source": "systemd"}, "plymouth.service": {"name": "plymouth.service", "state": "active", "status": "alias", "source": "systemd"}, "procps.service": {"name": "procps.service", "state": "active", "status": "alias", "source": "systemd"}, "quotaon.service": {"name": "quotaon.service", "state": "inactive", "status": "static", "source": "systemd"}, "rsync.service": {"name": "rsync.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "screen-cleanup.service": {"name": "screen-cleanup.service", "state": "inactive", "status": "masked", "source": "systemd"}, "serial-getty@.service": {"name": "serial-getty@.service", "state": "unknown", "status": "disabled", "source": "systemd"}, "sudo.service": {"name": "sudo.service", "state": "inactive", "status": "masked", "source": "systemd"}, "syslog.service": {"name": "syslog.service", "state": "active", "status": "alias", "source": "systemd"}, "system-update-cleanup.service": {"name": "system-update-cleanup.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-backlight@.service": {"name": "systemd-backlight@.service", "state": "unknown", "status": "static", "source": "systemd"}, "systemd-boot-check-no-failures.service": {"name": "systemd-boot-check-no-failures.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-confext.service": {"name": "systemd-confext.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-exit.service": {"name": "systemd-exit.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-fsck@.service": {"name": "systemd-fsck@.service", "state": "unknown", "status": "static", "source": "systemd"}, "systemd-growfs-root.service": {"name": "systemd-growfs-root.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-growfs@.service": {"name": "systemd-growfs@.service", "state": "unknown", "status": "static", "source": "systemd"}, "systemd-halt.service": {"name": "systemd-halt.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-hostnamed.service": {"name": "systemd-hostnamed.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-journald@.service": {"name": "systemd-journald@.service", "state": "unknown", "status": "static", "source": "systemd"}, "systemd-kexec.service": {"name": "systemd-kexec.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-localed.service": {"name": "systemd-localed.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-network-generator.service": {"name": "systemd-network-generator.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-networkd-wait-online@.service": {"name": "systemd-networkd-wait-online@.service", "state": "unknown", "status": "disabled", "source": "systemd"}, "systemd-pcrextend@.service": {"name": "systemd-pcrextend@.service", "state": "unknown", "status": "static", "source": "systemd"}, "systemd-pcrfs-root.service": {"name": "systemd-pcrfs-root.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-pcrfs@.service": {"name": "systemd-pcrfs@.service", "state": "unknown", "status": "static", "source": "systemd"}, "systemd-pcrlock-file-system.service": {"name": "systemd-pcrlock-file-system.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-pcrlock-firmware-code.service": {"name": "systemd-pcrlock-firmware-code.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-pcrlock-firmware-config.service": {"name": "systemd-pcrlock-firmware-config.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-pcrlock-machine-id.service": {"name": "systemd-pcrlock-machine-id.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-pcrlock-make-policy.service": {"name": "systemd-pcrlock-make-policy.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-pcrlock-secureboot-authority.service": {"name": "systemd-pcrlock-secureboot-authority.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-pcrlock-secureboot-policy.service": {"name": "systemd-pcrlock-secureboot-policy.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-poweroff.service": {"name": "systemd-poweroff.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-reboot.service": {"name": "systemd-reboot.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-storagetm.service": {"name": "systemd-storagetm.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-sysext@.service": {"name": "systemd-sysext@.service", "state": "unknown", "status": "static", "source": "systemd"}, "systemd-sysupdate-reboot.service": {"name": "systemd-sysupdate-reboot.service", "state": "inactive", "status": "indirect", "source": "systemd"}, "systemd-sysupdate.service": {"name": "systemd-sysupdate.service", "state": "inactive", "status": "indirect", "source": "systemd"}, "systemd-time-wait-sync.service": {"name": "systemd-time-wait-sync.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-timedated.service": {"name": "systemd-timedated.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-volatile-root.service": {"name": "systemd-volatile-root.service", "state": "inactive", "status": "static", "source": "systemd"}, "udev.service": {"name": "udev.service", "state": "active", "status": "alias", "source": "systemd"}, "update-notifier-download.service": {"name": "update-notifier-download.service", "state": "inactive", "status": "static", "source": "systemd"}, "usb_modeswitch@.service": {"name": "usb_modeswitch@.service", "state": "unknown", "status": "static", "source": "systemd"}, "usbmuxd.service": {"name": "usbmuxd.service", "state": "inactive", "status": "static", "source": "systemd"}, "user-runtime-dir@.service": {"name": "user-runtime-dir@.service", "state": "unknown", "status": "static", "source": "systemd"}, "user@.service": {"name": "user@.service", "state": "unknown", "status": "static", "source": "systemd"}, "vmtoolsd.service": {"name": "vmtoolsd.service", "state": "active", "status": "alias", "source": "systemd"}, "x11-common.service": {"name": "x11-common.service", "state": "inactive", "status": "masked", "source": "systemd"}, "xfs_scrub@.service": {"name": "xfs_scrub@.service", "state": "unknown", "status": "static", "source": "systemd"}, "xfs_scrub_all.service": {"name": "xfs_scrub_all.service", "state": "inactive", "status": "static", "source": "systemd"}, "xfs_scrub_fail@.service": {"name": "xfs_scrub_fail@.service", "state": "unknown", "status": "static", "source": "systemd"}}}, "invocation": {"module_args": {}}}\n', b'')
2025-10-25 17:45:02,499 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Populate service facts] ******************************************
2025-10-25 17:45:02,620 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  ansible_facts:
    services:
      ModemManager.service:
        name: ModemManager.service
        source: systemd
        state: running
        status: enabled
      NetworkManager.service:
        name: NetworkManager.service
        source: systemd
        state: stopped
        status: not-found
      apparmor:
        name: apparmor
        source: sysv
        state: running
      apparmor.service:
        name: apparmor.service
        source: systemd
        state: stopped
        status: enabled
      apport:
        name: apport
        source: sysv
        state: running
      apport-autoreport.service:
        name: apport-autoreport.service
        source: systemd
        state: stopped
        status: static
      apport-coredump-hook@.service:
        name: apport-coredump-hook@.service
        source: systemd
        state: unknown
        status: static
      apport-forward@.service:
        name: apport-forward@.service
        source: systemd
        state: unknown
        status: static
      apport.service:
        name: apport.service
        source: systemd
        state: stopped
        status: enabled
      apt-daily-upgrade.service:
        name: apt-daily-upgrade.service
        source: systemd
        state: stopped
        status: static
      apt-daily.service:
        name: apt-daily.service
        source: systemd
        state: stopped
        status: static
      apt-news.service:
        name: apt-news.service
        source: systemd
        state: inactive
        status: static
      auditd.service:
        name: auditd.service
        source: systemd
        state: stopped
        status: not-found
      autovt@.service:
        name: autovt@.service
        source: systemd
        state: unknown
        status: alias
      blk-availability.service:
        name: blk-availability.service
        source: systemd
        state: stopped
        status: enabled
      bolt.service:
        name: bolt.service
        source: systemd
        state: inactive
        status: static
      cloud-config.service:
        name: cloud-config.service
        source: systemd
        state: inactive
        status: enabled
      cloud-final.service:
        name: cloud-final.service
        source: systemd
        state: inactive
        status: enabled
      cloud-init-hotplugd.service:
        name: cloud-init-hotplugd.service
        source: systemd
        state: inactive
        status: static
      cloud-init-local.service:
        name: cloud-init-local.service
        source: systemd
        state: stopped
        status: enabled
      cloud-init.service:
        name: cloud-init.service
        source: systemd
        state: inactive
        status: enabled
      connman.service:
        name: connman.service
        source: systemd
        state: stopped
        status: not-found
      console-getty.service:
        name: console-getty.service
        source: systemd
        state: inactive
        status: disabled
      console-screen.service:
        name: console-screen.service
        source: systemd
        state: stopped
        status: not-found
      console-setup.service:
        name: console-setup.service
        source: systemd
        state: stopped
        status: enabled
      console-setup.sh:
        name: console-setup.sh
        source: sysv
        state: stopped
      container-getty@.service:
        name: container-getty@.service
        source: systemd
        state: unknown
        status: static
      cron:
        name: cron
        source: sysv
        state: running
      cron.service:
        name: cron.service
        source: systemd
        state: running
        status: enabled
      cryptdisks:
        name: cryptdisks
        source: sysv
        state: stopped
      cryptdisks-early:
        name: cryptdisks-early
        source: sysv
        state: stopped
      cryptdisks-early.service:
        name: cryptdisks-early.service
        source: systemd
        state: inactive
        status: masked
      cryptdisks.service:
        name: cryptdisks.service
        source: systemd
        state: inactive
        status: masked
      dbus:
        name: dbus
        source: sysv
        state: running
      dbus-org.freedesktop.ModemManager1.service:
        name: dbus-org.freedesktop.ModemManager1.service
        source: systemd
        state: active
        status: alias
      dbus-org.freedesktop.hostname1.service:
        name: dbus-org.freedesktop.hostname1.service
        source: systemd
        state: inactive
        status: alias
      dbus-org.freedesktop.locale1.service:
        name: dbus-org.freedesktop.locale1.service
        source: systemd
        state: inactive
        status: alias
      dbus-org.freedesktop.login1.service:
        name: dbus-org.freedesktop.login1.service
        source: systemd
        state: active
        status: alias
      dbus-org.freedesktop.resolve1.service:
        name: dbus-org.freedesktop.resolve1.service
        source: systemd
        state: active
        status: alias
      dbus-org.freedesktop.thermald.service:
        name: dbus-org.freedesktop.thermald.service
        source: systemd
        state: inactive
        status: alias
      dbus-org.freedesktop.timedate1.service:
        name: dbus-org.freedesktop.timedate1.service
        source: systemd
        state: inactive
        status: alias
      dbus-org.freedesktop.timesync1.service:
        name: dbus-org.freedesktop.timesync1.service
        source: systemd
        state: active
        status: alias
      dbus.service:
        name: dbus.service
        source: systemd
        state: running
        status: static
      debug-shell.service:
        name: debug-shell.service
        source: systemd
        state: inactive
        status: disabled
      display-manager.service:
        name: display-manager.service
        source: systemd
        state: stopped
        status: not-found
      dm-event.service:
        name: dm-event.service
        source: systemd
        state: stopped
        status: static
      dmesg.service:
        name: dmesg.service
        source: systemd
        state: stopped
        status: enabled
      dpkg-db-backup.service:
        name: dpkg-db-backup.service
        source: systemd
        state: stopped
        status: static
      e2scrub@.service:
        name: e2scrub@.service
        source: systemd
        state: unknown
        status: static
      e2scrub_all.service:
        name: e2scrub_all.service
        source: systemd
        state: stopped
        status: static
      e2scrub_fail@.service:
        name: e2scrub_fail@.service
        source: systemd
        state: unknown
        status: static
      e2scrub_reap.service:
        name: e2scrub_reap.service
        source: systemd
        state: stopped
        status: enabled
      emergency.service:
        name: emergency.service
        source: systemd
        state: stopped
        status: static
      esm-cache.service:
        name: esm-cache.service
        source: systemd
        state: inactive
        status: static
      fcoe.service:
        name: fcoe.service
        source: systemd
        state: stopped
        status: not-found
      finalrd.service:
        name: finalrd.service
        source: systemd
        state: stopped
        status: enabled
      friendly-recovery.service:
        name: friendly-recovery.service
        source: systemd
        state: inactive
        status: static
      fstrim.service:
        name: fstrim.service
        source: systemd
        state: stopped
        status: static
      fwupd-offline-update.service:
        name: fwupd-offline-update.service
        source: systemd
        state: inactive
        status: static
      fwupd-refresh.service:
        name: fwupd-refresh.service
        source: systemd
        state: stopped
        status: failed
      fwupd.service:
        name: fwupd.service
        source: systemd
        state: running
        status: static
      getty-static.service:
        name: getty-static.service
        source: systemd
        state: stopped
        status: static
      getty@.service:
        name: getty@.service
        source: systemd
        state: unknown
        status: enabled
      getty@tty1.service:
        name: getty@tty1.service
        source: systemd
        state: running
        status: active
      gpu-manager.service:
        name: gpu-manager.service
        source: systemd
        state: inactive
        status: enabled
      grub-common:
        name: grub-common
        source: sysv
        state: stopped
      grub-common.service:
        name: grub-common.service
        source: systemd
        state: stopped
        status: enabled
      grub-initrd-fallback.service:
        name: grub-initrd-fallback.service
        source: systemd
        state: inactive
        status: enabled
      hv_kvp_daemon.service:
        name: hv_kvp_daemon.service
        source: systemd
        state: stopped
        status: not-found
      hwclock.service:
        name: hwclock.service
        source: systemd
        state: inactive
        status: masked
      initrd-cleanup.service:
        name: initrd-cleanup.service
        source: systemd
        state: stopped
        status: static
      initrd-parse-etc.service:
        name: initrd-parse-etc.service
        source: systemd
        state: stopped
        status: static
      initrd-switch-root.service:
        name: initrd-switch-root.service
        source: systemd
        state: stopped
        status: static
      initrd-udevadm-cleanup-db.service:
        name: initrd-udevadm-cleanup-db.service
        source: systemd
        state: stopped
        status: static
      iscsi-shutdown.service:
        name: iscsi-shutdown.service
        source: systemd
        state: stopped
        status: not-found
      iscsi.service:
        name: iscsi.service
        source: systemd
        state: inactive
        status: alias
      iscsid:
        name: iscsid
        source: sysv
        state: stopped
      iscsid.service:
        name: iscsid.service
        source: systemd
        state: stopped
        status: disabled
      kbd.service:
        name: kbd.service
        source: systemd
        state: stopped
        status: not-found
      keyboard-setup.service:
        name: keyboard-setup.service
        source: systemd
        state: stopped
        status: enabled
      keyboard-setup.sh:
        name: keyboard-setup.sh
        source: sysv
        state: stopped
      kmod:
        name: kmod
        source: sysv
        state: running
      kmod-static-nodes.service:
        name: kmod-static-nodes.service
        source: systemd
        state: stopped
        status: static
      kmod.service:
        name: kmod.service
        source: systemd
        state: active
        status: alias
      ldconfig.service:
        name: ldconfig.service
        source: systemd
        state: stopped
        status: static
      loaded:
        name: loaded
        source: systemd
        state: stopped
        status: failed
      logrotate.service:
        name: logrotate.service
        source: systemd
        state: stopped
        status: static
      lvm2-activation-early.service:
        name: lvm2-activation-early.service
        source: systemd
        state: stopped
        status: not-found
      lvm2-lvmpolld.service:
        name: lvm2-lvmpolld.service
        source: systemd
        state: stopped
        status: static
      lvm2-monitor.service:
        name: lvm2-monitor.service
        source: systemd
        state: stopped
        status: enabled
      lxd-agent.service:
        name: lxd-agent.service
        source: systemd
        state: inactive
        status: static
      lxd-installer@.service:
        name: lxd-installer@.service
        source: systemd
        state: unknown
        status: static
      man-db.service:
        name: man-db.service
        source: systemd
        state: stopped
        status: static
      mdadm-grow-continue@.service:
        name: mdadm-grow-continue@.service
        source: systemd
        state: unknown
        status: static
      mdadm-last-resort@.service:
        name: mdadm-last-resort@.service
        source: systemd
        state: unknown
        status: static
      mdcheck_continue.service:
        name: mdcheck_continue.service
        source: systemd
        state: inactive
        status: static
      mdcheck_start.service:
        name: mdcheck_start.service
        source: systemd
        state: inactive
        status: static
      mdmon@.service:
        name: mdmon@.service
        source: systemd
        state: unknown
        status: static
      mdmonitor-oneshot.service:
        name: mdmonitor-oneshot.service
        source: systemd
        state: inactive
        status: static
      mdmonitor.service:
        name: mdmonitor.service
        source: systemd
        state: inactive
        status: static
      modprobe@.service:
        name: modprobe@.service
        source: systemd
        state: unknown
        status: static
      modprobe@configfs.service:
        name: modprobe@configfs.service
        source: systemd
        state: stopped
        status: inactive
      modprobe@dm_mod.service:
        name: modprobe@dm_mod.service
        source: systemd
        state: stopped
        status: inactive
      modprobe@drm.service:
        name: modprobe@drm.service
        source: systemd
        state: stopped
        status: inactive
      modprobe@efi_pstore.service:
        name: modprobe@efi_pstore.service
        source: systemd
        state: stopped
        status: inactive
      modprobe@fuse.service:
        name: modprobe@fuse.service
        source: systemd
        state: stopped
        status: inactive
      modprobe@loop.service:
        name: modprobe@loop.service
        source: systemd
        state: stopped
        status: inactive
      motd-news.service:
        name: motd-news.service
        source: systemd
        state: stopped
        status: static
      multipath-tools-boot.service:
        name: multipath-tools-boot.service
        source: systemd
        state: inactive
        status: masked
      multipath-tools.service:
        name: multipath-tools.service
        source: systemd
        state: active
        status: alias
      multipathd.service:
        name: multipathd.service
        source: systemd
        state: running
        status: enabled
      netplan-ovs-cleanup.service:
        name: netplan-ovs-cleanup.service
        source: systemd
        state: stopped
        status: enabled-runtime
      networkd-dispatcher.service:
        name: networkd-dispatcher.service
        source: systemd
        state: stopped
        status: enabled
      nftables.service:
        name: nftables.service
        source: systemd
        state: inactive
        status: disabled
      open-iscsi:
        name: open-iscsi
        source: sysv
        state: stopped
      open-iscsi.service:
        name: open-iscsi.service
        source: systemd
        state: stopped
        status: enabled
      open-vm-tools:
        name: open-vm-tools
        source: sysv
        state: running
      open-vm-tools.service:
        name: open-vm-tools.service
        source: systemd
        state: running
        status: enabled
      ovsdb-server.service:
        name: ovsdb-server.service
        source: systemd
        state: stopped
        status: not-found
      packagekit-offline-update.service:
        name: packagekit-offline-update.service
        source: systemd
        state: inactive
        status: static
      packagekit.service:
        name: packagekit.service
        source: systemd
        state: inactive
        status: static
      pam_namespace.service:
        name: pam_namespace.service
        source: systemd
        state: inactive
        status: static
      plymouth:
        name: plymouth
        source: sysv
        state: running
      plymouth-halt.service:
        name: plymouth-halt.service
        source: systemd
        state: inactive
        status: static
      plymouth-kexec.service:
        name: plymouth-kexec.service
        source: systemd
        state: inactive
        status: static
      plymouth-log:
        name: plymouth-log
        source: sysv
        state: running
      plymouth-log.service:
        name: plymouth-log.service
        source: systemd
        state: active
        status: alias
      plymouth-poweroff.service:
        name: plymouth-poweroff.service
        source: systemd
        state: inactive
        status: static
      plymouth-quit-wait.service:
        name: plymouth-quit-wait.service
        source: systemd
        state: stopped
        status: static
      plymouth-quit.service:
        name: plymouth-quit.service
        source: systemd
        state: stopped
        status: static
      plymouth-read-write.service:
        name: plymouth-read-write.service
        source: systemd
        state: stopped
        status: static
      plymouth-reboot.service:
        name: plymouth-reboot.service
        source: systemd
        state: inactive
        status: static
      plymouth-start.service:
        name: plymouth-start.service
        source: systemd
        state: stopped
        status: static
      plymouth-switch-root-initramfs.service:
        name: plymouth-switch-root-initramfs.service
        source: systemd
        state: inactive
        status: static
      plymouth-switch-root.service:
        name: plymouth-switch-root.service
        source: systemd
        state: stopped
        status: static
      plymouth.service:
        name: plymouth.service
        source: systemd
        state: active
        status: alias
      polkit.service:
        name: polkit.service
        source: systemd
        state: running
        status: static
      pollinate.service:
        name: pollinate.service
        source: systemd
        state: stopped
        status: enabled
      procps:
        name: procps
        source: sysv
        state: running
      procps.service:
        name: procps.service
        source: systemd
        state: active
        status: alias
      quotaon.service:
        name: quotaon.service
        source: systemd
        state: inactive
        status: static
      rbdmap.service:
        name: rbdmap.service
        source: systemd
        state: stopped
        status: not-found
      rc-local.service:
        name: rc-local.service
        source: systemd
        state: stopped
        status: static
      rescue.service:
        name: rescue.service
        source: systemd
        state: stopped
        status: static
      rke2-agent.service:
        name: rke2-agent.service
        source: systemd
        state: stopped
        status: masked
      rke2-server.service:
        name: rke2-server.service
        source: systemd
        state: running
        status: enabled
      rsync:
        name: rsync
        source: sysv
        state: stopped
      rsync.service:
        name: rsync.service
        source: systemd
        state: inactive
        status: disabled
      rsyslog.service:
        name: rsyslog.service
        source: systemd
        state: running
        status: enabled
      screen-cleanup:
        name: screen-cleanup
        source: sysv
        state: stopped
      screen-cleanup.service:
        name: screen-cleanup.service
        source: systemd
        state: inactive
        status: masked
      secureboot-db.service:
        name: secureboot-db.service
        source: systemd
        state: stopped
        status: enabled
      serial-getty@.service:
        name: serial-getty@.service
        source: systemd
        state: unknown
        status: disabled
      setvtrgb.service:
        name: setvtrgb.service
        source: systemd
        state: stopped
        status: enabled
      snapd.apparmor.service:
        name: snapd.apparmor.service
        source: systemd
        state: stopped
        status: enabled
      snapd.autoimport.service:
        name: snapd.autoimport.service
        source: systemd
        state: stopped
        status: enabled
      snapd.core-fixup.service:
        name: snapd.core-fixup.service
        source: systemd
        state: stopped
        status: enabled
      snapd.failure.service:
        name: snapd.failure.service
        source: systemd
        state: stopped
        status: static
      snapd.recovery-chooser-trigger.service:
        name: snapd.recovery-chooser-trigger.service
        source: systemd
        state: stopped
        status: enabled
      snapd.seeded.service:
        name: snapd.seeded.service
        source: systemd
        state: stopped
        status: enabled
      snapd.service:
        name: snapd.service
        source: systemd
        state: stopped
        status: enabled
      snapd.snap-repair.service:
        name: snapd.snap-repair.service
        source: systemd
        state: stopped
        status: static
      snapd.system-shutdown.service:
        name: snapd.system-shutdown.service
        source: systemd
        state: stopped
        status: enabled
      ssh:
        name: ssh
        source: sysv
        state: running
      ssh.service:
        name: ssh.service
        source: systemd
        state: running
        status: disabled
      sudo.service:
        name: sudo.service
        source: systemd
        state: inactive
        status: masked
      syslog.service:
        name: syslog.service
        source: systemd
        state: active
        status: alias
      sysstat:
        name: sysstat
        source: sysv
        state: running
      sysstat-collect.service:
        name: sysstat-collect.service
        source: systemd
        state: stopped
        status: static
      sysstat-summary.service:
        name: sysstat-summary.service
        source: systemd
        state: stopped
        status: static
      sysstat.service:
        name: sysstat.service
        source: systemd
        state: stopped
        status: enabled
      system-update-cleanup.service:
        name: system-update-cleanup.service
        source: systemd
        state: inactive
        status: static
      systemd-ask-password-console.service:
        name: systemd-ask-password-console.service
        source: systemd
        state: stopped
        status: static
      systemd-ask-password-plymouth.service:
        name: systemd-ask-password-plymouth.service
        source: systemd
        state: stopped
        status: static
      systemd-ask-password-wall.service:
        name: systemd-ask-password-wall.service
        source: systemd
        state: stopped
        status: static
      systemd-backlight@.service:
        name: systemd-backlight@.service
        source: systemd
        state: unknown
        status: static
      systemd-battery-check.service:
        name: systemd-battery-check.service
        source: systemd
        state: stopped
        status: static
      systemd-binfmt.service:
        name: systemd-binfmt.service
        source: systemd
        state: stopped
        status: static
      systemd-boot-check-no-failures.service:
        name: systemd-boot-check-no-failures.service
        source: systemd
        state: inactive
        status: disabled
      systemd-bsod.service:
        name: systemd-bsod.service
        source: systemd
        state: stopped
        status: static
      systemd-confext.service:
        name: systemd-confext.service
        source: systemd
        state: inactive
        status: disabled
      systemd-exit.service:
        name: systemd-exit.service
        source: systemd
        state: inactive
        status: static
      systemd-firstboot.service:
        name: systemd-firstboot.service
        source: systemd
        state: stopped
        status: static
      systemd-fsck-root.service:
        name: systemd-fsck-root.service
        source: systemd
        state: stopped
        status: enabled-runtime
      systemd-fsck@.service:
        name: systemd-fsck@.service
        source: systemd
        state: unknown
        status: static
      systemd-fsck@dev-disk-by\x2duuid-9bb05da7\x2d85f0\x2d4338\x2d9b8f\x2d06ca0d639ac6.service:
        name: systemd-fsck@dev-disk-by\x2duuid-9bb05da7\x2d85f0\x2d4338\x2d9b8f\x2d06ca0d639ac6.service
        source: systemd
        state: stopped
        status: active
      systemd-fsckd.service:
        name: systemd-fsckd.service
        source: systemd
        state: stopped
        status: static
      systemd-growfs-root.service:
        name: systemd-growfs-root.service
        source: systemd
        state: inactive
        status: static
      systemd-growfs@.service:
        name: systemd-growfs@.service
        source: systemd
        state: unknown
        status: static
      systemd-halt.service:
        name: systemd-halt.service
        source: systemd
        state: inactive
        status: static
      systemd-hibernate-resume.service:
        name: systemd-hibernate-resume.service
        source: systemd
        state: stopped
        status: static
      systemd-hibernate.service:
        name: systemd-hibernate.service
        source: systemd
        state: stopped
        status: static
      systemd-hostnamed.service:
        name: systemd-hostnamed.service
        source: systemd
        state: inactive
        status: static
      systemd-hwdb-update.service:
        name: systemd-hwdb-update.service
        source: systemd
        state: stopped
        status: static
      systemd-hybrid-sleep.service:
        name: systemd-hybrid-sleep.service
        source: systemd
        state: stopped
        status: static
      systemd-initctl.service:
        name: systemd-initctl.service
        source: systemd
        state: stopped
        status: static
      systemd-journal-catalog-update.service:
        name: systemd-journal-catalog-update.service
        source: systemd
        state: stopped
        status: static
      systemd-journal-flush.service:
        name: systemd-journal-flush.service
        source: systemd
        state: stopped
        status: static
      systemd-journald.service:
        name: systemd-journald.service
        source: systemd
        state: running
        status: static
      systemd-journald@.service:
        name: systemd-journald@.service
        source: systemd
        state: unknown
        status: static
      systemd-kexec.service:
        name: systemd-kexec.service
        source: systemd
        state: inactive
        status: static
      systemd-localed.service:
        name: systemd-localed.service
        source: systemd
        state: inactive
        status: static
      systemd-logind.service:
        name: systemd-logind.service
        source: systemd
        state: running
        status: static
      systemd-machine-id-commit.service:
        name: systemd-machine-id-commit.service
        source: systemd
        state: stopped
        status: static
      systemd-modules-load.service:
        name: systemd-modules-load.service
        source: systemd
        state: stopped
        status: static
      systemd-network-generator.service:
        name: systemd-network-generator.service
        source: systemd
        state: inactive
        status: disabled
      systemd-networkd-wait-online.service:
        name: systemd-networkd-wait-online.service
        source: systemd
        state: stopped
        status: enabled
      systemd-networkd-wait-online@.service:
        name: systemd-networkd-wait-online@.service
        source: systemd
        state: unknown
        status: disabled
      systemd-networkd.service:
        name: systemd-networkd.service
        source: systemd
        state: running
        status: enabled
      systemd-oomd.service:
        name: systemd-oomd.service
        source: systemd
        state: stopped
        status: not-found
      systemd-pcrextend@.service:
        name: systemd-pcrextend@.service
        source: systemd
        state: unknown
        status: static
      systemd-pcrfs-root.service:
        name: systemd-pcrfs-root.service
        source: systemd
        state: inactive
        status: static
      systemd-pcrfs@.service:
        name: systemd-pcrfs@.service
        source: systemd
        state: unknown
        status: static
      systemd-pcrlock-file-system.service:
        name: systemd-pcrlock-file-system.service
        source: systemd
        state: inactive
        status: disabled
      systemd-pcrlock-firmware-code.service:
        name: systemd-pcrlock-firmware-code.service
        source: systemd
        state: inactive
        status: disabled
      systemd-pcrlock-firmware-config.service:
        name: systemd-pcrlock-firmware-config.service
        source: systemd
        state: inactive
        status: disabled
      systemd-pcrlock-machine-id.service:
        name: systemd-pcrlock-machine-id.service
        source: systemd
        state: inactive
        status: disabled
      systemd-pcrlock-make-policy.service:
        name: systemd-pcrlock-make-policy.service
        source: systemd
        state: inactive
        status: disabled
      systemd-pcrlock-secureboot-authority.service:
        name: systemd-pcrlock-secureboot-authority.service
        source: systemd
        state: inactive
        status: disabled
      systemd-pcrlock-secureboot-policy.service:
        name: systemd-pcrlock-secureboot-policy.service
        source: systemd
        state: inactive
        status: disabled
      systemd-pcrmachine.service:
        name: systemd-pcrmachine.service
        source: systemd
        state: stopped
        status: static
      systemd-pcrphase-initrd.service:
        name: systemd-pcrphase-initrd.service
        source: systemd
        state: stopped
        status: static
      systemd-pcrphase-sysinit.service:
        name: systemd-pcrphase-sysinit.service
        source: systemd
        state: stopped
        status: static
      systemd-pcrphase.service:
        name: systemd-pcrphase.service
        source: systemd
        state: stopped
        status: static
      systemd-poweroff.service:
        name: systemd-poweroff.service
        source: systemd
        state: inactive
        status: static
      systemd-pstore.service:
        name: systemd-pstore.service
        source: systemd
        state: stopped
        status: enabled
      systemd-quotacheck.service:
        name: systemd-quotacheck.service
        source: systemd
        state: stopped
        status: static
      systemd-random-seed.service:
        name: systemd-random-seed.service
        source: systemd
        state: stopped
        status: static
      systemd-reboot.service:
        name: systemd-reboot.service
        source: systemd
        state: inactive
        status: static
      systemd-remount-fs.service:
        name: systemd-remount-fs.service
        source: systemd
        state: stopped
        status: enabled-runtime
      systemd-repart.service:
        name: systemd-repart.service
        source: systemd
        state: stopped
        status: static
      systemd-resolved.service:
        name: systemd-resolved.service
        source: systemd
        state: running
        status: enabled
      systemd-rfkill.service:
        name: systemd-rfkill.service
        source: systemd
        state: stopped
        status: static
      systemd-soft-reboot.service:
        name: systemd-soft-reboot.service
        source: systemd
        state: stopped
        status: static
      systemd-storagetm.service:
        name: systemd-storagetm.service
        source: systemd
        state: inactive
        status: static
      systemd-suspend-then-hibernate.service:
        name: systemd-suspend-then-hibernate.service
        source: systemd
        state: stopped
        status: static
      systemd-suspend.service:
        name: systemd-suspend.service
        source: systemd
        state: stopped
        status: static
      systemd-sysctl.service:
        name: systemd-sysctl.service
        source: systemd
        state: stopped
        status: static
      systemd-sysext.service:
        name: systemd-sysext.service
        source: systemd
        state: stopped
        status: disabled
      systemd-sysext@.service:
        name: systemd-sysext@.service
        source: systemd
        state: unknown
        status: static
      systemd-sysupdate-reboot.service:
        name: systemd-sysupdate-reboot.service
        source: systemd
        state: inactive
        status: indirect
      systemd-sysupdate.service:
        name: systemd-sysupdate.service
        source: systemd
        state: inactive
        status: indirect
      systemd-sysusers.service:
        name: systemd-sysusers.service
        source: systemd
        state: stopped
        status: static
      systemd-time-wait-sync.service:
        name: systemd-time-wait-sync.service
        source: systemd
        state: inactive
        status: disabled
      systemd-timedated.service:
        name: systemd-timedated.service
        source: systemd
        state: inactive
        status: static
      systemd-timesyncd.service:
        name: systemd-timesyncd.service
        source: systemd
        state: running
        status: enabled
      systemd-tmpfiles-clean.service:
        name: systemd-tmpfiles-clean.service
        source: systemd
        state: stopped
        status: static
      systemd-tmpfiles-setup-dev-early.service:
        name: systemd-tmpfiles-setup-dev-early.service
        source: systemd
        state: stopped
        status: static
      systemd-tmpfiles-setup-dev.service:
        name: systemd-tmpfiles-setup-dev.service
        source: systemd
        state: stopped
        status: static
      systemd-tmpfiles-setup.service:
        name: systemd-tmpfiles-setup.service
        source: systemd
        state: stopped
        status: static
      systemd-tpm2-setup-early.service:
        name: systemd-tpm2-setup-early.service
        source: systemd
        state: stopped
        status: static
      systemd-tpm2-setup.service:
        name: systemd-tpm2-setup.service
        source: systemd
        state: stopped
        status: static
      systemd-udev-settle.service:
        name: systemd-udev-settle.service
        source: systemd
        state: stopped
        status: static
      systemd-udev-trigger.service:
        name: systemd-udev-trigger.service
        source: systemd
        state: stopped
        status: static
      systemd-udevd.service:
        name: systemd-udevd.service
        source: systemd
        state: running
        status: static
      systemd-update-done.service:
        name: systemd-update-done.service
        source: systemd
        state: stopped
        status: static
      systemd-update-utmp-runlevel.service:
        name: systemd-update-utmp-runlevel.service
        source: systemd
        state: stopped
        status: static
      systemd-update-utmp.service:
        name: systemd-update-utmp.service
        source: systemd
        state: stopped
        status: static
      systemd-user-sessions.service:
        name: systemd-user-sessions.service
        source: systemd
        state: stopped
        status: static
      systemd-vconsole-setup.service:
        name: systemd-vconsole-setup.service
        source: systemd
        state: stopped
        status: not-found
      systemd-volatile-root.service:
        name: systemd-volatile-root.service
        source: systemd
        state: inactive
        status: static
      thermald.service:
        name: thermald.service
        source: systemd
        state: stopped
        status: enabled
      tpm-udev.service:
        name: tpm-udev.service
        source: systemd
        state: stopped
        status: static
      ua-auto-attach.service:
        name: ua-auto-attach.service
        source: systemd
        state: stopped
        status: not-found
      ua-reboot-cmds.service:
        name: ua-reboot-cmds.service
        source: systemd
        state: stopped
        status: enabled
      ua-timer.service:
        name: ua-timer.service
        source: systemd
        state: stopped
        status: static
      ubuntu-advantage-cloud-id-shim.service:
        name: ubuntu-advantage-cloud-id-shim.service
        source: systemd
        state: stopped
        status: not-found
      ubuntu-advantage.service:
        name: ubuntu-advantage.service
        source: systemd
        state: stopped
        status: enabled
      udev.service:
        name: udev.service
        source: systemd
        state: active
        status: alias
      udisks2.service:
        name: udisks2.service
        source: systemd
        state: running
        status: enabled
      ufw:
        name: ufw
        source: sysv
        state: running
      ufw.service:
        name: ufw.service
        source: systemd
        state: stopped
        status: enabled
      unattended-upgrades:
        name: unattended-upgrades
        source: sysv
        state: running
      unattended-upgrades.service:
        name: unattended-upgrades.service
        source: systemd
        state: running
        status: enabled
      update-notifier-download.service:
        name: update-notifier-download.service
        source: systemd
        state: inactive
        status: static
      update-notifier-motd.service:
        name: update-notifier-motd.service
        source: systemd
        state: stopped
        status: static
      upower.service:
        name: upower.service
        source: systemd
        state: running
        status: disabled
      usb_modeswitch@.service:
        name: usb_modeswitch@.service
        source: systemd
        state: unknown
        status: static
      usbmuxd.service:
        name: usbmuxd.service
        source: systemd
        state: inactive
        status: static
      user-runtime-dir@.service:
        name: user-runtime-dir@.service
        source: systemd
        state: unknown
        status: static
      user-runtime-dir@1000.service:
        name: user-runtime-dir@1000.service
        source: systemd
        state: stopped
        status: active
      user@.service:
        name: user@.service
        source: systemd
        state: unknown
        status: static
      user@1000.service:
        name: user@1000.service
        source: systemd
        state: running
        status: active
      uuidd:
        name: uuidd
        source: sysv
        state: stopped
      uuidd.service:
        name: uuidd.service
        source: systemd
        state: stopped
        status: indirect
      vgauth.service:
        name: vgauth.service
        source: systemd
        state: running
        status: enabled
      vmtoolsd.service:
        name: vmtoolsd.service
        source: systemd
        state: active
        status: alias
      x11-common.service:
        name: x11-common.service
        source: systemd
        state: inactive
        status: masked
      xfs_scrub@.service:
        name: xfs_scrub@.service
        source: systemd
        state: unknown
        status: static
      xfs_scrub_all.service:
        name: xfs_scrub_all.service
        source: systemd
        state: inactive
        status: static
      xfs_scrub_fail@.service:
        name: xfs_scrub_fail@.service
        source: systemd
        state: unknown
        status: static
      zfs-mount.service:
        name: zfs-mount.service
        source: systemd
        state: stopped
        status: not-found
  invocation:
    module_args: {}
2025-10-25 17:45:02,651 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/stat.py
2025-10-25 17:45:02,652 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:45:02,652 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:02,653 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=mqvmiewdmguuxjggdmvowslffddgilvs] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-mqvmiewdmguuxjggdmvowslffddgilvs ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:45:02,692 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:02,961 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "stat": {"exists": true, "path": "/usr/local", "mode": "0755", "isdir": true, "ischr": false, "isblk": false, "isreg": false, "isfifo": false, "islnk": false, "issock": false, "uid": 0, "gid": 0, "size": 114, "inode": 281, "dev": 64512, "nlink": 10, "atime": 1761299421.0354981, "mtime": 1761388284.1710286, "ctime": 1761388284.1710286, "wusr": true, "rusr": true, "xusr": true, "wgrp": false, "rgrp": true, "xgrp": true, "woth": false, "roth": true, "xoth": true, "isuid": false, "isgid": false, "blocks": 0, "block_size": 4096, "device_type": 0, "readable": true, "writeable": true, "executable": true, "pw_name": "root", "gr_name": "root", "mimetype": "inode/directory", "charset": "binary", "version": "1187673614", "attributes": [], "attr_flags": ""}, "invocation": {"module_args": {"path": "/usr/local", "follow": false, "get_checksum": true, "get_mime": true, "get_attributes": true, "checksum_algorithm": "sha1"}}}\n', b'')
2025-10-25 17:45:02,964 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Get stats of the FS object] **************************************
2025-10-25 17:45:02,968 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  invocation:
    module_args:
      checksum_algorithm: sha1
      follow: false
      get_attributes: true
      get_checksum: true
      get_mime: true
      path: /usr/local
  stat:
    atime: 1761299421.0354981
    attr_flags: ''
    attributes: []
    block_size: 4096
    blocks: 0
    charset: binary
    ctime: 1761388284.1710286
    dev: 64512
    device_type: 0
    executable: true
    exists: true
    gid: 0
    gr_name: root
    inode: 281
    isblk: false
    ischr: false
    isdir: true
    isfifo: false
    isgid: false
    islnk: false
    isreg: false
    issock: false
    isuid: false
    mimetype: inode/directory
    mode: '0755'
    mtime: 1761388284.1710286
    nlink: 10
    path: /usr/local
    pw_name: root
    readable: true
    rgrp: true
    roth: true
    rusr: true
    size: 114
    uid: 0
    version: '1187673614'
    wgrp: false
    woth: false
    writeable: true
    wusr: true
    xgrp: true
    xoth: true
    xusr: true
2025-10-25 17:45:03,004 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:45:03,004 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:45:03,004 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:03,004 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=jbnvgzsrsuwuklemkktboiqbmyppegwc] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-jbnvgzsrsuwuklemkktboiqbmyppegwc ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:45:03,043 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:03,314 p=55263 u=root n=ansible | <10.0.6.11> (1, b'\n{"changed": true, "stdout": "", "stderr": "", "rc": 1, "cmd": ["grep", "/usr/local ", "/proc/mounts"], "start": "2025-10-25 10:45:03.551222", "end": "2025-10-25 10:45:03.563950", "delta": "0:00:00.012728", "failed": true, "msg": "non-zero return code", "invocation": {"module_args": {"_raw_params": "grep \'/usr/local \' /proc/mounts", "_uses_shell": false, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "executable": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:45:03,314 p=55263 u=root n=ansible | <10.0.6.11> Failed to connect to the host via ssh: 
2025-10-25 17:45:03,320 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Check if separate partition] *************************************
2025-10-25 17:45:03,321 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  cmd:
  - grep
  - '/usr/local '
  - /proc/mounts
  delta: '0:00:00.012728'
  end: '2025-10-25 10:45:03.563950'
  failed_when_result: false
  invocation:
    module_args:
      _raw_params: grep '/usr/local ' /proc/mounts
      _uses_shell: false
      argv: null
      chdir: null
      creates: null
      executable: null
      expand_argument_vars: true
      removes: null
      stdin: null
      stdin_add_newline: true
      strip_empty_ends: true
  msg: non-zero return code
  rc: 1
  start: '2025-10-25 10:45:03.551222'
  stderr: ''
  stderr_lines: <omitted>
  stdout: ''
  stdout_lines: <omitted>
2025-10-25 17:45:03,355 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 bin path] ***********************************************
2025-10-25 17:45:03,355 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  ansible_facts:
    rke2_bin_path: /usr/local/bin/rke2
2025-10-25 17:45:03,397 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:45:03,398 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:45:03,398 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:03,398 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=smudtedtsovdbxtubeaepsklgagwfhws] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-smudtedtsovdbxtubeaepsklgagwfhws ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:45:03,441 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:04,313 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "{\\"installed_version\\":\\"v1.34.1+rke2r1\\",\\"running_version\\":\\"v1.34.1+rke2r1\\"}", "stderr": "", "rc": 0, "cmd": "set -euo pipefail\\n\\nrke2_bin_path=\\"/usr/local/bin/rke2\\"\\nrke2_service_name=\\"rke2-server.service\\"\\nrke2_version=\\"v1.34.1+rke2r1\\"\\n\\nif [ -f \\"$rke2_bin_path\\" ]; then\\n  installed_version=\\"$($rke2_bin_path --version | awk \'/rke2 version/ {print $3}\')\\"\\nelse\\n  installed_version=\\"not installed\\"\\nfi\\n\\nif systemctl is-active --quiet $rke2_service_name && rke2_service_pid=$(systemctl show $rke2_service_name --property MainPID --value); then\\n  rke2_bin_path=\\"$(realpath \\"/proc/$rke2_service_pid/exe\\")\\"\\nfi\\n\\n# Linux appends the target of removed proc exe with \' (deleted)\', making the path unavailable.\\nif [ -f \\"$rke2_bin_path\\" ]; then\\n  running_version=\\"$($rke2_bin_path --version | awk \'/rke2 version/ {print $3}\')\\"\\nelif [ \\"$installed_version\\" = \\"not installed\\" ]; then\\n  running_version=\\"$rke2_version\\"\\nelse\\n  running_version=\\"outdated\\"\\nfi\\n\\necho \\"{\\\\\\"installed_version\\\\\\":\\\\\\"$installed_version\\\\\\",\\\\\\"running_version\\\\\\":\\\\\\"$running_version\\\\\\"}\\"\\n", "start": "2025-10-25 10:45:03.946623", "end": "2025-10-25 10:45:04.559611", "delta": "0:00:00.612988", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -euo pipefail\\n\\nrke2_bin_path=\\"/usr/local/bin/rke2\\"\\nrke2_service_name=\\"rke2-server.service\\"\\nrke2_version=\\"v1.34.1+rke2r1\\"\\n\\nif [ -f \\"$rke2_bin_path\\" ]; then\\n  installed_version=\\"$($rke2_bin_path --version | awk \'/rke2 version/ {print $3}\')\\"\\nelse\\n  installed_version=\\"not installed\\"\\nfi\\n\\nif systemctl is-active --quiet $rke2_service_name && rke2_service_pid=$(systemctl show $rke2_service_name --property MainPID --value); then\\n  rke2_bin_path=\\"$(realpath \\"/proc/$rke2_service_pid/exe\\")\\"\\nfi\\n\\n# Linux appends the target of removed proc exe with \' (deleted)\', making the path unavailable.\\nif [ -f \\"$rke2_bin_path\\" ]; then\\n  running_version=\\"$($rke2_bin_path --version | awk \'/rke2 version/ {print $3}\')\\"\\nelif [ \\"$installed_version\\" = \\"not installed\\" ]; then\\n  running_version=\\"$rke2_version\\"\\nelse\\n  running_version=\\"outdated\\"\\nfi\\n\\necho \\"{\\\\\\"installed_version\\\\\\":\\\\\\"$installed_version\\\\\\",\\\\\\"running_version\\\\\\":\\\\\\"$running_version\\\\\\"}\\"\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:45:04,316 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Check RKE2 version] **********************************************
2025-10-25 17:45:04,317 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  cmd: |-
    set -euo pipefail
  
    rke2_bin_path="/usr/local/bin/rke2"
    rke2_service_name="rke2-server.service"
    rke2_version="v1.34.1+rke2r1"
  
    if [ -f "$rke2_bin_path" ]; then
      installed_version="$($rke2_bin_path --version | awk '/rke2 version/ {print $3}')"
    else
      installed_version="not installed"
    fi
  
    if systemctl is-active --quiet $rke2_service_name && rke2_service_pid=$(systemctl show $rke2_service_name --property MainPID --value); then
      rke2_bin_path="$(realpath "/proc/$rke2_service_pid/exe")"
    fi
  
    # Linux appends the target of removed proc exe with ' (deleted)', making the path unavailable.
    if [ -f "$rke2_bin_path" ]; then
      running_version="$($rke2_bin_path --version | awk '/rke2 version/ {print $3}')"
    elif [ "$installed_version" = "not installed" ]; then
      running_version="$rke2_version"
    else
      running_version="outdated"
    fi
  
    echo "{\"installed_version\":\"$installed_version\",\"running_version\":\"$running_version\"}"
  delta: '0:00:00.612988'
  end: '2025-10-25 10:45:04.559611'
  invocation:
    module_args:
      _raw_params: |-
        set -euo pipefail
  
        rke2_bin_path="/usr/local/bin/rke2"
        rke2_service_name="rke2-server.service"
        rke2_version="v1.34.1+rke2r1"
  
        if [ -f "$rke2_bin_path" ]; then
          installed_version="$($rke2_bin_path --version | awk '/rke2 version/ {print $3}')"
        else
          installed_version="not installed"
        fi
  
        if systemctl is-active --quiet $rke2_service_name && rke2_service_pid=$(systemctl show $rke2_service_name --property MainPID --value); then
          rke2_bin_path="$(realpath "/proc/$rke2_service_pid/exe")"
        fi
  
        # Linux appends the target of removed proc exe with ' (deleted)', making the path unavailable.
        if [ -f "$rke2_bin_path" ]; then
          running_version="$($rke2_bin_path --version | awk '/rke2 version/ {print $3}')"
        elif [ "$installed_version" = "not installed" ]; then
          running_version="$rke2_version"
        else
          running_version="outdated"
        fi
  
        echo "{\"installed_version\":\"$installed_version\",\"running_version\":\"$running_version\"}"
      _uses_shell: true
      argv: null
      chdir: null
      creates: null
      executable: /bin/bash
      expand_argument_vars: true
      removes: null
      stdin: null
      stdin_add_newline: true
      strip_empty_ends: true
  msg: ''
  rc: 0
  start: '2025-10-25 10:45:03.946623'
  stderr: ''
  stderr_lines: <omitted>
  stdout: '{"installed_version":"v1.34.1+rke2r1","running_version":"v1.34.1+rke2r1"}'
  stdout_lines: <omitted>
2025-10-25 17:45:04,372 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 versions] ***********************************************
2025-10-25 17:45:04,372 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  ansible_facts:
    installed_version: v1.34.1+rke2r1
    running_version: v1.34.1+rke2r1
2025-10-25 17:45:04,440 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Prevent accidental RKE2 downgrade] *******************************
2025-10-25 17:45:04,442 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: |-
    Version check passed. Current running version: ['1.34.1'], desired version: ['1.34.1']."
2025-10-25 17:45:04,754 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Find Active Server] **********************************************
2025-10-25 17:45:04,779 p=55263 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/find_active_server.yml for k8s-m1
2025-10-25 17:45:04,833 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/service_facts.py
2025-10-25 17:45:04,834 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:45:04,834 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:04,835 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=qjgocadpryzfpylihzmhdvqjytjpgdur] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-qjgocadpryzfpylihzmhdvqjytjpgdur ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:45:04,875 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:10,715 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"ansible_facts": {"services": {"apparmor": {"name": "apparmor", "state": "running", "source": "sysv"}, "apport": {"name": "apport", "state": "running", "source": "sysv"}, "console-setup.sh": {"name": "console-setup.sh", "state": "stopped", "source": "sysv"}, "cron": {"name": "cron", "state": "running", "source": "sysv"}, "cryptdisks": {"name": "cryptdisks", "state": "stopped", "source": "sysv"}, "cryptdisks-early": {"name": "cryptdisks-early", "state": "stopped", "source": "sysv"}, "dbus": {"name": "dbus", "state": "running", "source": "sysv"}, "grub-common": {"name": "grub-common", "state": "stopped", "source": "sysv"}, "iscsid": {"name": "iscsid", "state": "stopped", "source": "sysv"}, "keyboard-setup.sh": {"name": "keyboard-setup.sh", "state": "stopped", "source": "sysv"}, "kmod": {"name": "kmod", "state": "running", "source": "sysv"}, "open-iscsi": {"name": "open-iscsi", "state": "stopped", "source": "sysv"}, "open-vm-tools": {"name": "open-vm-tools", "state": "running", "source": "sysv"}, "plymouth": {"name": "plymouth", "state": "running", "source": "sysv"}, "plymouth-log": {"name": "plymouth-log", "state": "running", "source": "sysv"}, "procps": {"name": "procps", "state": "running", "source": "sysv"}, "rsync": {"name": "rsync", "state": "stopped", "source": "sysv"}, "screen-cleanup": {"name": "screen-cleanup", "state": "stopped", "source": "sysv"}, "ssh": {"name": "ssh", "state": "running", "source": "sysv"}, "sysstat": {"name": "sysstat", "state": "running", "source": "sysv"}, "ufw": {"name": "ufw", "state": "running", "source": "sysv"}, "unattended-upgrades": {"name": "unattended-upgrades", "state": "running", "source": "sysv"}, "uuidd": {"name": "uuidd", "state": "stopped", "source": "sysv"}, "apparmor.service": {"name": "apparmor.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "apport-autoreport.service": {"name": "apport-autoreport.service", "state": "stopped", "status": "static", "source": "systemd"}, "apport.service": {"name": "apport.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "apt-daily-upgrade.service": {"name": "apt-daily-upgrade.service", "state": "stopped", "status": "static", "source": "systemd"}, "apt-daily.service": {"name": "apt-daily.service", "state": "stopped", "status": "static", "source": "systemd"}, "auditd.service": {"name": "auditd.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "blk-availability.service": {"name": "blk-availability.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "cloud-init-local.service": {"name": "cloud-init-local.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "connman.service": {"name": "connman.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "console-screen.service": {"name": "console-screen.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "console-setup.service": {"name": "console-setup.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "cron.service": {"name": "cron.service", "state": "running", "status": "enabled", "source": "systemd"}, "dbus.service": {"name": "dbus.service", "state": "running", "status": "static", "source": "systemd"}, "display-manager.service": {"name": "display-manager.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "dm-event.service": {"name": "dm-event.service", "state": "stopped", "status": "static", "source": "systemd"}, "dmesg.service": {"name": "dmesg.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "dpkg-db-backup.service": {"name": "dpkg-db-backup.service", "state": "stopped", "status": "static", "source": "systemd"}, "e2scrub_all.service": {"name": "e2scrub_all.service", "state": "stopped", "status": "static", "source": "systemd"}, "e2scrub_reap.service": {"name": "e2scrub_reap.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "emergency.service": {"name": "emergency.service", "state": "stopped", "status": "static", "source": "systemd"}, "fcoe.service": {"name": "fcoe.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "finalrd.service": {"name": "finalrd.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "fstrim.service": {"name": "fstrim.service", "state": "stopped", "status": "static", "source": "systemd"}, "fwupd-refresh.service": {"name": "fwupd-refresh.service", "state": "stopped", "status": "failed", "source": "systemd"}, "fwupd.service": {"name": "fwupd.service", "state": "running", "status": "static", "source": "systemd"}, "getty-static.service": {"name": "getty-static.service", "state": "stopped", "status": "static", "source": "systemd"}, "getty@tty1.service": {"name": "getty@tty1.service", "state": "running", "status": "active", "source": "systemd"}, "grub-common.service": {"name": "grub-common.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "loaded": {"name": "loaded", "state": "stopped", "status": "failed", "source": "systemd"}, "hv_kvp_daemon.service": {"name": "hv_kvp_daemon.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "initrd-cleanup.service": {"name": "initrd-cleanup.service", "state": "stopped", "status": "static", "source": "systemd"}, "initrd-parse-etc.service": {"name": "initrd-parse-etc.service", "state": "stopped", "status": "static", "source": "systemd"}, "initrd-switch-root.service": {"name": "initrd-switch-root.service", "state": "stopped", "status": "static", "source": "systemd"}, "initrd-udevadm-cleanup-db.service": {"name": "initrd-udevadm-cleanup-db.service", "state": "stopped", "status": "static", "source": "systemd"}, "iscsi-shutdown.service": {"name": "iscsi-shutdown.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "iscsid.service": {"name": "iscsid.service", "state": "stopped", "status": "disabled", "source": "systemd"}, "kbd.service": {"name": "kbd.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "keyboard-setup.service": {"name": "keyboard-setup.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "kmod-static-nodes.service": {"name": "kmod-static-nodes.service", "state": "stopped", "status": "static", "source": "systemd"}, "ldconfig.service": {"name": "ldconfig.service", "state": "stopped", "status": "static", "source": "systemd"}, "logrotate.service": {"name": "logrotate.service", "state": "stopped", "status": "static", "source": "systemd"}, "lvm2-activation-early.service": {"name": "lvm2-activation-early.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "lvm2-lvmpolld.service": {"name": "lvm2-lvmpolld.service", "state": "stopped", "status": "static", "source": "systemd"}, "lvm2-monitor.service": {"name": "lvm2-monitor.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "man-db.service": {"name": "man-db.service", "state": "stopped", "status": "static", "source": "systemd"}, "ModemManager.service": {"name": "ModemManager.service", "state": "running", "status": "enabled", "source": "systemd"}, "modprobe@configfs.service": {"name": "modprobe@configfs.service", "state": "stopped", "status": "inactive", "source": "systemd"}, "modprobe@dm_mod.service": {"name": "modprobe@dm_mod.service", "state": "stopped", "status": "inactive", "source": "systemd"}, "modprobe@drm.service": {"name": "modprobe@drm.service", "state": "stopped", "status": "inactive", "source": "systemd"}, "modprobe@efi_pstore.service": {"name": "modprobe@efi_pstore.service", "state": "stopped", "status": "inactive", "source": "systemd"}, "modprobe@fuse.service": {"name": "modprobe@fuse.service", "state": "stopped", "status": "inactive", "source": "systemd"}, "modprobe@loop.service": {"name": "modprobe@loop.service", "state": "stopped", "status": "inactive", "source": "systemd"}, "motd-news.service": {"name": "motd-news.service", "state": "stopped", "status": "static", "source": "systemd"}, "multipathd.service": {"name": "multipathd.service", "state": "running", "status": "enabled", "source": "systemd"}, "netplan-ovs-cleanup.service": {"name": "netplan-ovs-cleanup.service", "state": "stopped", "status": "enabled-runtime", "source": "systemd"}, "networkd-dispatcher.service": {"name": "networkd-dispatcher.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "NetworkManager.service": {"name": "NetworkManager.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "open-iscsi.service": {"name": "open-iscsi.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "open-vm-tools.service": {"name": "open-vm-tools.service", "state": "running", "status": "enabled", "source": "systemd"}, "ovsdb-server.service": {"name": "ovsdb-server.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "plymouth-quit-wait.service": {"name": "plymouth-quit-wait.service", "state": "stopped", "status": "static", "source": "systemd"}, "plymouth-quit.service": {"name": "plymouth-quit.service", "state": "stopped", "status": "static", "source": "systemd"}, "plymouth-read-write.service": {"name": "plymouth-read-write.service", "state": "stopped", "status": "static", "source": "systemd"}, "plymouth-start.service": {"name": "plymouth-start.service", "state": "stopped", "status": "static", "source": "systemd"}, "plymouth-switch-root.service": {"name": "plymouth-switch-root.service", "state": "stopped", "status": "static", "source": "systemd"}, "polkit.service": {"name": "polkit.service", "state": "running", "status": "static", "source": "systemd"}, "pollinate.service": {"name": "pollinate.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "rbdmap.service": {"name": "rbdmap.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "rc-local.service": {"name": "rc-local.service", "state": "stopped", "status": "static", "source": "systemd"}, "rescue.service": {"name": "rescue.service", "state": "stopped", "status": "static", "source": "systemd"}, "rke2-agent.service": {"name": "rke2-agent.service", "state": "stopped", "status": "masked", "source": "systemd"}, "rke2-server.service": {"name": "rke2-server.service", "state": "running", "status": "enabled", "source": "systemd"}, "rsyslog.service": {"name": "rsyslog.service", "state": "running", "status": "enabled", "source": "systemd"}, "secureboot-db.service": {"name": "secureboot-db.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "setvtrgb.service": {"name": "setvtrgb.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "snapd.apparmor.service": {"name": "snapd.apparmor.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "snapd.autoimport.service": {"name": "snapd.autoimport.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "snapd.core-fixup.service": {"name": "snapd.core-fixup.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "snapd.failure.service": {"name": "snapd.failure.service", "state": "stopped", "status": "static", "source": "systemd"}, "snapd.recovery-chooser-trigger.service": {"name": "snapd.recovery-chooser-trigger.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "snapd.seeded.service": {"name": "snapd.seeded.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "snapd.service": {"name": "snapd.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "snapd.snap-repair.service": {"name": "snapd.snap-repair.service", "state": "stopped", "status": "static", "source": "systemd"}, "snapd.system-shutdown.service": {"name": "snapd.system-shutdown.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "ssh.service": {"name": "ssh.service", "state": "running", "status": "disabled", "source": "systemd"}, "sysstat-collect.service": {"name": "sysstat-collect.service", "state": "stopped", "status": "static", "source": "systemd"}, "sysstat-summary.service": {"name": "sysstat-summary.service", "state": "stopped", "status": "static", "source": "systemd"}, "sysstat.service": {"name": "sysstat.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "systemd-ask-password-console.service": {"name": "systemd-ask-password-console.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-ask-password-plymouth.service": {"name": "systemd-ask-password-plymouth.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-ask-password-wall.service": {"name": "systemd-ask-password-wall.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-battery-check.service": {"name": "systemd-battery-check.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-binfmt.service": {"name": "systemd-binfmt.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-bsod.service": {"name": "systemd-bsod.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-firstboot.service": {"name": "systemd-firstboot.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-fsck-root.service": {"name": "systemd-fsck-root.service", "state": "stopped", "status": "enabled-runtime", "source": "systemd"}, "systemd-fsck@dev-disk-by\\\\x2duuid-9bb05da7\\\\x2d85f0\\\\x2d4338\\\\x2d9b8f\\\\x2d06ca0d639ac6.service": {"name": "systemd-fsck@dev-disk-by\\\\x2duuid-9bb05da7\\\\x2d85f0\\\\x2d4338\\\\x2d9b8f\\\\x2d06ca0d639ac6.service", "state": "stopped", "status": "active", "source": "systemd"}, "systemd-fsckd.service": {"name": "systemd-fsckd.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-hibernate-resume.service": {"name": "systemd-hibernate-resume.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-hibernate.service": {"name": "systemd-hibernate.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-hwdb-update.service": {"name": "systemd-hwdb-update.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-hybrid-sleep.service": {"name": "systemd-hybrid-sleep.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-initctl.service": {"name": "systemd-initctl.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-journal-catalog-update.service": {"name": "systemd-journal-catalog-update.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-journal-flush.service": {"name": "systemd-journal-flush.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-journald.service": {"name": "systemd-journald.service", "state": "running", "status": "static", "source": "systemd"}, "systemd-logind.service": {"name": "systemd-logind.service", "state": "running", "status": "static", "source": "systemd"}, "systemd-machine-id-commit.service": {"name": "systemd-machine-id-commit.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-modules-load.service": {"name": "systemd-modules-load.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-networkd-wait-online.service": {"name": "systemd-networkd-wait-online.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "systemd-networkd.service": {"name": "systemd-networkd.service", "state": "running", "status": "enabled", "source": "systemd"}, "systemd-oomd.service": {"name": "systemd-oomd.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "systemd-pcrmachine.service": {"name": "systemd-pcrmachine.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-pcrphase-initrd.service": {"name": "systemd-pcrphase-initrd.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-pcrphase-sysinit.service": {"name": "systemd-pcrphase-sysinit.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-pcrphase.service": {"name": "systemd-pcrphase.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-pstore.service": {"name": "systemd-pstore.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "systemd-quotacheck.service": {"name": "systemd-quotacheck.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-random-seed.service": {"name": "systemd-random-seed.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-remount-fs.service": {"name": "systemd-remount-fs.service", "state": "stopped", "status": "enabled-runtime", "source": "systemd"}, "systemd-repart.service": {"name": "systemd-repart.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-resolved.service": {"name": "systemd-resolved.service", "state": "running", "status": "enabled", "source": "systemd"}, "systemd-rfkill.service": {"name": "systemd-rfkill.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-soft-reboot.service": {"name": "systemd-soft-reboot.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-suspend-then-hibernate.service": {"name": "systemd-suspend-then-hibernate.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-suspend.service": {"name": "systemd-suspend.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-sysctl.service": {"name": "systemd-sysctl.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-sysext.service": {"name": "systemd-sysext.service", "state": "stopped", "status": "disabled", "source": "systemd"}, "systemd-sysusers.service": {"name": "systemd-sysusers.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-timesyncd.service": {"name": "systemd-timesyncd.service", "state": "running", "status": "enabled", "source": "systemd"}, "systemd-tmpfiles-clean.service": {"name": "systemd-tmpfiles-clean.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-tmpfiles-setup-dev-early.service": {"name": "systemd-tmpfiles-setup-dev-early.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-tmpfiles-setup-dev.service": {"name": "systemd-tmpfiles-setup-dev.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-tmpfiles-setup.service": {"name": "systemd-tmpfiles-setup.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-tpm2-setup-early.service": {"name": "systemd-tpm2-setup-early.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-tpm2-setup.service": {"name": "systemd-tpm2-setup.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-udev-settle.service": {"name": "systemd-udev-settle.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-udev-trigger.service": {"name": "systemd-udev-trigger.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-udevd.service": {"name": "systemd-udevd.service", "state": "running", "status": "static", "source": "systemd"}, "systemd-update-done.service": {"name": "systemd-update-done.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-update-utmp-runlevel.service": {"name": "systemd-update-utmp-runlevel.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-update-utmp.service": {"name": "systemd-update-utmp.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-user-sessions.service": {"name": "systemd-user-sessions.service", "state": "stopped", "status": "static", "source": "systemd"}, "systemd-vconsole-setup.service": {"name": "systemd-vconsole-setup.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "thermald.service": {"name": "thermald.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "tpm-udev.service": {"name": "tpm-udev.service", "state": "stopped", "status": "static", "source": "systemd"}, "ua-auto-attach.service": {"name": "ua-auto-attach.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "ua-reboot-cmds.service": {"name": "ua-reboot-cmds.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "ua-timer.service": {"name": "ua-timer.service", "state": "stopped", "status": "static", "source": "systemd"}, "ubuntu-advantage-cloud-id-shim.service": {"name": "ubuntu-advantage-cloud-id-shim.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "ubuntu-advantage.service": {"name": "ubuntu-advantage.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "udisks2.service": {"name": "udisks2.service", "state": "running", "status": "enabled", "source": "systemd"}, "ufw.service": {"name": "ufw.service", "state": "stopped", "status": "enabled", "source": "systemd"}, "unattended-upgrades.service": {"name": "unattended-upgrades.service", "state": "running", "status": "enabled", "source": "systemd"}, "update-notifier-motd.service": {"name": "update-notifier-motd.service", "state": "stopped", "status": "static", "source": "systemd"}, "upower.service": {"name": "upower.service", "state": "running", "status": "disabled", "source": "systemd"}, "user-runtime-dir@1000.service": {"name": "user-runtime-dir@1000.service", "state": "stopped", "status": "active", "source": "systemd"}, "user@1000.service": {"name": "user@1000.service", "state": "running", "status": "active", "source": "systemd"}, "uuidd.service": {"name": "uuidd.service", "state": "stopped", "status": "indirect", "source": "systemd"}, "vgauth.service": {"name": "vgauth.service", "state": "running", "status": "enabled", "source": "systemd"}, "zfs-mount.service": {"name": "zfs-mount.service", "state": "stopped", "status": "not-found", "source": "systemd"}, "apport-coredump-hook@.service": {"name": "apport-coredump-hook@.service", "state": "unknown", "status": "static", "source": "systemd"}, "apport-forward@.service": {"name": "apport-forward@.service", "state": "unknown", "status": "static", "source": "systemd"}, "apt-news.service": {"name": "apt-news.service", "state": "inactive", "status": "static", "source": "systemd"}, "autovt@.service": {"name": "autovt@.service", "state": "unknown", "status": "alias", "source": "systemd"}, "bolt.service": {"name": "bolt.service", "state": "inactive", "status": "static", "source": "systemd"}, "cloud-config.service": {"name": "cloud-config.service", "state": "inactive", "status": "enabled", "source": "systemd"}, "cloud-final.service": {"name": "cloud-final.service", "state": "inactive", "status": "enabled", "source": "systemd"}, "cloud-init-hotplugd.service": {"name": "cloud-init-hotplugd.service", "state": "inactive", "status": "static", "source": "systemd"}, "cloud-init.service": {"name": "cloud-init.service", "state": "inactive", "status": "enabled", "source": "systemd"}, "console-getty.service": {"name": "console-getty.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "container-getty@.service": {"name": "container-getty@.service", "state": "unknown", "status": "static", "source": "systemd"}, "cryptdisks-early.service": {"name": "cryptdisks-early.service", "state": "inactive", "status": "masked", "source": "systemd"}, "cryptdisks.service": {"name": "cryptdisks.service", "state": "inactive", "status": "masked", "source": "systemd"}, "dbus-org.freedesktop.hostname1.service": {"name": "dbus-org.freedesktop.hostname1.service", "state": "inactive", "status": "alias", "source": "systemd"}, "dbus-org.freedesktop.locale1.service": {"name": "dbus-org.freedesktop.locale1.service", "state": "inactive", "status": "alias", "source": "systemd"}, "dbus-org.freedesktop.login1.service": {"name": "dbus-org.freedesktop.login1.service", "state": "active", "status": "alias", "source": "systemd"}, "dbus-org.freedesktop.ModemManager1.service": {"name": "dbus-org.freedesktop.ModemManager1.service", "state": "active", "status": "alias", "source": "systemd"}, "dbus-org.freedesktop.resolve1.service": {"name": "dbus-org.freedesktop.resolve1.service", "state": "active", "status": "alias", "source": "systemd"}, "dbus-org.freedesktop.thermald.service": {"name": "dbus-org.freedesktop.thermald.service", "state": "inactive", "status": "alias", "source": "systemd"}, "dbus-org.freedesktop.timedate1.service": {"name": "dbus-org.freedesktop.timedate1.service", "state": "inactive", "status": "alias", "source": "systemd"}, "dbus-org.freedesktop.timesync1.service": {"name": "dbus-org.freedesktop.timesync1.service", "state": "active", "status": "alias", "source": "systemd"}, "debug-shell.service": {"name": "debug-shell.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "e2scrub@.service": {"name": "e2scrub@.service", "state": "unknown", "status": "static", "source": "systemd"}, "e2scrub_fail@.service": {"name": "e2scrub_fail@.service", "state": "unknown", "status": "static", "source": "systemd"}, "esm-cache.service": {"name": "esm-cache.service", "state": "inactive", "status": "static", "source": "systemd"}, "friendly-recovery.service": {"name": "friendly-recovery.service", "state": "inactive", "status": "static", "source": "systemd"}, "fwupd-offline-update.service": {"name": "fwupd-offline-update.service", "state": "inactive", "status": "static", "source": "systemd"}, "getty@.service": {"name": "getty@.service", "state": "unknown", "status": "enabled", "source": "systemd"}, "gpu-manager.service": {"name": "gpu-manager.service", "state": "inactive", "status": "enabled", "source": "systemd"}, "grub-initrd-fallback.service": {"name": "grub-initrd-fallback.service", "state": "inactive", "status": "enabled", "source": "systemd"}, "hwclock.service": {"name": "hwclock.service", "state": "inactive", "status": "masked", "source": "systemd"}, "iscsi.service": {"name": "iscsi.service", "state": "inactive", "status": "alias", "source": "systemd"}, "kmod.service": {"name": "kmod.service", "state": "active", "status": "alias", "source": "systemd"}, "lxd-agent.service": {"name": "lxd-agent.service", "state": "inactive", "status": "static", "source": "systemd"}, "lxd-installer@.service": {"name": "lxd-installer@.service", "state": "unknown", "status": "static", "source": "systemd"}, "mdadm-grow-continue@.service": {"name": "mdadm-grow-continue@.service", "state": "unknown", "status": "static", "source": "systemd"}, "mdadm-last-resort@.service": {"name": "mdadm-last-resort@.service", "state": "unknown", "status": "static", "source": "systemd"}, "mdcheck_continue.service": {"name": "mdcheck_continue.service", "state": "inactive", "status": "static", "source": "systemd"}, "mdcheck_start.service": {"name": "mdcheck_start.service", "state": "inactive", "status": "static", "source": "systemd"}, "mdmon@.service": {"name": "mdmon@.service", "state": "unknown", "status": "static", "source": "systemd"}, "mdmonitor-oneshot.service": {"name": "mdmonitor-oneshot.service", "state": "inactive", "status": "static", "source": "systemd"}, "mdmonitor.service": {"name": "mdmonitor.service", "state": "inactive", "status": "static", "source": "systemd"}, "modprobe@.service": {"name": "modprobe@.service", "state": "unknown", "status": "static", "source": "systemd"}, "multipath-tools-boot.service": {"name": "multipath-tools-boot.service", "state": "inactive", "status": "masked", "source": "systemd"}, "multipath-tools.service": {"name": "multipath-tools.service", "state": "active", "status": "alias", "source": "systemd"}, "nftables.service": {"name": "nftables.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "packagekit-offline-update.service": {"name": "packagekit-offline-update.service", "state": "inactive", "status": "static", "source": "systemd"}, "packagekit.service": {"name": "packagekit.service", "state": "inactive", "status": "static", "source": "systemd"}, "pam_namespace.service": {"name": "pam_namespace.service", "state": "inactive", "status": "static", "source": "systemd"}, "plymouth-halt.service": {"name": "plymouth-halt.service", "state": "inactive", "status": "static", "source": "systemd"}, "plymouth-kexec.service": {"name": "plymouth-kexec.service", "state": "inactive", "status": "static", "source": "systemd"}, "plymouth-log.service": {"name": "plymouth-log.service", "state": "active", "status": "alias", "source": "systemd"}, "plymouth-poweroff.service": {"name": "plymouth-poweroff.service", "state": "inactive", "status": "static", "source": "systemd"}, "plymouth-reboot.service": {"name": "plymouth-reboot.service", "state": "inactive", "status": "static", "source": "systemd"}, "plymouth-switch-root-initramfs.service": {"name": "plymouth-switch-root-initramfs.service", "state": "inactive", "status": "static", "source": "systemd"}, "plymouth.service": {"name": "plymouth.service", "state": "active", "status": "alias", "source": "systemd"}, "procps.service": {"name": "procps.service", "state": "active", "status": "alias", "source": "systemd"}, "quotaon.service": {"name": "quotaon.service", "state": "inactive", "status": "static", "source": "systemd"}, "rsync.service": {"name": "rsync.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "screen-cleanup.service": {"name": "screen-cleanup.service", "state": "inactive", "status": "masked", "source": "systemd"}, "serial-getty@.service": {"name": "serial-getty@.service", "state": "unknown", "status": "disabled", "source": "systemd"}, "sudo.service": {"name": "sudo.service", "state": "inactive", "status": "masked", "source": "systemd"}, "syslog.service": {"name": "syslog.service", "state": "active", "status": "alias", "source": "systemd"}, "system-update-cleanup.service": {"name": "system-update-cleanup.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-backlight@.service": {"name": "systemd-backlight@.service", "state": "unknown", "status": "static", "source": "systemd"}, "systemd-boot-check-no-failures.service": {"name": "systemd-boot-check-no-failures.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-confext.service": {"name": "systemd-confext.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-exit.service": {"name": "systemd-exit.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-fsck@.service": {"name": "systemd-fsck@.service", "state": "unknown", "status": "static", "source": "systemd"}, "systemd-growfs-root.service": {"name": "systemd-growfs-root.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-growfs@.service": {"name": "systemd-growfs@.service", "state": "unknown", "status": "static", "source": "systemd"}, "systemd-halt.service": {"name": "systemd-halt.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-hostnamed.service": {"name": "systemd-hostnamed.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-journald@.service": {"name": "systemd-journald@.service", "state": "unknown", "status": "static", "source": "systemd"}, "systemd-kexec.service": {"name": "systemd-kexec.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-localed.service": {"name": "systemd-localed.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-network-generator.service": {"name": "systemd-network-generator.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-networkd-wait-online@.service": {"name": "systemd-networkd-wait-online@.service", "state": "unknown", "status": "disabled", "source": "systemd"}, "systemd-pcrextend@.service": {"name": "systemd-pcrextend@.service", "state": "unknown", "status": "static", "source": "systemd"}, "systemd-pcrfs-root.service": {"name": "systemd-pcrfs-root.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-pcrfs@.service": {"name": "systemd-pcrfs@.service", "state": "unknown", "status": "static", "source": "systemd"}, "systemd-pcrlock-file-system.service": {"name": "systemd-pcrlock-file-system.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-pcrlock-firmware-code.service": {"name": "systemd-pcrlock-firmware-code.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-pcrlock-firmware-config.service": {"name": "systemd-pcrlock-firmware-config.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-pcrlock-machine-id.service": {"name": "systemd-pcrlock-machine-id.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-pcrlock-make-policy.service": {"name": "systemd-pcrlock-make-policy.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-pcrlock-secureboot-authority.service": {"name": "systemd-pcrlock-secureboot-authority.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-pcrlock-secureboot-policy.service": {"name": "systemd-pcrlock-secureboot-policy.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-poweroff.service": {"name": "systemd-poweroff.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-reboot.service": {"name": "systemd-reboot.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-storagetm.service": {"name": "systemd-storagetm.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-sysext@.service": {"name": "systemd-sysext@.service", "state": "unknown", "status": "static", "source": "systemd"}, "systemd-sysupdate-reboot.service": {"name": "systemd-sysupdate-reboot.service", "state": "inactive", "status": "indirect", "source": "systemd"}, "systemd-sysupdate.service": {"name": "systemd-sysupdate.service", "state": "inactive", "status": "indirect", "source": "systemd"}, "systemd-time-wait-sync.service": {"name": "systemd-time-wait-sync.service", "state": "inactive", "status": "disabled", "source": "systemd"}, "systemd-timedated.service": {"name": "systemd-timedated.service", "state": "inactive", "status": "static", "source": "systemd"}, "systemd-volatile-root.service": {"name": "systemd-volatile-root.service", "state": "inactive", "status": "static", "source": "systemd"}, "udev.service": {"name": "udev.service", "state": "active", "status": "alias", "source": "systemd"}, "update-notifier-download.service": {"name": "update-notifier-download.service", "state": "inactive", "status": "static", "source": "systemd"}, "usb_modeswitch@.service": {"name": "usb_modeswitch@.service", "state": "unknown", "status": "static", "source": "systemd"}, "usbmuxd.service": {"name": "usbmuxd.service", "state": "inactive", "status": "static", "source": "systemd"}, "user-runtime-dir@.service": {"name": "user-runtime-dir@.service", "state": "unknown", "status": "static", "source": "systemd"}, "user@.service": {"name": "user@.service", "state": "unknown", "status": "static", "source": "systemd"}, "vmtoolsd.service": {"name": "vmtoolsd.service", "state": "active", "status": "alias", "source": "systemd"}, "x11-common.service": {"name": "x11-common.service", "state": "inactive", "status": "masked", "source": "systemd"}, "xfs_scrub@.service": {"name": "xfs_scrub@.service", "state": "unknown", "status": "static", "source": "systemd"}, "xfs_scrub_all.service": {"name": "xfs_scrub_all.service", "state": "inactive", "status": "static", "source": "systemd"}, "xfs_scrub_fail@.service": {"name": "xfs_scrub_fail@.service", "state": "unknown", "status": "static", "source": "systemd"}}}, "invocation": {"module_args": {}}}\n', b'')
2025-10-25 17:45:10,758 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Populate services facts] *****************************************
2025-10-25 17:45:10,784 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  ansible_facts:
    services:
      ModemManager.service:
        name: ModemManager.service
        source: systemd
        state: running
        status: enabled
      NetworkManager.service:
        name: NetworkManager.service
        source: systemd
        state: stopped
        status: not-found
      apparmor:
        name: apparmor
        source: sysv
        state: running
      apparmor.service:
        name: apparmor.service
        source: systemd
        state: stopped
        status: enabled
      apport:
        name: apport
        source: sysv
        state: running
      apport-autoreport.service:
        name: apport-autoreport.service
        source: systemd
        state: stopped
        status: static
      apport-coredump-hook@.service:
        name: apport-coredump-hook@.service
        source: systemd
        state: unknown
        status: static
      apport-forward@.service:
        name: apport-forward@.service
        source: systemd
        state: unknown
        status: static
      apport.service:
        name: apport.service
        source: systemd
        state: stopped
        status: enabled
      apt-daily-upgrade.service:
        name: apt-daily-upgrade.service
        source: systemd
        state: stopped
        status: static
      apt-daily.service:
        name: apt-daily.service
        source: systemd
        state: stopped
        status: static
      apt-news.service:
        name: apt-news.service
        source: systemd
        state: inactive
        status: static
      auditd.service:
        name: auditd.service
        source: systemd
        state: stopped
        status: not-found
      autovt@.service:
        name: autovt@.service
        source: systemd
        state: unknown
        status: alias
      blk-availability.service:
        name: blk-availability.service
        source: systemd
        state: stopped
        status: enabled
      bolt.service:
        name: bolt.service
        source: systemd
        state: inactive
        status: static
      cloud-config.service:
        name: cloud-config.service
        source: systemd
        state: inactive
        status: enabled
      cloud-final.service:
        name: cloud-final.service
        source: systemd
        state: inactive
        status: enabled
      cloud-init-hotplugd.service:
        name: cloud-init-hotplugd.service
        source: systemd
        state: inactive
        status: static
      cloud-init-local.service:
        name: cloud-init-local.service
        source: systemd
        state: stopped
        status: enabled
      cloud-init.service:
        name: cloud-init.service
        source: systemd
        state: inactive
        status: enabled
      connman.service:
        name: connman.service
        source: systemd
        state: stopped
        status: not-found
      console-getty.service:
        name: console-getty.service
        source: systemd
        state: inactive
        status: disabled
      console-screen.service:
        name: console-screen.service
        source: systemd
        state: stopped
        status: not-found
      console-setup.service:
        name: console-setup.service
        source: systemd
        state: stopped
        status: enabled
      console-setup.sh:
        name: console-setup.sh
        source: sysv
        state: stopped
      container-getty@.service:
        name: container-getty@.service
        source: systemd
        state: unknown
        status: static
      cron:
        name: cron
        source: sysv
        state: running
      cron.service:
        name: cron.service
        source: systemd
        state: running
        status: enabled
      cryptdisks:
        name: cryptdisks
        source: sysv
        state: stopped
      cryptdisks-early:
        name: cryptdisks-early
        source: sysv
        state: stopped
      cryptdisks-early.service:
        name: cryptdisks-early.service
        source: systemd
        state: inactive
        status: masked
      cryptdisks.service:
        name: cryptdisks.service
        source: systemd
        state: inactive
        status: masked
      dbus:
        name: dbus
        source: sysv
        state: running
      dbus-org.freedesktop.ModemManager1.service:
        name: dbus-org.freedesktop.ModemManager1.service
        source: systemd
        state: active
        status: alias
      dbus-org.freedesktop.hostname1.service:
        name: dbus-org.freedesktop.hostname1.service
        source: systemd
        state: inactive
        status: alias
      dbus-org.freedesktop.locale1.service:
        name: dbus-org.freedesktop.locale1.service
        source: systemd
        state: inactive
        status: alias
      dbus-org.freedesktop.login1.service:
        name: dbus-org.freedesktop.login1.service
        source: systemd
        state: active
        status: alias
      dbus-org.freedesktop.resolve1.service:
        name: dbus-org.freedesktop.resolve1.service
        source: systemd
        state: active
        status: alias
      dbus-org.freedesktop.thermald.service:
        name: dbus-org.freedesktop.thermald.service
        source: systemd
        state: inactive
        status: alias
      dbus-org.freedesktop.timedate1.service:
        name: dbus-org.freedesktop.timedate1.service
        source: systemd
        state: inactive
        status: alias
      dbus-org.freedesktop.timesync1.service:
        name: dbus-org.freedesktop.timesync1.service
        source: systemd
        state: active
        status: alias
      dbus.service:
        name: dbus.service
        source: systemd
        state: running
        status: static
      debug-shell.service:
        name: debug-shell.service
        source: systemd
        state: inactive
        status: disabled
      display-manager.service:
        name: display-manager.service
        source: systemd
        state: stopped
        status: not-found
      dm-event.service:
        name: dm-event.service
        source: systemd
        state: stopped
        status: static
      dmesg.service:
        name: dmesg.service
        source: systemd
        state: stopped
        status: enabled
      dpkg-db-backup.service:
        name: dpkg-db-backup.service
        source: systemd
        state: stopped
        status: static
      e2scrub@.service:
        name: e2scrub@.service
        source: systemd
        state: unknown
        status: static
      e2scrub_all.service:
        name: e2scrub_all.service
        source: systemd
        state: stopped
        status: static
      e2scrub_fail@.service:
        name: e2scrub_fail@.service
        source: systemd
        state: unknown
        status: static
      e2scrub_reap.service:
        name: e2scrub_reap.service
        source: systemd
        state: stopped
        status: enabled
      emergency.service:
        name: emergency.service
        source: systemd
        state: stopped
        status: static
      esm-cache.service:
        name: esm-cache.service
        source: systemd
        state: inactive
        status: static
      fcoe.service:
        name: fcoe.service
        source: systemd
        state: stopped
        status: not-found
      finalrd.service:
        name: finalrd.service
        source: systemd
        state: stopped
        status: enabled
      friendly-recovery.service:
        name: friendly-recovery.service
        source: systemd
        state: inactive
        status: static
      fstrim.service:
        name: fstrim.service
        source: systemd
        state: stopped
        status: static
      fwupd-offline-update.service:
        name: fwupd-offline-update.service
        source: systemd
        state: inactive
        status: static
      fwupd-refresh.service:
        name: fwupd-refresh.service
        source: systemd
        state: stopped
        status: failed
      fwupd.service:
        name: fwupd.service
        source: systemd
        state: running
        status: static
      getty-static.service:
        name: getty-static.service
        source: systemd
        state: stopped
        status: static
      getty@.service:
        name: getty@.service
        source: systemd
        state: unknown
        status: enabled
      getty@tty1.service:
        name: getty@tty1.service
        source: systemd
        state: running
        status: active
      gpu-manager.service:
        name: gpu-manager.service
        source: systemd
        state: inactive
        status: enabled
      grub-common:
        name: grub-common
        source: sysv
        state: stopped
      grub-common.service:
        name: grub-common.service
        source: systemd
        state: stopped
        status: enabled
      grub-initrd-fallback.service:
        name: grub-initrd-fallback.service
        source: systemd
        state: inactive
        status: enabled
      hv_kvp_daemon.service:
        name: hv_kvp_daemon.service
        source: systemd
        state: stopped
        status: not-found
      hwclock.service:
        name: hwclock.service
        source: systemd
        state: inactive
        status: masked
      initrd-cleanup.service:
        name: initrd-cleanup.service
        source: systemd
        state: stopped
        status: static
      initrd-parse-etc.service:
        name: initrd-parse-etc.service
        source: systemd
        state: stopped
        status: static
      initrd-switch-root.service:
        name: initrd-switch-root.service
        source: systemd
        state: stopped
        status: static
      initrd-udevadm-cleanup-db.service:
        name: initrd-udevadm-cleanup-db.service
        source: systemd
        state: stopped
        status: static
      iscsi-shutdown.service:
        name: iscsi-shutdown.service
        source: systemd
        state: stopped
        status: not-found
      iscsi.service:
        name: iscsi.service
        source: systemd
        state: inactive
        status: alias
      iscsid:
        name: iscsid
        source: sysv
        state: stopped
      iscsid.service:
        name: iscsid.service
        source: systemd
        state: stopped
        status: disabled
      kbd.service:
        name: kbd.service
        source: systemd
        state: stopped
        status: not-found
      keyboard-setup.service:
        name: keyboard-setup.service
        source: systemd
        state: stopped
        status: enabled
      keyboard-setup.sh:
        name: keyboard-setup.sh
        source: sysv
        state: stopped
      kmod:
        name: kmod
        source: sysv
        state: running
      kmod-static-nodes.service:
        name: kmod-static-nodes.service
        source: systemd
        state: stopped
        status: static
      kmod.service:
        name: kmod.service
        source: systemd
        state: active
        status: alias
      ldconfig.service:
        name: ldconfig.service
        source: systemd
        state: stopped
        status: static
      loaded:
        name: loaded
        source: systemd
        state: stopped
        status: failed
      logrotate.service:
        name: logrotate.service
        source: systemd
        state: stopped
        status: static
      lvm2-activation-early.service:
        name: lvm2-activation-early.service
        source: systemd
        state: stopped
        status: not-found
      lvm2-lvmpolld.service:
        name: lvm2-lvmpolld.service
        source: systemd
        state: stopped
        status: static
      lvm2-monitor.service:
        name: lvm2-monitor.service
        source: systemd
        state: stopped
        status: enabled
      lxd-agent.service:
        name: lxd-agent.service
        source: systemd
        state: inactive
        status: static
      lxd-installer@.service:
        name: lxd-installer@.service
        source: systemd
        state: unknown
        status: static
      man-db.service:
        name: man-db.service
        source: systemd
        state: stopped
        status: static
      mdadm-grow-continue@.service:
        name: mdadm-grow-continue@.service
        source: systemd
        state: unknown
        status: static
      mdadm-last-resort@.service:
        name: mdadm-last-resort@.service
        source: systemd
        state: unknown
        status: static
      mdcheck_continue.service:
        name: mdcheck_continue.service
        source: systemd
        state: inactive
        status: static
      mdcheck_start.service:
        name: mdcheck_start.service
        source: systemd
        state: inactive
        status: static
      mdmon@.service:
        name: mdmon@.service
        source: systemd
        state: unknown
        status: static
      mdmonitor-oneshot.service:
        name: mdmonitor-oneshot.service
        source: systemd
        state: inactive
        status: static
      mdmonitor.service:
        name: mdmonitor.service
        source: systemd
        state: inactive
        status: static
      modprobe@.service:
        name: modprobe@.service
        source: systemd
        state: unknown
        status: static
      modprobe@configfs.service:
        name: modprobe@configfs.service
        source: systemd
        state: stopped
        status: inactive
      modprobe@dm_mod.service:
        name: modprobe@dm_mod.service
        source: systemd
        state: stopped
        status: inactive
      modprobe@drm.service:
        name: modprobe@drm.service
        source: systemd
        state: stopped
        status: inactive
      modprobe@efi_pstore.service:
        name: modprobe@efi_pstore.service
        source: systemd
        state: stopped
        status: inactive
      modprobe@fuse.service:
        name: modprobe@fuse.service
        source: systemd
        state: stopped
        status: inactive
      modprobe@loop.service:
        name: modprobe@loop.service
        source: systemd
        state: stopped
        status: inactive
      motd-news.service:
        name: motd-news.service
        source: systemd
        state: stopped
        status: static
      multipath-tools-boot.service:
        name: multipath-tools-boot.service
        source: systemd
        state: inactive
        status: masked
      multipath-tools.service:
        name: multipath-tools.service
        source: systemd
        state: active
        status: alias
      multipathd.service:
        name: multipathd.service
        source: systemd
        state: running
        status: enabled
      netplan-ovs-cleanup.service:
        name: netplan-ovs-cleanup.service
        source: systemd
        state: stopped
        status: enabled-runtime
      networkd-dispatcher.service:
        name: networkd-dispatcher.service
        source: systemd
        state: stopped
        status: enabled
      nftables.service:
        name: nftables.service
        source: systemd
        state: inactive
        status: disabled
      open-iscsi:
        name: open-iscsi
        source: sysv
        state: stopped
      open-iscsi.service:
        name: open-iscsi.service
        source: systemd
        state: stopped
        status: enabled
      open-vm-tools:
        name: open-vm-tools
        source: sysv
        state: running
      open-vm-tools.service:
        name: open-vm-tools.service
        source: systemd
        state: running
        status: enabled
      ovsdb-server.service:
        name: ovsdb-server.service
        source: systemd
        state: stopped
        status: not-found
      packagekit-offline-update.service:
        name: packagekit-offline-update.service
        source: systemd
        state: inactive
        status: static
      packagekit.service:
        name: packagekit.service
        source: systemd
        state: inactive
        status: static
      pam_namespace.service:
        name: pam_namespace.service
        source: systemd
        state: inactive
        status: static
      plymouth:
        name: plymouth
        source: sysv
        state: running
      plymouth-halt.service:
        name: plymouth-halt.service
        source: systemd
        state: inactive
        status: static
      plymouth-kexec.service:
        name: plymouth-kexec.service
        source: systemd
        state: inactive
        status: static
      plymouth-log:
        name: plymouth-log
        source: sysv
        state: running
      plymouth-log.service:
        name: plymouth-log.service
        source: systemd
        state: active
        status: alias
      plymouth-poweroff.service:
        name: plymouth-poweroff.service
        source: systemd
        state: inactive
        status: static
      plymouth-quit-wait.service:
        name: plymouth-quit-wait.service
        source: systemd
        state: stopped
        status: static
      plymouth-quit.service:
        name: plymouth-quit.service
        source: systemd
        state: stopped
        status: static
      plymouth-read-write.service:
        name: plymouth-read-write.service
        source: systemd
        state: stopped
        status: static
      plymouth-reboot.service:
        name: plymouth-reboot.service
        source: systemd
        state: inactive
        status: static
      plymouth-start.service:
        name: plymouth-start.service
        source: systemd
        state: stopped
        status: static
      plymouth-switch-root-initramfs.service:
        name: plymouth-switch-root-initramfs.service
        source: systemd
        state: inactive
        status: static
      plymouth-switch-root.service:
        name: plymouth-switch-root.service
        source: systemd
        state: stopped
        status: static
      plymouth.service:
        name: plymouth.service
        source: systemd
        state: active
        status: alias
      polkit.service:
        name: polkit.service
        source: systemd
        state: running
        status: static
      pollinate.service:
        name: pollinate.service
        source: systemd
        state: stopped
        status: enabled
      procps:
        name: procps
        source: sysv
        state: running
      procps.service:
        name: procps.service
        source: systemd
        state: active
        status: alias
      quotaon.service:
        name: quotaon.service
        source: systemd
        state: inactive
        status: static
      rbdmap.service:
        name: rbdmap.service
        source: systemd
        state: stopped
        status: not-found
      rc-local.service:
        name: rc-local.service
        source: systemd
        state: stopped
        status: static
      rescue.service:
        name: rescue.service
        source: systemd
        state: stopped
        status: static
      rke2-agent.service:
        name: rke2-agent.service
        source: systemd
        state: stopped
        status: masked
      rke2-server.service:
        name: rke2-server.service
        source: systemd
        state: running
        status: enabled
      rsync:
        name: rsync
        source: sysv
        state: stopped
      rsync.service:
        name: rsync.service
        source: systemd
        state: inactive
        status: disabled
      rsyslog.service:
        name: rsyslog.service
        source: systemd
        state: running
        status: enabled
      screen-cleanup:
        name: screen-cleanup
        source: sysv
        state: stopped
      screen-cleanup.service:
        name: screen-cleanup.service
        source: systemd
        state: inactive
        status: masked
      secureboot-db.service:
        name: secureboot-db.service
        source: systemd
        state: stopped
        status: enabled
      serial-getty@.service:
        name: serial-getty@.service
        source: systemd
        state: unknown
        status: disabled
      setvtrgb.service:
        name: setvtrgb.service
        source: systemd
        state: stopped
        status: enabled
      snapd.apparmor.service:
        name: snapd.apparmor.service
        source: systemd
        state: stopped
        status: enabled
      snapd.autoimport.service:
        name: snapd.autoimport.service
        source: systemd
        state: stopped
        status: enabled
      snapd.core-fixup.service:
        name: snapd.core-fixup.service
        source: systemd
        state: stopped
        status: enabled
      snapd.failure.service:
        name: snapd.failure.service
        source: systemd
        state: stopped
        status: static
      snapd.recovery-chooser-trigger.service:
        name: snapd.recovery-chooser-trigger.service
        source: systemd
        state: stopped
        status: enabled
      snapd.seeded.service:
        name: snapd.seeded.service
        source: systemd
        state: stopped
        status: enabled
      snapd.service:
        name: snapd.service
        source: systemd
        state: stopped
        status: enabled
      snapd.snap-repair.service:
        name: snapd.snap-repair.service
        source: systemd
        state: stopped
        status: static
      snapd.system-shutdown.service:
        name: snapd.system-shutdown.service
        source: systemd
        state: stopped
        status: enabled
      ssh:
        name: ssh
        source: sysv
        state: running
      ssh.service:
        name: ssh.service
        source: systemd
        state: running
        status: disabled
      sudo.service:
        name: sudo.service
        source: systemd
        state: inactive
        status: masked
      syslog.service:
        name: syslog.service
        source: systemd
        state: active
        status: alias
      sysstat:
        name: sysstat
        source: sysv
        state: running
      sysstat-collect.service:
        name: sysstat-collect.service
        source: systemd
        state: stopped
        status: static
      sysstat-summary.service:
        name: sysstat-summary.service
        source: systemd
        state: stopped
        status: static
      sysstat.service:
        name: sysstat.service
        source: systemd
        state: stopped
        status: enabled
      system-update-cleanup.service:
        name: system-update-cleanup.service
        source: systemd
        state: inactive
        status: static
      systemd-ask-password-console.service:
        name: systemd-ask-password-console.service
        source: systemd
        state: stopped
        status: static
      systemd-ask-password-plymouth.service:
        name: systemd-ask-password-plymouth.service
        source: systemd
        state: stopped
        status: static
      systemd-ask-password-wall.service:
        name: systemd-ask-password-wall.service
        source: systemd
        state: stopped
        status: static
      systemd-backlight@.service:
        name: systemd-backlight@.service
        source: systemd
        state: unknown
        status: static
      systemd-battery-check.service:
        name: systemd-battery-check.service
        source: systemd
        state: stopped
        status: static
      systemd-binfmt.service:
        name: systemd-binfmt.service
        source: systemd
        state: stopped
        status: static
      systemd-boot-check-no-failures.service:
        name: systemd-boot-check-no-failures.service
        source: systemd
        state: inactive
        status: disabled
      systemd-bsod.service:
        name: systemd-bsod.service
        source: systemd
        state: stopped
        status: static
      systemd-confext.service:
        name: systemd-confext.service
        source: systemd
        state: inactive
        status: disabled
      systemd-exit.service:
        name: systemd-exit.service
        source: systemd
        state: inactive
        status: static
      systemd-firstboot.service:
        name: systemd-firstboot.service
        source: systemd
        state: stopped
        status: static
      systemd-fsck-root.service:
        name: systemd-fsck-root.service
        source: systemd
        state: stopped
        status: enabled-runtime
      systemd-fsck@.service:
        name: systemd-fsck@.service
        source: systemd
        state: unknown
        status: static
      systemd-fsck@dev-disk-by\x2duuid-9bb05da7\x2d85f0\x2d4338\x2d9b8f\x2d06ca0d639ac6.service:
        name: systemd-fsck@dev-disk-by\x2duuid-9bb05da7\x2d85f0\x2d4338\x2d9b8f\x2d06ca0d639ac6.service
        source: systemd
        state: stopped
        status: active
      systemd-fsckd.service:
        name: systemd-fsckd.service
        source: systemd
        state: stopped
        status: static
      systemd-growfs-root.service:
        name: systemd-growfs-root.service
        source: systemd
        state: inactive
        status: static
      systemd-growfs@.service:
        name: systemd-growfs@.service
        source: systemd
        state: unknown
        status: static
      systemd-halt.service:
        name: systemd-halt.service
        source: systemd
        state: inactive
        status: static
      systemd-hibernate-resume.service:
        name: systemd-hibernate-resume.service
        source: systemd
        state: stopped
        status: static
      systemd-hibernate.service:
        name: systemd-hibernate.service
        source: systemd
        state: stopped
        status: static
      systemd-hostnamed.service:
        name: systemd-hostnamed.service
        source: systemd
        state: inactive
        status: static
      systemd-hwdb-update.service:
        name: systemd-hwdb-update.service
        source: systemd
        state: stopped
        status: static
      systemd-hybrid-sleep.service:
        name: systemd-hybrid-sleep.service
        source: systemd
        state: stopped
        status: static
      systemd-initctl.service:
        name: systemd-initctl.service
        source: systemd
        state: stopped
        status: static
      systemd-journal-catalog-update.service:
        name: systemd-journal-catalog-update.service
        source: systemd
        state: stopped
        status: static
      systemd-journal-flush.service:
        name: systemd-journal-flush.service
        source: systemd
        state: stopped
        status: static
      systemd-journald.service:
        name: systemd-journald.service
        source: systemd
        state: running
        status: static
      systemd-journald@.service:
        name: systemd-journald@.service
        source: systemd
        state: unknown
        status: static
      systemd-kexec.service:
        name: systemd-kexec.service
        source: systemd
        state: inactive
        status: static
      systemd-localed.service:
        name: systemd-localed.service
        source: systemd
        state: inactive
        status: static
      systemd-logind.service:
        name: systemd-logind.service
        source: systemd
        state: running
        status: static
      systemd-machine-id-commit.service:
        name: systemd-machine-id-commit.service
        source: systemd
        state: stopped
        status: static
      systemd-modules-load.service:
        name: systemd-modules-load.service
        source: systemd
        state: stopped
        status: static
      systemd-network-generator.service:
        name: systemd-network-generator.service
        source: systemd
        state: inactive
        status: disabled
      systemd-networkd-wait-online.service:
        name: systemd-networkd-wait-online.service
        source: systemd
        state: stopped
        status: enabled
      systemd-networkd-wait-online@.service:
        name: systemd-networkd-wait-online@.service
        source: systemd
        state: unknown
        status: disabled
      systemd-networkd.service:
        name: systemd-networkd.service
        source: systemd
        state: running
        status: enabled
      systemd-oomd.service:
        name: systemd-oomd.service
        source: systemd
        state: stopped
        status: not-found
      systemd-pcrextend@.service:
        name: systemd-pcrextend@.service
        source: systemd
        state: unknown
        status: static
      systemd-pcrfs-root.service:
        name: systemd-pcrfs-root.service
        source: systemd
        state: inactive
        status: static
      systemd-pcrfs@.service:
        name: systemd-pcrfs@.service
        source: systemd
        state: unknown
        status: static
      systemd-pcrlock-file-system.service:
        name: systemd-pcrlock-file-system.service
        source: systemd
        state: inactive
        status: disabled
      systemd-pcrlock-firmware-code.service:
        name: systemd-pcrlock-firmware-code.service
        source: systemd
        state: inactive
        status: disabled
      systemd-pcrlock-firmware-config.service:
        name: systemd-pcrlock-firmware-config.service
        source: systemd
        state: inactive
        status: disabled
      systemd-pcrlock-machine-id.service:
        name: systemd-pcrlock-machine-id.service
        source: systemd
        state: inactive
        status: disabled
      systemd-pcrlock-make-policy.service:
        name: systemd-pcrlock-make-policy.service
        source: systemd
        state: inactive
        status: disabled
      systemd-pcrlock-secureboot-authority.service:
        name: systemd-pcrlock-secureboot-authority.service
        source: systemd
        state: inactive
        status: disabled
      systemd-pcrlock-secureboot-policy.service:
        name: systemd-pcrlock-secureboot-policy.service
        source: systemd
        state: inactive
        status: disabled
      systemd-pcrmachine.service:
        name: systemd-pcrmachine.service
        source: systemd
        state: stopped
        status: static
      systemd-pcrphase-initrd.service:
        name: systemd-pcrphase-initrd.service
        source: systemd
        state: stopped
        status: static
      systemd-pcrphase-sysinit.service:
        name: systemd-pcrphase-sysinit.service
        source: systemd
        state: stopped
        status: static
      systemd-pcrphase.service:
        name: systemd-pcrphase.service
        source: systemd
        state: stopped
        status: static
      systemd-poweroff.service:
        name: systemd-poweroff.service
        source: systemd
        state: inactive
        status: static
      systemd-pstore.service:
        name: systemd-pstore.service
        source: systemd
        state: stopped
        status: enabled
      systemd-quotacheck.service:
        name: systemd-quotacheck.service
        source: systemd
        state: stopped
        status: static
      systemd-random-seed.service:
        name: systemd-random-seed.service
        source: systemd
        state: stopped
        status: static
      systemd-reboot.service:
        name: systemd-reboot.service
        source: systemd
        state: inactive
        status: static
      systemd-remount-fs.service:
        name: systemd-remount-fs.service
        source: systemd
        state: stopped
        status: enabled-runtime
      systemd-repart.service:
        name: systemd-repart.service
        source: systemd
        state: stopped
        status: static
      systemd-resolved.service:
        name: systemd-resolved.service
        source: systemd
        state: running
        status: enabled
      systemd-rfkill.service:
        name: systemd-rfkill.service
        source: systemd
        state: stopped
        status: static
      systemd-soft-reboot.service:
        name: systemd-soft-reboot.service
        source: systemd
        state: stopped
        status: static
      systemd-storagetm.service:
        name: systemd-storagetm.service
        source: systemd
        state: inactive
        status: static
      systemd-suspend-then-hibernate.service:
        name: systemd-suspend-then-hibernate.service
        source: systemd
        state: stopped
        status: static
      systemd-suspend.service:
        name: systemd-suspend.service
        source: systemd
        state: stopped
        status: static
      systemd-sysctl.service:
        name: systemd-sysctl.service
        source: systemd
        state: stopped
        status: static
      systemd-sysext.service:
        name: systemd-sysext.service
        source: systemd
        state: stopped
        status: disabled
      systemd-sysext@.service:
        name: systemd-sysext@.service
        source: systemd
        state: unknown
        status: static
      systemd-sysupdate-reboot.service:
        name: systemd-sysupdate-reboot.service
        source: systemd
        state: inactive
        status: indirect
      systemd-sysupdate.service:
        name: systemd-sysupdate.service
        source: systemd
        state: inactive
        status: indirect
      systemd-sysusers.service:
        name: systemd-sysusers.service
        source: systemd
        state: stopped
        status: static
      systemd-time-wait-sync.service:
        name: systemd-time-wait-sync.service
        source: systemd
        state: inactive
        status: disabled
      systemd-timedated.service:
        name: systemd-timedated.service
        source: systemd
        state: inactive
        status: static
      systemd-timesyncd.service:
        name: systemd-timesyncd.service
        source: systemd
        state: running
        status: enabled
      systemd-tmpfiles-clean.service:
        name: systemd-tmpfiles-clean.service
        source: systemd
        state: stopped
        status: static
      systemd-tmpfiles-setup-dev-early.service:
        name: systemd-tmpfiles-setup-dev-early.service
        source: systemd
        state: stopped
        status: static
      systemd-tmpfiles-setup-dev.service:
        name: systemd-tmpfiles-setup-dev.service
        source: systemd
        state: stopped
        status: static
      systemd-tmpfiles-setup.service:
        name: systemd-tmpfiles-setup.service
        source: systemd
        state: stopped
        status: static
      systemd-tpm2-setup-early.service:
        name: systemd-tpm2-setup-early.service
        source: systemd
        state: stopped
        status: static
      systemd-tpm2-setup.service:
        name: systemd-tpm2-setup.service
        source: systemd
        state: stopped
        status: static
      systemd-udev-settle.service:
        name: systemd-udev-settle.service
        source: systemd
        state: stopped
        status: static
      systemd-udev-trigger.service:
        name: systemd-udev-trigger.service
        source: systemd
        state: stopped
        status: static
      systemd-udevd.service:
        name: systemd-udevd.service
        source: systemd
        state: running
        status: static
      systemd-update-done.service:
        name: systemd-update-done.service
        source: systemd
        state: stopped
        status: static
      systemd-update-utmp-runlevel.service:
        name: systemd-update-utmp-runlevel.service
        source: systemd
        state: stopped
        status: static
      systemd-update-utmp.service:
        name: systemd-update-utmp.service
        source: systemd
        state: stopped
        status: static
      systemd-user-sessions.service:
        name: systemd-user-sessions.service
        source: systemd
        state: stopped
        status: static
      systemd-vconsole-setup.service:
        name: systemd-vconsole-setup.service
        source: systemd
        state: stopped
        status: not-found
      systemd-volatile-root.service:
        name: systemd-volatile-root.service
        source: systemd
        state: inactive
        status: static
      thermald.service:
        name: thermald.service
        source: systemd
        state: stopped
        status: enabled
      tpm-udev.service:
        name: tpm-udev.service
        source: systemd
        state: stopped
        status: static
      ua-auto-attach.service:
        name: ua-auto-attach.service
        source: systemd
        state: stopped
        status: not-found
      ua-reboot-cmds.service:
        name: ua-reboot-cmds.service
        source: systemd
        state: stopped
        status: enabled
      ua-timer.service:
        name: ua-timer.service
        source: systemd
        state: stopped
        status: static
      ubuntu-advantage-cloud-id-shim.service:
        name: ubuntu-advantage-cloud-id-shim.service
        source: systemd
        state: stopped
        status: not-found
      ubuntu-advantage.service:
        name: ubuntu-advantage.service
        source: systemd
        state: stopped
        status: enabled
      udev.service:
        name: udev.service
        source: systemd
        state: active
        status: alias
      udisks2.service:
        name: udisks2.service
        source: systemd
        state: running
        status: enabled
      ufw:
        name: ufw
        source: sysv
        state: running
      ufw.service:
        name: ufw.service
        source: systemd
        state: stopped
        status: enabled
      unattended-upgrades:
        name: unattended-upgrades
        source: sysv
        state: running
      unattended-upgrades.service:
        name: unattended-upgrades.service
        source: systemd
        state: running
        status: enabled
      update-notifier-download.service:
        name: update-notifier-download.service
        source: systemd
        state: inactive
        status: static
      update-notifier-motd.service:
        name: update-notifier-motd.service
        source: systemd
        state: stopped
        status: static
      upower.service:
        name: upower.service
        source: systemd
        state: running
        status: disabled
      usb_modeswitch@.service:
        name: usb_modeswitch@.service
        source: systemd
        state: unknown
        status: static
      usbmuxd.service:
        name: usbmuxd.service
        source: systemd
        state: inactive
        status: static
      user-runtime-dir@.service:
        name: user-runtime-dir@.service
        source: systemd
        state: unknown
        status: static
      user-runtime-dir@1000.service:
        name: user-runtime-dir@1000.service
        source: systemd
        state: stopped
        status: active
      user@.service:
        name: user@.service
        source: systemd
        state: unknown
        status: static
      user@1000.service:
        name: user@1000.service
        source: systemd
        state: running
        status: active
      uuidd:
        name: uuidd
        source: sysv
        state: stopped
      uuidd.service:
        name: uuidd.service
        source: systemd
        state: stopped
        status: indirect
      vgauth.service:
        name: vgauth.service
        source: systemd
        state: running
        status: enabled
      vmtoolsd.service:
        name: vmtoolsd.service
        source: systemd
        state: active
        status: alias
      x11-common.service:
        name: x11-common.service
        source: systemd
        state: inactive
        status: masked
      xfs_scrub@.service:
        name: xfs_scrub@.service
        source: systemd
        state: unknown
        status: static
      xfs_scrub_all.service:
        name: xfs_scrub_all.service
        source: systemd
        state: inactive
        status: static
      xfs_scrub_fail@.service:
        name: xfs_scrub_fail@.service
        source: systemd
        state: unknown
        status: static
      zfs-mount.service:
        name: zfs-mount.service
        source: systemd
        state: stopped
        status: not-found
  invocation:
    module_args: {}
2025-10-25 17:45:10,846 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Set the Active Server variable] **********************************
2025-10-25 17:45:10,847 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=k8s-m1) => changed=false 
  ansible_facts:
    active_server: k8s-m1
  ansible_loop_var: item
  item: k8s-m1
2025-10-25 17:45:10,873 p=55263 u=root n=ansible | ok: [k8s-m1 -> k8s-w01(10.0.6.14)] => (item=k8s-w01) => changed=false 
  ansible_facts:
    active_server: k8s-m1
  ansible_loop_var: item
  item: k8s-w01
2025-10-25 17:45:10,958 p=55263 u=root n=ansible | ok: [k8s-m1 -> k8s-w02(10.0.6.15)] => (item=k8s-w02) => changed=false 
  ansible_facts:
    active_server: k8s-m1
  ansible_loop_var: item
  item: k8s-w02
2025-10-25 17:45:10,992 p=55263 u=root n=ansible | ok: [k8s-m1 -> k8s-w03(10.0.6.16)] => (item=k8s-w03) => changed=false 
  ansible_facts:
    active_server: k8s-m1
  ansible_loop_var: item
  item: k8s-w03
2025-10-25 17:45:11,323 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Download kubeconfig to ansible localhost] ************************
2025-10-25 17:45:11,339 p=55263 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/download_kubeconfig.yaml for k8s-m1
2025-10-25 17:45:11,592 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/slurp.py
2025-10-25 17:45:11,593 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:45:11,593 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:11,593 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=xzlydhlwlgnbjlaaltpwjlrcegoxbecd] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-xzlydhlwlgnbjlaaltpwjlrcegoxbecd ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:45:11,634 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:11,886 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"content": "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", "source": "/etc/rancher/rke2/rke2.yaml", "encoding": "base64", "invocation": {"module_args": {"src": "/etc/rancher/rke2/rke2.yaml"}}}\n', b'')
2025-10-25 17:45:11,891 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 kubeconfig to localhost] ***************************
2025-10-25 17:45:11,892 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  checksum: 82cd5e77fc3b8337eece997592ea1a0d86f0abd8
  dest: /tmp/rke2.yaml
  file: /etc/rancher/rke2/rke2.yaml
  md5sum: 822a5e1f3f6c242c317604d70f723f1e
2025-10-25 17:45:12,047 p=55263 u=root n=ansible | <localhost> ESTABLISH LOCAL CONNECTION FOR USER: root
2025-10-25 17:45:12,047 p=55263 u=root n=ansible | <localhost> EXEC /bin/sh -c 'echo ~root && sleep 0'
2025-10-25 17:45:12,052 p=55263 u=root n=ansible | <localhost> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /root/.ansible/tmp `"&& mkdir "` echo /root/.ansible/tmp/ansible-tmp-1761389112.051615-58075-205824402886225 `" && echo ansible-tmp-1761389112.051615-58075-205824402886225="` echo /root/.ansible/tmp/ansible-tmp-1761389112.051615-58075-205824402886225 `" ) && sleep 0'
2025-10-25 17:45:12,195 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/replace.py
2025-10-25 17:45:12,196 p=55263 u=root n=ansible | <localhost> PUT /root/.ansible/tmp/ansible-local-5526377e173g8/tmpck7khrfv TO /root/.ansible/tmp/ansible-tmp-1761389112.051615-58075-205824402886225/AnsiballZ_replace.py
2025-10-25 17:45:12,196 p=55263 u=root n=ansible | <localhost> EXEC /bin/sh -c 'chmod u+x /root/.ansible/tmp/ansible-tmp-1761389112.051615-58075-205824402886225/ /root/.ansible/tmp/ansible-tmp-1761389112.051615-58075-205824402886225/AnsiballZ_replace.py && sleep 0'
2025-10-25 17:45:12,207 p=55263 u=root n=ansible | <localhost> EXEC /bin/sh -c '/usr/bin/python3.12 /root/.ansible/tmp/ansible-tmp-1761389112.051615-58075-205824402886225/AnsiballZ_replace.py && sleep 0'
2025-10-25 17:45:12,467 p=55263 u=root n=ansible | <localhost> EXEC /bin/sh -c 'rm -f -r /root/.ansible/tmp/ansible-tmp-1761389112.051615-58075-205824402886225/ > /dev/null 2>&1 && sleep 0'
2025-10-25 17:45:12,475 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Replace loopback IP by master server IP] *************************
2025-10-25 17:45:12,476 p=55263 u=root n=ansible | changed: [k8s-m1 -> localhost] => changed=true 
  invocation:
    module_args:
      after: null
      attributes: null
      backup: false
      before: null
      encoding: utf-8
      group: null
      mode: '0600'
      owner: null
      path: /tmp/rke2.yaml
      regexp: 127\.0\.0\.1|\[::1\]
      replace: 10.0.6.11
      selevel: null
      serole: null
      setype: null
      seuser: null
      unsafe_writes: false
      validate: null
  msg: 1 replacements made
  rc: 0
2025-10-25 17:45:12,583 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Prepare and join remaining nodes of the cluster] *****************
2025-10-25 17:45:12,607 p=55263 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/remaining_nodes.yml for k8s-m1
2025-10-25 17:45:12,672 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/file.py
2025-10-25 17:45:12,672 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:45:12,673 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:12,674 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=plvjphtvezuuldojrxxihmuvstnrmxux] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-plvjphtvezuuldojrxxihmuvstnrmxux ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:45:12,714 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:12,996 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"path": "/etc/rancher/rke2", "changed": false, "diff": {"before": {"path": "/etc/rancher/rke2"}, "after": {"path": "/etc/rancher/rke2"}}, "uid": 0, "gid": 0, "owner": "root", "group": "root", "mode": "0755", "state": "directory", "size": 86, "invocation": {"module_args": {"state": "directory", "path": "/etc/rancher/rke2", "owner": "root", "group": "root", "mode": 493, "recurse": false, "force": false, "follow": true, "modification_time_format": "%Y%m%d%H%M.%S", "access_time_format": "%Y%m%d%H%M.%S", "unsafe_writes": false, "_original_basename": null, "_diff_peek": null, "src": null, "modification_time": null, "access_time": null, "seuser": null, "serole": null, "selevel": null, "setype": null, "attributes": null}}}\n', b'')
2025-10-25 17:45:12,999 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Create the RKE2 config dir] **************************************
2025-10-25 17:45:13,000 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  diff:
    after:
      path: /etc/rancher/rke2
    before:
      path: /etc/rancher/rke2
  gid: 0
  group: root
  invocation:
    module_args:
      _diff_peek: null
      _original_basename: null
      access_time: null
      access_time_format: '%Y%m%d%H%M.%S'
      attributes: null
      follow: true
      force: false
      group: root
      mode: 493
      modification_time: null
      modification_time_format: '%Y%m%d%H%M.%S'
      owner: root
      path: /etc/rancher/rke2
      recurse: false
      selevel: null
      serole: null
      setype: null
      seuser: null
      src: null
      state: directory
      unsafe_writes: false
  mode: '0755'
  owner: root
  path: /etc/rancher/rke2
  size: 86
  state: directory
  uid: 0
2025-10-25 17:45:13,043 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Set server taints] ***********************************************
2025-10-25 17:45:13,043 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  ansible_facts:
    combined_node_taints:
    - CriticalAddonsOnly=true:NoExecute
    - node-role.kubernetes.io/control-plane=true:NoSchedule
    - node-role.kubernetes.io/etcd=true:NoExecute
2025-10-25 17:45:13,106 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:13,106 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'echo ~ubuntu && sleep 0'"'"''
2025-10-25 17:45:13,125 p=55263 u=root n=ansible | <10.0.6.11> (0, b'/home/ubuntu\n', b'')
2025-10-25 17:45:13,126 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:13,126 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'( umask 77 && mkdir -p "` echo /home/ubuntu/.ansible/tmp `"&& mkdir "` echo /home/ubuntu/.ansible/tmp/ansible-tmp-1761389113.1258295-58148-155435560707935 `" && echo ansible-tmp-1761389113.1258295-58148-155435560707935="` echo /home/ubuntu/.ansible/tmp/ansible-tmp-1761389113.1258295-58148-155435560707935 `" ) && sleep 0'"'"''
2025-10-25 17:45:13,155 p=55263 u=root n=ansible | <10.0.6.11> (0, b'ansible-tmp-1761389113.1258295-58148-155435560707935=/home/ubuntu/.ansible/tmp/ansible-tmp-1761389113.1258295-58148-155435560707935\n', b'')
2025-10-25 17:45:13,265 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/stat.py
2025-10-25 17:45:13,265 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:45:13,266 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:13,266 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=zbyeqzdofwsvvemdydlsoqcstipxphis] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-zbyeqzdofwsvvemdydlsoqcstipxphis ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:45:13,314 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:13,657 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "stat": {"exists": true, "path": "/etc/rancher/rke2/config.yaml", "mode": "0600", "isdir": false, "ischr": false, "isblk": false, "isreg": true, "isfifo": false, "islnk": false, "issock": false, "uid": 0, "gid": 0, "size": 932, "inode": 34771521, "dev": 64512, "nlink": 1, "atime": 1761388373.4950278, "mtime": 1761388370.1620278, "ctime": 1761388370.464028, "wusr": true, "rusr": true, "xusr": false, "wgrp": false, "rgrp": false, "xgrp": false, "woth": false, "roth": false, "xoth": false, "isuid": false, "isgid": false, "blocks": 8, "block_size": 4096, "device_type": 0, "readable": true, "writeable": true, "executable": false, "pw_name": "root", "gr_name": "root", "checksum": "098cada71bdccdd1cab3dc5de6c016a7d3c2163b", "mimetype": "text/plain", "charset": "us-ascii", "version": "2464113231", "attributes": [], "attr_flags": ""}, "invocation": {"module_args": {"path": "/etc/rancher/rke2/config.yaml", "follow": false, "get_checksum": true, "checksum_algorithm": "sha1", "get_mime": true, "get_attributes": true}}}\n', b'')
2025-10-25 17:45:13,658 p=55263 u=root n=ansible | <10.0.6.11> PUT /root/.ansible/tmp/ansible-local-5526377e173g8/tmpyf29764v/config.yaml.j2 TO /home/ubuntu/.ansible/tmp/ansible-tmp-1761389113.1258295-58148-155435560707935/source
2025-10-25 17:45:13,658 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC sftp -b - -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' '[10.0.6.11]'
2025-10-25 17:45:13,688 p=55263 u=root n=ansible | <10.0.6.11> (0, b'sftp> put /root/.ansible/tmp/ansible-local-5526377e173g8/tmpyf29764v/config.yaml.j2 /home/ubuntu/.ansible/tmp/ansible-tmp-1761389113.1258295-58148-155435560707935/source\n', b'')
2025-10-25 17:45:13,688 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:13,688 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'chmod u+x /home/ubuntu/.ansible/tmp/ansible-tmp-1761389113.1258295-58148-155435560707935/ /home/ubuntu/.ansible/tmp/ansible-tmp-1761389113.1258295-58148-155435560707935/source && sleep 0'"'"''
2025-10-25 17:45:13,709 p=55263 u=root n=ansible | <10.0.6.11> (0, b'', b'')
2025-10-25 17:45:13,868 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/copy.py
2025-10-25 17:45:13,868 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:45:13,868 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:13,868 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=izktegikyqmwhhjugblvtcnpdomsassa] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-izktegikyqmwhhjugblvtcnpdomsassa ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:45:13,913 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:14,178 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"dest": "/etc/rancher/rke2/config.yaml", "src": "/home/ubuntu/.ansible/tmp/ansible-tmp-1761389113.1258295-58148-155435560707935/source", "md5sum": "5ba04904abeb21dd8df662fe249785c4", "checksum": "fd2894d1c8c05b680da6278db606ff0bec2fbca9", "changed": true, "uid": 0, "gid": 0, "owner": "root", "group": "root", "mode": "0600", "state": "file", "size": 963, "invocation": {"module_args": {"src": "/home/ubuntu/.ansible/tmp/ansible-tmp-1761389113.1258295-58148-155435560707935/source", "dest": "/etc/rancher/rke2/config.yaml", "owner": "root", "group": "root", "mode": 384, "follow": false, "_original_basename": "config.yaml.j2", "checksum": "fd2894d1c8c05b680da6278db606ff0bec2fbca9", "backup": false, "force": true, "unsafe_writes": false, "content": null, "validate": null, "directory_mode": null, "remote_src": null, "local_follow": null, "seuser": null, "serole": null, "selevel": null, "setype": null, "attributes": null}}}\n', b'')
2025-10-25 17:45:14,179 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:14,180 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'rm -f -r /home/ubuntu/.ansible/tmp/ansible-tmp-1761389113.1258295-58148-155435560707935/ > /dev/null 2>&1 && sleep 0'"'"''
2025-10-25 17:45:14,209 p=55263 u=root n=ansible | <10.0.6.11> (0, b'', b'')
2025-10-25 17:45:14,211 p=55263 u=root n=ansible | Notification for handler Config file changed has been saved.
2025-10-25 17:45:14,212 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Copy RKE2 config] ************************************************
2025-10-25 17:45:14,215 p=55263 u=root n=ansible | changed: [k8s-m1] => changed=true 
  checksum: fd2894d1c8c05b680da6278db606ff0bec2fbca9
  dest: /etc/rancher/rke2/config.yaml
  diff: []
  gid: 0
  group: root
  invocation:
    module_args:
      _original_basename: config.yaml.j2
      attributes: null
      backup: false
      checksum: fd2894d1c8c05b680da6278db606ff0bec2fbca9
      content: null
      dest: /etc/rancher/rke2/config.yaml
      directory_mode: null
      follow: false
      force: true
      group: root
      local_follow: null
      mode: 384
      owner: root
      remote_src: null
      selevel: null
      serole: null
      setype: null
      seuser: null
      src: /home/ubuntu/.ansible/tmp/ansible-tmp-1761389113.1258295-58148-155435560707935/source
      unsafe_writes: false
      validate: null
  md5sum: 5ba04904abeb21dd8df662fe249785c4
  mode: '0600'
  owner: root
  size: 963
  src: /home/ubuntu/.ansible/tmp/ansible-tmp-1761389113.1258295-58148-155435560707935/source
  state: file
  uid: 0
2025-10-25 17:45:14,346 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:14,347 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'echo ~ubuntu && sleep 0'"'"''
2025-10-25 17:45:14,372 p=55263 u=root n=ansible | <10.0.6.11> (0, b'/home/ubuntu\n', b'')
2025-10-25 17:45:14,372 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:14,372 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'( umask 77 && mkdir -p "` echo /home/ubuntu/.ansible/tmp `"&& mkdir "` echo /home/ubuntu/.ansible/tmp/ansible-tmp-1761389114.3721242-58201-67626655452043 `" && echo ansible-tmp-1761389114.3721242-58201-67626655452043="` echo /home/ubuntu/.ansible/tmp/ansible-tmp-1761389114.3721242-58201-67626655452043 `" ) && sleep 0'"'"''
2025-10-25 17:45:14,398 p=55263 u=root n=ansible | <10.0.6.11> (0, b'ansible-tmp-1761389114.3721242-58201-67626655452043=/home/ubuntu/.ansible/tmp/ansible-tmp-1761389114.3721242-58201-67626655452043\n', b'')
2025-10-25 17:45:14,425 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/stat.py
2025-10-25 17:45:14,426 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:45:14,426 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:14,426 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=uybqnqbyjrsucwwvahodlbfsjodgqozx] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-uybqnqbyjrsucwwvahodlbfsjodgqozx ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:45:14,469 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:15,765 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": false, "stat": {"exists": true, "path": "/etc/rancher/rke2/registries.yaml", "mode": "0600", "isdir": false, "ischr": false, "isblk": false, "isreg": true, "isfifo": false, "islnk": false, "issock": false, "uid": 0, "gid": 0, "size": 396, "inode": 995087, "dev": 64512, "nlink": 1, "atime": 1761388375.9260278, "mtime": 1761388371.0280278, "ctime": 1761388371.3370278, "wusr": true, "rusr": true, "xusr": false, "wgrp": false, "rgrp": false, "xgrp": false, "woth": false, "roth": false, "xoth": false, "isuid": false, "isgid": false, "blocks": 8, "block_size": 4096, "device_type": 0, "readable": true, "writeable": true, "executable": false, "pw_name": "root", "gr_name": "root", "checksum": "9ec6bb33716d3b345dccb88039fed997604acd9f", "mimetype": "text/plain", "charset": "us-ascii", "version": "3278074088", "attributes": [], "attr_flags": ""}, "invocation": {"module_args": {"path": "/etc/rancher/rke2/registries.yaml", "follow": false, "get_checksum": true, "checksum_algorithm": "sha1", "get_mime": true, "get_attributes": true}}}\n', b'')
2025-10-25 17:45:15,767 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/file.py
2025-10-25 17:45:15,767 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:45:15,768 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:15,768 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=swktofipllukuychadyfslpwbtjucyuj] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-swktofipllukuychadyfslpwbtjucyuj ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:45:15,805 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:16,103 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"path": "/etc/rancher/rke2/registries.yaml", "changed": false, "diff": {"before": {"path": "/etc/rancher/rke2/registries.yaml"}, "after": {"path": "/etc/rancher/rke2/registries.yaml"}}, "uid": 0, "gid": 0, "owner": "root", "group": "root", "mode": "0600", "state": "file", "size": 396, "invocation": {"module_args": {"owner": "root", "group": "root", "mode": 384, "dest": "/etc/rancher/rke2/registries.yaml", "_original_basename": "registries.yaml.j2", "recurse": false, "state": "file", "path": "/etc/rancher/rke2/registries.yaml", "force": false, "follow": true, "modification_time_format": "%Y%m%d%H%M.%S", "access_time_format": "%Y%m%d%H%M.%S", "unsafe_writes": false, "_diff_peek": null, "src": null, "modification_time": null, "access_time": null, "seuser": null, "serole": null, "selevel": null, "setype": null, "attributes": null}}}\n', b'')
2025-10-25 17:45:16,104 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:16,105 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'rm -f -r /home/ubuntu/.ansible/tmp/ansible-tmp-1761389114.3721242-58201-67626655452043/ > /dev/null 2>&1 && sleep 0'"'"''
2025-10-25 17:45:16,124 p=55263 u=root n=ansible | <10.0.6.11> (0, b'', b'')
2025-10-25 17:45:16,127 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Copy Containerd Registry Configuration file] *********************
2025-10-25 17:45:16,128 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  checksum: 9ec6bb33716d3b345dccb88039fed997604acd9f
  dest: /etc/rancher/rke2/registries.yaml
  diff:
    after:
      path: /etc/rancher/rke2/registries.yaml
    before:
      path: /etc/rancher/rke2/registries.yaml
  gid: 0
  group: root
  invocation:
    module_args:
      _diff_peek: null
      _original_basename: registries.yaml.j2
      access_time: null
      access_time_format: '%Y%m%d%H%M.%S'
      attributes: null
      dest: /etc/rancher/rke2/registries.yaml
      follow: true
      force: false
      group: root
      mode: 384
      modification_time: null
      modification_time_format: '%Y%m%d%H%M.%S'
      owner: root
      path: /etc/rancher/rke2/registries.yaml
      recurse: false
      selevel: null
      serole: null
      setype: null
      seuser: null
      src: null
      state: file
      unsafe_writes: false
  mode: '0600'
  owner: root
  path: /etc/rancher/rke2/registries.yaml
  size: 396
  state: file
  uid: 0
2025-10-25 17:45:16,511 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/systemd.py
2025-10-25 17:45:16,512 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:45:16,512 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:16,513 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=akpagndsfmclidoouttzfxokbkizvfqd] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-akpagndsfmclidoouttzfxokbkizvfqd ; RKE2_TOKEN=O+HkGqhGqCukrMS5 /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:45:16,566 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:17,086 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"name": "rke2-server.service", "changed": false, "status": {"Type": "notify", "ExitType": "main", "Restart": "always", "RestartMode": "normal", "NotifyAccess": "main", "RestartUSec": "5s", "RestartSteps": "0", "RestartMaxDelayUSec": "infinity", "RestartUSecNext": "5s", "TimeoutStartUSec": "infinity", "TimeoutStopUSec": "1min 30s", "TimeoutAbortUSec": "1min 30s", "TimeoutStartFailureMode": "terminate", "TimeoutStopFailureMode": "terminate", "RuntimeMaxUSec": "infinity", "RuntimeRandomizedExtraUSec": "0", "WatchdogUSec": "0", "WatchdogTimestampMonotonic": "0", "RootDirectoryStartOnly": "no", "RemainAfterExit": "no", "GuessMainPID": "yes", "MainPID": "9853", "ControlPID": "0", "FileDescriptorStoreMax": "0", "NFileDescriptorStore": "0", "FileDescriptorStorePreserve": "restart", "StatusErrno": "0", "Result": "success", "ReloadResult": "success", "CleanResult": "success", "UID": "[not set]", "GID": "[not set]", "NRestarts": "0", "OOMPolicy": "continue", "ReloadSignal": "1", "ExecMainStartTimestamp": "Sat 2025-10-25 10:32:53 UTC", "ExecMainStartTimestampMonotonic": "7092425848", "ExecMainExitTimestampMonotonic": "0", "ExecMainPID": "9853", "ExecMainCode": "0", "ExecMainStatus": "0", "ExecStartPre": "{ path=/sbin/modprobe ; argv[]=/sbin/modprobe overlay ; ignore_errors=yes ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStartPreEx": "{ path=/sbin/modprobe ; argv[]=/sbin/modprobe overlay ; flags=ignore-failure ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStart": "{ path=/usr/local/bin/rke2 ; argv[]=/usr/local/bin/rke2 server ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStartEx": "{ path=/usr/local/bin/rke2 ; argv[]=/usr/local/bin/rke2 server ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStopPost": "{ path=/bin/sh ; argv[]=/bin/sh -c systemd-cgls /system.slice/rke2-server.service | grep -Eo \'[0-9]+ (containerd|kubelet)\' | awk \'{print $1}\' | xargs -r kill ; ignore_errors=yes ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStopPostEx": "{ path=/bin/sh ; argv[]=/bin/sh -c systemd-cgls /system.slice/rke2-server.service | grep -Eo \'[0-9]+ (containerd|kubelet)\' | awk \'{print $1}\' | xargs -r kill ; flags=ignore-failure ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "Slice": "system.slice", "ControlGroup": "/system.slice/rke2-server.service", "ControlGroupId": "5969", "MemoryCurrent": "960663552", "MemoryPeak": "3275689984", "MemorySwapCurrent": "192512", "MemorySwapPeak": "192512", "MemoryZSwapCurrent": "0", "MemoryAvailable": "2728685568", "CPUUsageNSec": "269483847000", "EffectiveCPUs": "0-3", "EffectiveMemoryNodes": "0", "TasksCurrent": "134", "IPIngressBytes": "[no data]", "IPIngressPackets": "[no data]", "IPEgressBytes": "[no data]", "IPEgressPackets": "[no data]", "IOReadBytes": "[not set]", "IOReadOperations": "[not set]", "IOWriteBytes": "[not set]", "IOWriteOperations": "[not set]", "Delegate": "yes", "DelegateControllers": "cpu cpuset io memory pids", "CPUAccounting": "yes", "CPUWeight": "[not set]", "StartupCPUWeight": "[not set]", "CPUShares": "[not set]", "StartupCPUShares": "[not set]", "CPUQuotaPerSecUSec": "infinity", "CPUQuotaPeriodUSec": "infinity", "IOAccounting": "no", "IOWeight": "[not set]", "StartupIOWeight": "[not set]", "BlockIOAccounting": "no", "BlockIOWeight": "[not set]", "StartupBlockIOWeight": "[not set]", "MemoryAccounting": "yes", "DefaultMemoryLow": "0", "DefaultStartupMemoryLow": "0", "DefaultMemoryMin": "0", "MemoryMin": "0", "MemoryLow": "0", "StartupMemoryLow": "0", "MemoryHigh": "infinity", "StartupMemoryHigh": "infinity", "MemoryMax": "infinity", "StartupMemoryMax": "infinity", "MemorySwapMax": "infinity", "StartupMemorySwapMax": "infinity", "MemoryZSwapMax": "infinity", "StartupMemoryZSwapMax": "infinity", "MemoryLimit": "infinity", "DevicePolicy": "auto", "TasksAccounting": "yes", "TasksMax": "infinity", "IPAccounting": "no", "ManagedOOMSwap": "auto", "ManagedOOMMemoryPressure": "auto", "ManagedOOMMemoryPressureLimit": "0", "ManagedOOMPreference": "none", "MemoryPressureWatch": "auto", "MemoryPressureThresholdUSec": "200ms", "CoredumpReceive": "no", "EnvironmentFiles": "/usr/local/lib/systemd/system/rke2-server.env (ignore_errors=yes)", "UMask": "0022", "LimitCPU": "infinity", "LimitCPUSoft": "infinity", "LimitFSIZE": "infinity", "LimitFSIZESoft": "infinity", "LimitDATA": "infinity", "LimitDATASoft": "infinity", "LimitSTACK": "infinity", "LimitSTACKSoft": "8388608", "LimitCORE": "infinity", "LimitCORESoft": "infinity", "LimitRSS": "infinity", "LimitRSSSoft": "infinity", "LimitNOFILE": "1048576", "LimitNOFILESoft": "1048576", "LimitAS": "infinity", "LimitASSoft": "infinity", "LimitNPROC": "infinity", "LimitNPROCSoft": "infinity", "LimitMEMLOCK": "8388608", "LimitMEMLOCKSoft": "8388608", "LimitLOCKS": "infinity", "LimitLOCKSSoft": "infinity", "LimitSIGPENDING": "15158", "LimitSIGPENDINGSoft": "15158", "LimitMSGQUEUE": "819200", "LimitMSGQUEUESoft": "819200", "LimitNICE": "0", "LimitNICESoft": "0", "LimitRTPRIO": "0", "LimitRTPRIOSoft": "0", "LimitRTTIME": "infinity", "LimitRTTIMESoft": "infinity", "RootEphemeral": "no", "OOMScoreAdjust": "0", "CoredumpFilter": "0x33", "Nice": "0", "IOSchedulingClass": "2", "IOSchedulingPriority": "4", "CPUSchedulingPolicy": "0", "CPUSchedulingPriority": "0", "CPUAffinityFromNUMA": "no", "NUMAPolicy": "n/a", "TimerSlackNSec": "50000", "CPUSchedulingResetOnFork": "no", "NonBlocking": "no", "StandardInput": "null", "StandardOutput": "journal", "StandardError": "inherit", "TTYReset": "no", "TTYVHangup": "no", "TTYVTDisallocate": "no", "SyslogPriority": "30", "SyslogLevelPrefix": "yes", "SyslogLevel": "6", "SyslogFacility": "3", "LogLevelMax": "-1", "LogRateLimitIntervalUSec": "0", "LogRateLimitBurst": "0", "SecureBits": "0", "CapabilityBoundingSet": "cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read cap_perfmon cap_bpf cap_checkpoint_restore", "DynamicUser": "no", "SetLoginEnvironment": "no", "RemoveIPC": "no", "PrivateTmp": "no", "PrivateDevices": "no", "ProtectClock": "no", "ProtectKernelTunables": "no", "ProtectKernelModules": "no", "ProtectKernelLogs": "no", "ProtectControlGroups": "no", "PrivateNetwork": "no", "PrivateUsers": "no", "PrivateMounts": "no", "PrivateIPC": "no", "ProtectHome": "no", "ProtectSystem": "no", "SameProcessGroup": "no", "UtmpMode": "init", "IgnoreSIGPIPE": "yes", "NoNewPrivileges": "no", "SystemCallErrorNumber": "2147483646", "LockPersonality": "no", "RuntimeDirectoryPreserve": "no", "RuntimeDirectoryMode": "0755", "StateDirectoryMode": "0755", "CacheDirectoryMode": "0755", "LogsDirectoryMode": "0755", "ConfigurationDirectoryMode": "0755", "TimeoutCleanUSec": "infinity", "MemoryDenyWriteExecute": "no", "RestrictRealtime": "no", "RestrictSUIDSGID": "no", "RestrictNamespaces": "no", "MountAPIVFS": "no", "KeyringMode": "private", "ProtectProc": "default", "ProcSubset": "all", "ProtectHostname": "no", "MemoryKSM": "no", "RootImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "MountImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "ExtensionImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "KillMode": "process", "KillSignal": "15", "RestartKillSignal": "15", "FinalKillSignal": "9", "SendSIGKILL": "yes", "SendSIGHUP": "no", "WatchdogSignal": "6", "Id": "rke2-server.service", "Names": "rke2-server.service", "Requires": "system.slice sysinit.target", "Wants": "network-online.target", "WantedBy": "multi-user.target", "Conflicts": "shutdown.target rke2-agent.service", "Before": "multi-user.target shutdown.target", "After": "network-online.target basic.target system.slice systemd-journald.socket sysinit.target", "Documentation": "https://github.com/rancher/rke2#readme", "Description": "Rancher Kubernetes Engine v2 (server)", "LoadState": "loaded", "ActiveState": "active", "FreezerState": "running", "SubState": "running", "FragmentPath": "/usr/local/lib/systemd/system/rke2-server.service", "UnitFileState": "enabled", "UnitFilePreset": "enabled", "StateChangeTimestamp": "Sat 2025-10-25 10:35:37 UTC", "StateChangeTimestampMonotonic": "7257060396", "InactiveExitTimestamp": "Sat 2025-10-25 10:32:53 UTC", "InactiveExitTimestampMonotonic": "7092379564", "ActiveEnterTimestamp": "Sat 2025-10-25 10:35:37 UTC", "ActiveEnterTimestampMonotonic": "7257060396", "ActiveExitTimestampMonotonic": "0", "InactiveEnterTimestampMonotonic": "0", "CanStart": "yes", "CanStop": "yes", "CanReload": "no", "CanIsolate": "no", "CanFreeze": "yes", "StopWhenUnneeded": "no", "RefuseManualStart": "no", "RefuseManualStop": "no", "AllowIsolate": "no", "DefaultDependencies": "yes", "SurviveFinalKillSignal": "no", "OnSuccessJobMode": "fail", "OnFailureJobMode": "replace", "IgnoreOnIsolate": "no", "NeedDaemonReload": "no", "JobTimeoutUSec": "infinity", "JobRunningTimeoutUSec": "infinity", "JobTimeoutAction": "none", "ConditionResult": "yes", "AssertResult": "yes", "ConditionTimestamp": "Sat 2025-10-25 10:32:53 UTC", "ConditionTimestampMonotonic": "7092357804", "AssertTimestamp": "Sat 2025-10-25 10:32:53 UTC", "AssertTimestampMonotonic": "7092357806", "Transient": "no", "Perpetual": "no", "StartLimitIntervalUSec": "10s", "StartLimitBurst": "5", "StartLimitAction": "none", "FailureAction": "none", "SuccessAction": "none", "InvocationID": "b0d2a2fbe21c4c8f9d7bffc99a9906ef", "CollectMode": "inactive"}, "enabled": true, "state": "started", "invocation": {"module_args": {"name": "rke2-server.service", "state": "started", "enabled": true, "daemon_reload": false, "daemon_reexec": false, "scope": "system", "no_block": false, "force": null, "masked": null}}}\n', b'')
2025-10-25 17:45:17,097 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Start RKE2 service on the rest of the nodes] *********************
2025-10-25 17:45:17,102 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  attempts: 1
  enabled: true
  invocation:
    module_args:
      daemon_reexec: false
      daemon_reload: false
      enabled: true
      force: null
      masked: null
      name: rke2-server.service
      no_block: false
      scope: system
      state: started
  name: rke2-server.service
  state: started
  status:
    ActiveEnterTimestamp: Sat 2025-10-25 10:35:37 UTC
    ActiveEnterTimestampMonotonic: '7257060396'
    ActiveExitTimestampMonotonic: '0'
    ActiveState: active
    After: network-online.target basic.target system.slice systemd-journald.socket sysinit.target
    AllowIsolate: 'no'
    AssertResult: 'yes'
    AssertTimestamp: Sat 2025-10-25 10:32:53 UTC
    AssertTimestampMonotonic: '7092357806'
    Before: multi-user.target shutdown.target
    BlockIOAccounting: 'no'
    BlockIOWeight: '[not set]'
    CPUAccounting: 'yes'
    CPUAffinityFromNUMA: 'no'
    CPUQuotaPerSecUSec: infinity
    CPUQuotaPeriodUSec: infinity
    CPUSchedulingPolicy: '0'
    CPUSchedulingPriority: '0'
    CPUSchedulingResetOnFork: 'no'
    CPUShares: '[not set]'
    CPUUsageNSec: '269483847000'
    CPUWeight: '[not set]'
    CacheDirectoryMode: '0755'
    CanFreeze: 'yes'
    CanIsolate: 'no'
    CanReload: 'no'
    CanStart: 'yes'
    CanStop: 'yes'
    CapabilityBoundingSet: cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read cap_perfmon cap_bpf cap_checkpoint_restore
    CleanResult: success
    CollectMode: inactive
    ConditionResult: 'yes'
    ConditionTimestamp: Sat 2025-10-25 10:32:53 UTC
    ConditionTimestampMonotonic: '7092357804'
    ConfigurationDirectoryMode: '0755'
    Conflicts: shutdown.target rke2-agent.service
    ControlGroup: /system.slice/rke2-server.service
    ControlGroupId: '5969'
    ControlPID: '0'
    CoredumpFilter: '0x33'
    CoredumpReceive: 'no'
    DefaultDependencies: 'yes'
    DefaultMemoryLow: '0'
    DefaultMemoryMin: '0'
    DefaultStartupMemoryLow: '0'
    Delegate: 'yes'
    DelegateControllers: cpu cpuset io memory pids
    Description: Rancher Kubernetes Engine v2 (server)
    DevicePolicy: auto
    Documentation: https://github.com/rancher/rke2#readme
    DynamicUser: 'no'
    EffectiveCPUs: 0-3
    EffectiveMemoryNodes: '0'
    EnvironmentFiles: /usr/local/lib/systemd/system/rke2-server.env (ignore_errors=yes)
    ExecMainCode: '0'
    ExecMainExitTimestampMonotonic: '0'
    ExecMainPID: '9853'
    ExecMainStartTimestamp: Sat 2025-10-25 10:32:53 UTC
    ExecMainStartTimestampMonotonic: '7092425848'
    ExecMainStatus: '0'
    ExecStart: '{ path=/usr/local/bin/rke2 ; argv[]=/usr/local/bin/rke2 server ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'
    ExecStartEx: '{ path=/usr/local/bin/rke2 ; argv[]=/usr/local/bin/rke2 server ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'
    ExecStartPre: '{ path=/sbin/modprobe ; argv[]=/sbin/modprobe overlay ; ignore_errors=yes ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'
    ExecStartPreEx: '{ path=/sbin/modprobe ; argv[]=/sbin/modprobe overlay ; flags=ignore-failure ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'
    ExecStopPost: '{ path=/bin/sh ; argv[]=/bin/sh -c systemd-cgls /system.slice/rke2-server.service | grep -Eo ''[0-9]+ (containerd|kubelet)'' | awk ''{print $1}'' | xargs -r kill ; ignore_errors=yes ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'
    ExecStopPostEx: '{ path=/bin/sh ; argv[]=/bin/sh -c systemd-cgls /system.slice/rke2-server.service | grep -Eo ''[0-9]+ (containerd|kubelet)'' | awk ''{print $1}'' | xargs -r kill ; flags=ignore-failure ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'
    ExitType: main
    ExtensionImagePolicy: root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent
    FailureAction: none
    FileDescriptorStoreMax: '0'
    FileDescriptorStorePreserve: restart
    FinalKillSignal: '9'
    FragmentPath: /usr/local/lib/systemd/system/rke2-server.service
    FreezerState: running
    GID: '[not set]'
    GuessMainPID: 'yes'
    IOAccounting: 'no'
    IOReadBytes: '[not set]'
    IOReadOperations: '[not set]'
    IOSchedulingClass: '2'
    IOSchedulingPriority: '4'
    IOWeight: '[not set]'
    IOWriteBytes: '[not set]'
    IOWriteOperations: '[not set]'
    IPAccounting: 'no'
    IPEgressBytes: '[no data]'
    IPEgressPackets: '[no data]'
    IPIngressBytes: '[no data]'
    IPIngressPackets: '[no data]'
    Id: rke2-server.service
    IgnoreOnIsolate: 'no'
    IgnoreSIGPIPE: 'yes'
    InactiveEnterTimestampMonotonic: '0'
    InactiveExitTimestamp: Sat 2025-10-25 10:32:53 UTC
    InactiveExitTimestampMonotonic: '7092379564'
    InvocationID: b0d2a2fbe21c4c8f9d7bffc99a9906ef
    JobRunningTimeoutUSec: infinity
    JobTimeoutAction: none
    JobTimeoutUSec: infinity
    KeyringMode: private
    KillMode: process
    KillSignal: '15'
    LimitAS: infinity
    LimitASSoft: infinity
    LimitCORE: infinity
    LimitCORESoft: infinity
    LimitCPU: infinity
    LimitCPUSoft: infinity
    LimitDATA: infinity
    LimitDATASoft: infinity
    LimitFSIZE: infinity
    LimitFSIZESoft: infinity
    LimitLOCKS: infinity
    LimitLOCKSSoft: infinity
    LimitMEMLOCK: '8388608'
    LimitMEMLOCKSoft: '8388608'
    LimitMSGQUEUE: '819200'
    LimitMSGQUEUESoft: '819200'
    LimitNICE: '0'
    LimitNICESoft: '0'
    LimitNOFILE: '1048576'
    LimitNOFILESoft: '1048576'
    LimitNPROC: infinity
    LimitNPROCSoft: infinity
    LimitRSS: infinity
    LimitRSSSoft: infinity
    LimitRTPRIO: '0'
    LimitRTPRIOSoft: '0'
    LimitRTTIME: infinity
    LimitRTTIMESoft: infinity
    LimitSIGPENDING: '15158'
    LimitSIGPENDINGSoft: '15158'
    LimitSTACK: infinity
    LimitSTACKSoft: '8388608'
    LoadState: loaded
    LockPersonality: 'no'
    LogLevelMax: '-1'
    LogRateLimitBurst: '0'
    LogRateLimitIntervalUSec: '0'
    LogsDirectoryMode: '0755'
    MainPID: '9853'
    ManagedOOMMemoryPressure: auto
    ManagedOOMMemoryPressureLimit: '0'
    ManagedOOMPreference: none
    ManagedOOMSwap: auto
    MemoryAccounting: 'yes'
    MemoryAvailable: '2728685568'
    MemoryCurrent: '960663552'
    MemoryDenyWriteExecute: 'no'
    MemoryHigh: infinity
    MemoryKSM: 'no'
    MemoryLimit: infinity
    MemoryLow: '0'
    MemoryMax: infinity
    MemoryMin: '0'
    MemoryPeak: '3275689984'
    MemoryPressureThresholdUSec: 200ms
    MemoryPressureWatch: auto
    MemorySwapCurrent: '192512'
    MemorySwapMax: infinity
    MemorySwapPeak: '192512'
    MemoryZSwapCurrent: '0'
    MemoryZSwapMax: infinity
    MountAPIVFS: 'no'
    MountImagePolicy: root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent
    NFileDescriptorStore: '0'
    NRestarts: '0'
    NUMAPolicy: n/a
    Names: rke2-server.service
    NeedDaemonReload: 'no'
    Nice: '0'
    NoNewPrivileges: 'no'
    NonBlocking: 'no'
    NotifyAccess: main
    OOMPolicy: continue
    OOMScoreAdjust: '0'
    OnFailureJobMode: replace
    OnSuccessJobMode: fail
    Perpetual: 'no'
    PrivateDevices: 'no'
    PrivateIPC: 'no'
    PrivateMounts: 'no'
    PrivateNetwork: 'no'
    PrivateTmp: 'no'
    PrivateUsers: 'no'
    ProcSubset: all
    ProtectClock: 'no'
    ProtectControlGroups: 'no'
    ProtectHome: 'no'
    ProtectHostname: 'no'
    ProtectKernelLogs: 'no'
    ProtectKernelModules: 'no'
    ProtectKernelTunables: 'no'
    ProtectProc: default
    ProtectSystem: 'no'
    RefuseManualStart: 'no'
    RefuseManualStop: 'no'
    ReloadResult: success
    ReloadSignal: '1'
    RemainAfterExit: 'no'
    RemoveIPC: 'no'
    Requires: system.slice sysinit.target
    Restart: always
    RestartKillSignal: '15'
    RestartMaxDelayUSec: infinity
    RestartMode: normal
    RestartSteps: '0'
    RestartUSec: 5s
    RestartUSecNext: 5s
    RestrictNamespaces: 'no'
    RestrictRealtime: 'no'
    RestrictSUIDSGID: 'no'
    Result: success
    RootDirectoryStartOnly: 'no'
    RootEphemeral: 'no'
    RootImagePolicy: root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent
    RuntimeDirectoryMode: '0755'
    RuntimeDirectoryPreserve: 'no'
    RuntimeMaxUSec: infinity
    RuntimeRandomizedExtraUSec: '0'
    SameProcessGroup: 'no'
    SecureBits: '0'
    SendSIGHUP: 'no'
    SendSIGKILL: 'yes'
    SetLoginEnvironment: 'no'
    Slice: system.slice
    StandardError: inherit
    StandardInput: 'null'
    StandardOutput: journal
    StartLimitAction: none
    StartLimitBurst: '5'
    StartLimitIntervalUSec: 10s
    StartupBlockIOWeight: '[not set]'
    StartupCPUShares: '[not set]'
    StartupCPUWeight: '[not set]'
    StartupIOWeight: '[not set]'
    StartupMemoryHigh: infinity
    StartupMemoryLow: '0'
    StartupMemoryMax: infinity
    StartupMemorySwapMax: infinity
    StartupMemoryZSwapMax: infinity
    StateChangeTimestamp: Sat 2025-10-25 10:35:37 UTC
    StateChangeTimestampMonotonic: '7257060396'
    StateDirectoryMode: '0755'
    StatusErrno: '0'
    StopWhenUnneeded: 'no'
    SubState: running
    SuccessAction: none
    SurviveFinalKillSignal: 'no'
    SyslogFacility: '3'
    SyslogLevel: '6'
    SyslogLevelPrefix: 'yes'
    SyslogPriority: '30'
    SystemCallErrorNumber: '2147483646'
    TTYReset: 'no'
    TTYVHangup: 'no'
    TTYVTDisallocate: 'no'
    TasksAccounting: 'yes'
    TasksCurrent: '134'
    TasksMax: infinity
    TimeoutAbortUSec: 1min 30s
    TimeoutCleanUSec: infinity
    TimeoutStartFailureMode: terminate
    TimeoutStartUSec: infinity
    TimeoutStopFailureMode: terminate
    TimeoutStopUSec: 1min 30s
    TimerSlackNSec: '50000'
    Transient: 'no'
    Type: notify
    UID: '[not set]'
    UMask: '0022'
    UnitFilePreset: enabled
    UnitFileState: enabled
    UtmpMode: init
    WantedBy: multi-user.target
    Wants: network-online.target
    WatchdogSignal: '6'
    WatchdogTimestampMonotonic: '0'
    WatchdogUSec: '0'
2025-10-25 17:45:17,198 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/systemd.py
2025-10-25 17:45:17,199 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:45:17,199 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:17,199 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=ffykdwlzhengpxithpblolbsghauiccb] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-ffykdwlzhengpxithpblolbsghauiccb ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:45:17,242 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:17,779 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"name": "rke2-agent.service", "changed": false, "status": {"ExitType": "main", "Restart": "no", "RestartMode": "normal", "NotifyAccess": "none", "RestartUSec": "100ms", "RestartSteps": "0", "RestartMaxDelayUSec": "infinity", "RestartUSecNext": "100ms", "TimeoutStartUSec": "1min 30s", "TimeoutStopUSec": "1min 30s", "TimeoutAbortUSec": "1min 30s", "TimeoutStartFailureMode": "terminate", "TimeoutStopFailureMode": "terminate", "RuntimeMaxUSec": "infinity", "RuntimeRandomizedExtraUSec": "0", "WatchdogUSec": "infinity", "WatchdogTimestampMonotonic": "0", "RootDirectoryStartOnly": "no", "RemainAfterExit": "no", "GuessMainPID": "yes", "MainPID": "0", "ControlPID": "0", "FileDescriptorStoreMax": "0", "NFileDescriptorStore": "0", "FileDescriptorStorePreserve": "restart", "StatusErrno": "0", "Result": "success", "ReloadResult": "success", "CleanResult": "success", "UID": "[not set]", "GID": "[not set]", "NRestarts": "0", "ReloadSignal": "1", "ExecMainStartTimestampMonotonic": "0", "ExecMainExitTimestampMonotonic": "0", "ExecMainPID": "0", "ExecMainCode": "0", "ExecMainStatus": "0", "ControlGroupId": "0", "MemoryCurrent": "[not set]", "MemoryPeak": "[not set]", "MemorySwapCurrent": "[not set]", "MemorySwapPeak": "[not set]", "MemoryZSwapCurrent": "[not set]", "MemoryAvailable": "[not set]", "CPUUsageNSec": "[not set]", "TasksCurrent": "[not set]", "IPIngressBytes": "[no data]", "IPIngressPackets": "[no data]", "IPEgressBytes": "[no data]", "IPEgressPackets": "[no data]", "IOReadBytes": "[not set]", "IOReadOperations": "[not set]", "IOWriteBytes": "[not set]", "IOWriteOperations": "[not set]", "Delegate": "no", "CPUAccounting": "yes", "CPUWeight": "[not set]", "StartupCPUWeight": "[not set]", "CPUShares": "[not set]", "StartupCPUShares": "[not set]", "CPUQuotaPerSecUSec": "infinity", "CPUQuotaPeriodUSec": "infinity", "IOAccounting": "no", "IOWeight": "[not set]", "StartupIOWeight": "[not set]", "BlockIOAccounting": "no", "BlockIOWeight": "[not set]", "StartupBlockIOWeight": "[not set]", "MemoryAccounting": "yes", "DefaultMemoryLow": "0", "DefaultStartupMemoryLow": "0", "DefaultMemoryMin": "0", "MemoryMin": "0", "MemoryLow": "0", "StartupMemoryLow": "0", "MemoryHigh": "infinity", "StartupMemoryHigh": "infinity", "MemoryMax": "infinity", "StartupMemoryMax": "infinity", "MemorySwapMax": "infinity", "StartupMemorySwapMax": "infinity", "MemoryZSwapMax": "infinity", "StartupMemoryZSwapMax": "infinity", "MemoryLimit": "infinity", "DevicePolicy": "auto", "TasksAccounting": "yes", "TasksMax": "4547", "IPAccounting": "no", "ManagedOOMSwap": "auto", "ManagedOOMMemoryPressure": "auto", "ManagedOOMMemoryPressureLimit": "0", "ManagedOOMPreference": "none", "MemoryPressureWatch": "auto", "MemoryPressureThresholdUSec": "200ms", "CoredumpReceive": "no", "UMask": "0022", "LimitCPU": "infinity", "LimitCPUSoft": "infinity", "LimitFSIZE": "infinity", "LimitFSIZESoft": "infinity", "LimitDATA": "infinity", "LimitDATASoft": "infinity", "LimitSTACK": "infinity", "LimitSTACKSoft": "8388608", "LimitCORE": "infinity", "LimitCORESoft": "0", "LimitRSS": "infinity", "LimitRSSSoft": "infinity", "LimitNOFILE": "1048576", "LimitNOFILESoft": "1048576", "LimitAS": "infinity", "LimitASSoft": "infinity", "LimitNPROC": "15158", "LimitNPROCSoft": "15158", "LimitMEMLOCK": "506986496", "LimitMEMLOCKSoft": "506986496", "LimitLOCKS": "infinity", "LimitLOCKSSoft": "infinity", "LimitSIGPENDING": "15158", "LimitSIGPENDINGSoft": "15158", "LimitMSGQUEUE": "819200", "LimitMSGQUEUESoft": "819200", "LimitNICE": "0", "LimitNICESoft": "0", "LimitRTPRIO": "0", "LimitRTPRIOSoft": "0", "LimitRTTIME": "infinity", "LimitRTTIMESoft": "infinity", "RootEphemeral": "no", "OOMScoreAdjust": "0", "CoredumpFilter": "0x33", "Nice": "0", "IOSchedulingClass": "2", "IOSchedulingPriority": "4", "CPUSchedulingPolicy": "0", "CPUSchedulingPriority": "0", "CPUAffinityFromNUMA": "no", "NUMAPolicy": "n/a", "TimerSlackNSec": "50000", "CPUSchedulingResetOnFork": "no", "NonBlocking": "no", "StandardInput": "null", "StandardOutput": "inherit", "StandardError": "inherit", "TTYReset": "no", "TTYVHangup": "no", "TTYVTDisallocate": "no", "SyslogPriority": "30", "SyslogLevelPrefix": "yes", "SyslogLevel": "6", "SyslogFacility": "3", "LogLevelMax": "-1", "LogRateLimitIntervalUSec": "0", "LogRateLimitBurst": "0", "SecureBits": "0", "CapabilityBoundingSet": "cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read cap_perfmon cap_bpf cap_checkpoint_restore", "DynamicUser": "no", "SetLoginEnvironment": "no", "RemoveIPC": "no", "PrivateTmp": "no", "PrivateDevices": "no", "ProtectClock": "no", "ProtectKernelTunables": "no", "ProtectKernelModules": "no", "ProtectKernelLogs": "no", "ProtectControlGroups": "no", "PrivateNetwork": "no", "PrivateUsers": "no", "PrivateMounts": "no", "PrivateIPC": "no", "ProtectHome": "no", "ProtectSystem": "no", "SameProcessGroup": "no", "UtmpMode": "init", "IgnoreSIGPIPE": "yes", "NoNewPrivileges": "no", "SystemCallErrorNumber": "2147483646", "LockPersonality": "no", "RuntimeDirectoryPreserve": "no", "RuntimeDirectoryMode": "0755", "StateDirectoryMode": "0755", "CacheDirectoryMode": "0755", "LogsDirectoryMode": "0755", "ConfigurationDirectoryMode": "0755", "TimeoutCleanUSec": "infinity", "MemoryDenyWriteExecute": "no", "RestrictRealtime": "no", "RestrictSUIDSGID": "no", "RestrictNamespaces": "no", "MountAPIVFS": "no", "KeyringMode": "private", "ProtectProc": "default", "ProcSubset": "all", "ProtectHostname": "no", "MemoryKSM": "no", "RootImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "MountImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "ExtensionImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "KillMode": "control-group", "KillSignal": "15", "RestartKillSignal": "15", "FinalKillSignal": "9", "SendSIGKILL": "yes", "SendSIGHUP": "no", "WatchdogSignal": "6", "Id": "rke2-agent.service", "Names": "rke2-agent.service", "ConflictedBy": "rke2-server.service", "Description": "rke2-agent.service", "LoadState": "masked", "ActiveState": "inactive", "FreezerState": "running", "SubState": "dead", "FragmentPath": "/etc/systemd/system/rke2-agent.service", "UnitFileState": "masked", "UnitFilePreset": "enabled", "StateChangeTimestamp": "Sat 2025-10-25 10:35:39 UTC", "StateChangeTimestampMonotonic": "7258194303", "InactiveExitTimestampMonotonic": "0", "ActiveEnterTimestampMonotonic": "0", "ActiveExitTimestampMonotonic": "0", "InactiveEnterTimestampMonotonic": "0", "CanStart": "no", "CanStop": "yes", "CanReload": "no", "CanIsolate": "no", "CanFreeze": "yes", "StopWhenUnneeded": "no", "RefuseManualStart": "no", "RefuseManualStop": "no", "AllowIsolate": "no", "DefaultDependencies": "yes", "SurviveFinalKillSignal": "no", "OnSuccessJobMode": "fail", "OnFailureJobMode": "replace", "IgnoreOnIsolate": "no", "NeedDaemonReload": "no", "JobTimeoutUSec": "infinity", "JobRunningTimeoutUSec": "infinity", "JobTimeoutAction": "none", "ConditionResult": "no", "AssertResult": "no", "ConditionTimestampMonotonic": "0", "AssertTimestampMonotonic": "0", "LoadError": "org.freedesktop.systemd1.UnitMasked \\"Unit rke2-agent.service is masked.\\"", "Transient": "no", "Perpetual": "no", "StartLimitIntervalUSec": "10s", "StartLimitBurst": "5", "StartLimitAction": "none", "FailureAction": "none", "SuccessAction": "none", "CollectMode": "inactive"}, "enabled": false, "invocation": {"module_args": {"name": "rke2-agent.service", "enabled": false, "masked": true, "daemon_reload": false, "daemon_reexec": false, "scope": "system", "no_block": false, "state": null, "force": null}}}\n', b'')
2025-10-25 17:45:17,786 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Mask other RKE2 service on the rest of the nodes] ****************
2025-10-25 17:45:17,791 p=55263 u=root n=ansible | ok: [k8s-m1] => (item=agent) => changed=false 
  ansible_loop_var: item
  enabled: false
  invocation:
    module_args:
      daemon_reexec: false
      daemon_reload: false
      enabled: false
      force: null
      masked: true
      name: rke2-agent.service
      no_block: false
      scope: system
      state: null
  item: agent
  name: rke2-agent.service
  status:
    ActiveEnterTimestampMonotonic: '0'
    ActiveExitTimestampMonotonic: '0'
    ActiveState: inactive
    AllowIsolate: 'no'
    AssertResult: 'no'
    AssertTimestampMonotonic: '0'
    BlockIOAccounting: 'no'
    BlockIOWeight: '[not set]'
    CPUAccounting: 'yes'
    CPUAffinityFromNUMA: 'no'
    CPUQuotaPerSecUSec: infinity
    CPUQuotaPeriodUSec: infinity
    CPUSchedulingPolicy: '0'
    CPUSchedulingPriority: '0'
    CPUSchedulingResetOnFork: 'no'
    CPUShares: '[not set]'
    CPUUsageNSec: '[not set]'
    CPUWeight: '[not set]'
    CacheDirectoryMode: '0755'
    CanFreeze: 'yes'
    CanIsolate: 'no'
    CanReload: 'no'
    CanStart: 'no'
    CanStop: 'yes'
    CapabilityBoundingSet: cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read cap_perfmon cap_bpf cap_checkpoint_restore
    CleanResult: success
    CollectMode: inactive
    ConditionResult: 'no'
    ConditionTimestampMonotonic: '0'
    ConfigurationDirectoryMode: '0755'
    ConflictedBy: rke2-server.service
    ControlGroupId: '0'
    ControlPID: '0'
    CoredumpFilter: '0x33'
    CoredumpReceive: 'no'
    DefaultDependencies: 'yes'
    DefaultMemoryLow: '0'
    DefaultMemoryMin: '0'
    DefaultStartupMemoryLow: '0'
    Delegate: 'no'
    Description: rke2-agent.service
    DevicePolicy: auto
    DynamicUser: 'no'
    ExecMainCode: '0'
    ExecMainExitTimestampMonotonic: '0'
    ExecMainPID: '0'
    ExecMainStartTimestampMonotonic: '0'
    ExecMainStatus: '0'
    ExitType: main
    ExtensionImagePolicy: root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent
    FailureAction: none
    FileDescriptorStoreMax: '0'
    FileDescriptorStorePreserve: restart
    FinalKillSignal: '9'
    FragmentPath: /etc/systemd/system/rke2-agent.service
    FreezerState: running
    GID: '[not set]'
    GuessMainPID: 'yes'
    IOAccounting: 'no'
    IOReadBytes: '[not set]'
    IOReadOperations: '[not set]'
    IOSchedulingClass: '2'
    IOSchedulingPriority: '4'
    IOWeight: '[not set]'
    IOWriteBytes: '[not set]'
    IOWriteOperations: '[not set]'
    IPAccounting: 'no'
    IPEgressBytes: '[no data]'
    IPEgressPackets: '[no data]'
    IPIngressBytes: '[no data]'
    IPIngressPackets: '[no data]'
    Id: rke2-agent.service
    IgnoreOnIsolate: 'no'
    IgnoreSIGPIPE: 'yes'
    InactiveEnterTimestampMonotonic: '0'
    InactiveExitTimestampMonotonic: '0'
    JobRunningTimeoutUSec: infinity
    JobTimeoutAction: none
    JobTimeoutUSec: infinity
    KeyringMode: private
    KillMode: control-group
    KillSignal: '15'
    LimitAS: infinity
    LimitASSoft: infinity
    LimitCORE: infinity
    LimitCORESoft: '0'
    LimitCPU: infinity
    LimitCPUSoft: infinity
    LimitDATA: infinity
    LimitDATASoft: infinity
    LimitFSIZE: infinity
    LimitFSIZESoft: infinity
    LimitLOCKS: infinity
    LimitLOCKSSoft: infinity
    LimitMEMLOCK: '506986496'
    LimitMEMLOCKSoft: '506986496'
    LimitMSGQUEUE: '819200'
    LimitMSGQUEUESoft: '819200'
    LimitNICE: '0'
    LimitNICESoft: '0'
    LimitNOFILE: '1048576'
    LimitNOFILESoft: '1048576'
    LimitNPROC: '15158'
    LimitNPROCSoft: '15158'
    LimitRSS: infinity
    LimitRSSSoft: infinity
    LimitRTPRIO: '0'
    LimitRTPRIOSoft: '0'
    LimitRTTIME: infinity
    LimitRTTIMESoft: infinity
    LimitSIGPENDING: '15158'
    LimitSIGPENDINGSoft: '15158'
    LimitSTACK: infinity
    LimitSTACKSoft: '8388608'
    LoadError: org.freedesktop.systemd1.UnitMasked "Unit rke2-agent.service is masked."
    LoadState: masked
    LockPersonality: 'no'
    LogLevelMax: '-1'
    LogRateLimitBurst: '0'
    LogRateLimitIntervalUSec: '0'
    LogsDirectoryMode: '0755'
    MainPID: '0'
    ManagedOOMMemoryPressure: auto
    ManagedOOMMemoryPressureLimit: '0'
    ManagedOOMPreference: none
    ManagedOOMSwap: auto
    MemoryAccounting: 'yes'
    MemoryAvailable: '[not set]'
    MemoryCurrent: '[not set]'
    MemoryDenyWriteExecute: 'no'
    MemoryHigh: infinity
    MemoryKSM: 'no'
    MemoryLimit: infinity
    MemoryLow: '0'
    MemoryMax: infinity
    MemoryMin: '0'
    MemoryPeak: '[not set]'
    MemoryPressureThresholdUSec: 200ms
    MemoryPressureWatch: auto
    MemorySwapCurrent: '[not set]'
    MemorySwapMax: infinity
    MemorySwapPeak: '[not set]'
    MemoryZSwapCurrent: '[not set]'
    MemoryZSwapMax: infinity
    MountAPIVFS: 'no'
    MountImagePolicy: root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent
    NFileDescriptorStore: '0'
    NRestarts: '0'
    NUMAPolicy: n/a
    Names: rke2-agent.service
    NeedDaemonReload: 'no'
    Nice: '0'
    NoNewPrivileges: 'no'
    NonBlocking: 'no'
    NotifyAccess: none
    OOMScoreAdjust: '0'
    OnFailureJobMode: replace
    OnSuccessJobMode: fail
    Perpetual: 'no'
    PrivateDevices: 'no'
    PrivateIPC: 'no'
    PrivateMounts: 'no'
    PrivateNetwork: 'no'
    PrivateTmp: 'no'
    PrivateUsers: 'no'
    ProcSubset: all
    ProtectClock: 'no'
    ProtectControlGroups: 'no'
    ProtectHome: 'no'
    ProtectHostname: 'no'
    ProtectKernelLogs: 'no'
    ProtectKernelModules: 'no'
    ProtectKernelTunables: 'no'
    ProtectProc: default
    ProtectSystem: 'no'
    RefuseManualStart: 'no'
    RefuseManualStop: 'no'
    ReloadResult: success
    ReloadSignal: '1'
    RemainAfterExit: 'no'
    RemoveIPC: 'no'
    Restart: 'no'
    RestartKillSignal: '15'
    RestartMaxDelayUSec: infinity
    RestartMode: normal
    RestartSteps: '0'
    RestartUSec: 100ms
    RestartUSecNext: 100ms
    RestrictNamespaces: 'no'
    RestrictRealtime: 'no'
    RestrictSUIDSGID: 'no'
    Result: success
    RootDirectoryStartOnly: 'no'
    RootEphemeral: 'no'
    RootImagePolicy: root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent
    RuntimeDirectoryMode: '0755'
    RuntimeDirectoryPreserve: 'no'
    RuntimeMaxUSec: infinity
    RuntimeRandomizedExtraUSec: '0'
    SameProcessGroup: 'no'
    SecureBits: '0'
    SendSIGHUP: 'no'
    SendSIGKILL: 'yes'
    SetLoginEnvironment: 'no'
    StandardError: inherit
    StandardInput: 'null'
    StandardOutput: inherit
    StartLimitAction: none
    StartLimitBurst: '5'
    StartLimitIntervalUSec: 10s
    StartupBlockIOWeight: '[not set]'
    StartupCPUShares: '[not set]'
    StartupCPUWeight: '[not set]'
    StartupIOWeight: '[not set]'
    StartupMemoryHigh: infinity
    StartupMemoryLow: '0'
    StartupMemoryMax: infinity
    StartupMemorySwapMax: infinity
    StartupMemoryZSwapMax: infinity
    StateChangeTimestamp: Sat 2025-10-25 10:35:39 UTC
    StateChangeTimestampMonotonic: '7258194303'
    StateDirectoryMode: '0755'
    StatusErrno: '0'
    StopWhenUnneeded: 'no'
    SubState: dead
    SuccessAction: none
    SurviveFinalKillSignal: 'no'
    SyslogFacility: '3'
    SyslogLevel: '6'
    SyslogLevelPrefix: 'yes'
    SyslogPriority: '30'
    SystemCallErrorNumber: '2147483646'
    TTYReset: 'no'
    TTYVHangup: 'no'
    TTYVTDisallocate: 'no'
    TasksAccounting: 'yes'
    TasksCurrent: '[not set]'
    TasksMax: '4547'
    TimeoutAbortUSec: 1min 30s
    TimeoutCleanUSec: infinity
    TimeoutStartFailureMode: terminate
    TimeoutStartUSec: 1min 30s
    TimeoutStopFailureMode: terminate
    TimeoutStopUSec: 1min 30s
    TimerSlackNSec: '50000'
    Transient: 'no'
    UID: '[not set]'
    UMask: '0022'
    UnitFilePreset: enabled
    UnitFileState: masked
    UtmpMode: init
    WatchdogSignal: '6'
    WatchdogTimestampMonotonic: '0'
    WatchdogUSec: infinity
2025-10-25 17:45:17,912 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:45:17,912 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:45:17,912 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:17,913 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=gfoecbojlxavagovweaquqmanolyldzw] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-gfoecbojlxavagovweaquqmanolyldzw ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:45:17,971 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:19,616 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:45:18.476671", "end": "2025-10-25 10:45:19.844619", "delta": "0:00:01.367948", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:45:34,640 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:45:34,640 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:45:34,640 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:34,641 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=rrhnfbrtatbmoapxekqtnvuirkuynrsu] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-rrhnfbrtatbmoapxekqtnvuirkuynrsu ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:45:34,683 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:35,064 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:45:35.185506", "end": "2025-10-25 10:45:35.309365", "delta": "0:00:00.123859", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:45:50,069 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:45:50,070 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:45:50,070 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:45:50,070 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=mfaedxwydijohbvyhutbdjcdgeejrrij] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-mfaedxwydijohbvyhutbdjcdgeejrrij ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:45:50,119 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:45:50,538 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:45:50.648686", "end": "2025-10-25 10:45:50.779264", "delta": "0:00:00.130578", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:46:05,544 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:46:05,544 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:46:05,545 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:46:05,545 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=qgzpjmttfwtbyswzyfnikhtrkzhkkbmg] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-qgzpjmttfwtbyswzyfnikhtrkzhkkbmg ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:46:05,593 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:46:06,026 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:46:06.129699", "end": "2025-10-25 10:46:06.269807", "delta": "0:00:00.140108", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:46:21,034 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:46:21,034 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:46:21,034 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:46:21,034 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=lrseebtajtngisclhvngdhspppzzyttc] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-lrseebtajtngisclhvngdhspppzzyttc ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:46:21,082 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:46:21,498 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:46:21.602222", "end": "2025-10-25 10:46:21.733051", "delta": "0:00:00.130829", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:46:36,506 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:46:36,506 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:46:36,507 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:46:36,507 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=hmnawhuwpufxznyyrhdhbbrvpbbhxwum] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-hmnawhuwpufxznyyrhdhbbrvpbbhxwum ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:46:36,572 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:46:37,014 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:46:37.108453", "end": "2025-10-25 10:46:37.255552", "delta": "0:00:00.147099", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:46:52,022 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:46:52,022 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:46:52,022 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:46:52,023 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=hknuylpzdhzandzmwgatedeadvpbmway] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-hknuylpzdhzandzmwgatedeadvpbmway ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:46:52,072 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:46:52,527 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:46:52.635133", "end": "2025-10-25 10:46:52.771683", "delta": "0:00:00.136550", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:47:07,534 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:47:07,535 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:47:07,535 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:47:07,535 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=jjziucvlkmbuvkdepyeutgmudbefccde] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-jjziucvlkmbuvkdepyeutgmudbefccde ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:47:07,610 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:47:08,028 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:47:08.149879", "end": "2025-10-25 10:47:08.270153", "delta": "0:00:00.120274", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:47:25,578 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:47:25,581 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:47:25,582 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:47:25,582 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=jgsupisqvoicdjillxydybypvlflinza] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-jgsupisqvoicdjillxydybypvlflinza ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:47:25,670 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:47:26,108 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:47:26.216552", "end": "2025-10-25 10:47:26.349626", "delta": "0:00:00.133074", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:47:41,117 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:47:41,118 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:47:41,118 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:47:41,118 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=caowrsimdnmlwrfrfkupdlyycfebyfst] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-caowrsimdnmlwrfrfkupdlyycfebyfst ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:47:41,171 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:47:41,631 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:47:41.720811", "end": "2025-10-25 10:47:41.861284", "delta": "0:00:00.140473", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:47:56,638 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:47:56,639 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:47:56,639 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:47:56,639 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=sasocisyectveeakayxkotervwhcvjug] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-sasocisyectveeakayxkotervwhcvjug ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:47:56,778 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:47:57,220 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:47:57.328699", "end": "2025-10-25 10:47:57.456055", "delta": "0:00:00.127356", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:48:12,226 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:48:12,227 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:48:12,227 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:48:12,227 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=xfibrqivmosrjbddqmgveoqphhcguygc] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-xfibrqivmosrjbddqmgveoqphhcguygc ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:48:12,288 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:48:12,729 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:48:12.833873", "end": "2025-10-25 10:48:12.968535", "delta": "0:00:00.134662", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:48:27,737 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:48:27,737 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:48:27,738 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:48:27,738 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=jgdqpcgkqqexxfpqajdcauesxonefqjh] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-jgdqpcgkqqexxfpqajdcauesxonefqjh ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:48:27,797 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:48:28,235 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:48:28.327940", "end": "2025-10-25 10:48:28.479138", "delta": "0:00:00.151198", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:48:43,240 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:48:43,240 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:48:43,240 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:48:43,241 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=aestpockqtmwajwfnjiqzrcbdjojmlhw] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-aestpockqtmwajwfnjiqzrcbdjojmlhw ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:48:43,285 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:48:43,664 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:48:43.792138", "end": "2025-10-25 10:48:43.910954", "delta": "0:00:00.118816", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:48:58,670 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:48:58,670 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:48:58,670 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:48:58,670 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=bioakemcuabogokvoknvuboadvnkcgml] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-bioakemcuabogokvoknvuboadvnkcgml ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:48:58,717 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:48:59,148 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:48:59.251227", "end": "2025-10-25 10:48:59.391465", "delta": "0:00:00.140238", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:49:14,155 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:49:14,156 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:49:14,157 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:49:14,157 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=zxzrqwixhfpcwditdacnrqqdjwtzhwiz] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-zxzrqwixhfpcwditdacnrqqdjwtzhwiz ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:49:14,227 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:49:14,675 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:49:14.771861", "end": "2025-10-25 10:49:14.915242", "delta": "0:00:00.143381", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:49:29,681 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:49:29,682 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:49:29,682 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:49:29,682 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=vuxtuxgftklikicxnvhtmuducjgtznkn] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-vuxtuxgftklikicxnvhtmuducjgtznkn ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:49:29,724 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:49:30,158 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:49:30.274387", "end": "2025-10-25 10:49:30.404905", "delta": "0:00:00.130518", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:49:37,283 p=66015 u=root n=ansible | PLAY [Bootstrap RedHat 8 nodes for RKE2 Airgap environment and Deploy RKE2] *************************
2025-10-25 17:49:39,099 p=66015 u=root n=ansible | TASK [Gathering Facts] ******************************************************************************
2025-10-25 17:49:39,099 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:49:39,764 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:49:39,784 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:49:39,822 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:49:40,082 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Validating arguments against arg spec 'main' - This is the main entrypoint for the lablabs.rke2 role.] ***
2025-10-25 17:49:40,082 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:49:40,112 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:49:40,124 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:49:40,171 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:49:41,266 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Set hostname based on inventory name] *******************************************
2025-10-25 17:49:41,266 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:49:41,270 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:49:41,294 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:49:41,467 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:49:41,657 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Download and install RKE2 v1.34.1+rke2r1] ***************************************
2025-10-25 17:49:41,849 p=66015 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/rke2.yml for k8s-m1, k8s-w01, k8s-w02, k8s-w03
2025-10-25 17:49:42,958 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Configure kernel modules] *******************************************************
2025-10-25 17:49:42,959 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:49:42,969 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:49:42,996 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:49:43,018 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:49:43,356 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Detect NetworkManager] **********************************************************
2025-10-25 17:49:43,356 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:49:43,370 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:49:43,393 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:49:43,420 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:49:44,389 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Configure sysctl parameters] ****************************************************
2025-10-25 17:49:44,390 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:49:44,405 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:49:44,428 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:49:44,446 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:49:44,981 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Apply system configurations] ****************************************************
2025-10-25 17:49:44,981 p=66015 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:49:44,997 p=66015 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:49:45,001 p=66015 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:49:45,032 p=66015 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:49:45,169 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:49:45,169 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:49:45,170 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:49:45,171 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=lnyjgkhyjnuvquxqxrmysitdsfavolqu] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-lnyjgkhyjnuvquxqxrmysitdsfavolqu ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:49:45,219 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:49:45,679 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:49:45.725042", "end": "2025-10-25 10:49:45.867375", "delta": "0:00:00.142333", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:49:47,663 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Ensure UFW is installed] ********************************************************
2025-10-25 17:49:47,664 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:49:47,665 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:49:47,688 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:49:47,716 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:49:48,972 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Enable UFW] *********************************************************************
2025-10-25 17:49:48,973 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:49:48,985 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:49:49,007 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:49:49,018 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:49:51,642 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Set default incoming policy to deny] ********************************************
2025-10-25 17:49:51,642 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:49:51,811 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:49:51,911 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:49:52,234 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:49:55,027 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Set default outgoing policy to allow] *******************************************
2025-10-25 17:49:55,028 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:49:55,300 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:49:55,305 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:49:55,600 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:49:56,523 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Allow required ports] ***********************************************************
2025-10-25 17:49:56,523 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=22/tcp)
2025-10-25 17:49:56,545 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=22/tcp)
2025-10-25 17:49:56,602 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=22/tcp)
2025-10-25 17:49:56,603 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=22/tcp)
2025-10-25 17:49:57,385 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=179/tcp)
2025-10-25 17:49:57,429 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=179/tcp)
2025-10-25 17:49:57,448 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=179/tcp)
2025-10-25 17:49:57,456 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=179/tcp)
2025-10-25 17:49:58,205 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=9345/tcp)
2025-10-25 17:49:58,275 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=9345/tcp)
2025-10-25 17:49:58,281 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=9345/tcp)
2025-10-25 17:49:58,310 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=9345/tcp)
2025-10-25 17:49:59,045 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=6443/tcp)
2025-10-25 17:49:59,089 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=6443/tcp)
2025-10-25 17:49:59,106 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=6443/tcp)
2025-10-25 17:49:59,175 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=6443/tcp)
2025-10-25 17:49:59,849 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=10250/tcp)
2025-10-25 17:49:59,899 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=10250/tcp)
2025-10-25 17:49:59,930 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=10250/tcp)
2025-10-25 17:50:00,028 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=10250/tcp)
2025-10-25 17:50:00,676 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=2379/tcp)
2025-10-25 17:50:00,684 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:50:00,684 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:50:00,684 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:50:00,685 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=zjcvswstvxfkcxmzxnzrlwcmjcdhogav] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-zjcvswstvxfkcxmzxnzrlwcmjcdhogav ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:50:00,722 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:50:00,802 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=2379/tcp)
2025-10-25 17:50:00,806 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=2379/tcp)
2025-10-25 17:50:00,858 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=2379/tcp)
2025-10-25 17:50:01,090 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:50:01.224190", "end": "2025-10-25 10:50:01.341284", "delta": "0:00:00.117094", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:50:01,542 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=2380/tcp)
2025-10-25 17:50:01,631 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=2380/tcp)
2025-10-25 17:50:01,632 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=2380/tcp)
2025-10-25 17:50:01,727 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=2380/tcp)
2025-10-25 17:50:02,404 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=2381/tcp)
2025-10-25 17:50:02,443 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=2381/tcp)
2025-10-25 17:50:02,447 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=2381/tcp)
2025-10-25 17:50:02,547 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=2381/tcp)
2025-10-25 17:50:03,231 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=30000:32767/tcp)
2025-10-25 17:50:03,266 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=30000:32767/tcp)
2025-10-25 17:50:03,267 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=30000:32767/tcp)
2025-10-25 17:50:03,406 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=30000:32767/tcp)
2025-10-25 17:50:04,033 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=8472/udp)
2025-10-25 17:50:04,073 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=8472/udp)
2025-10-25 17:50:04,088 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=8472/udp)
2025-10-25 17:50:04,217 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=8472/udp)
2025-10-25 17:50:04,863 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=6081/udp)
2025-10-25 17:50:04,887 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=6081/udp)
2025-10-25 17:50:04,927 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=6081/udp)
2025-10-25 17:50:05,050 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=6081/udp)
2025-10-25 17:50:05,687 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=4240/tcp)
2025-10-25 17:50:05,713 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=4240/tcp)
2025-10-25 17:50:05,751 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=4240/tcp)
2025-10-25 17:50:05,973 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=4240/tcp)
2025-10-25 17:50:06,518 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=4244/tcp)
2025-10-25 17:50:06,524 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=4244/tcp)
2025-10-25 17:50:06,593 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=4244/tcp)
2025-10-25 17:50:06,833 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=4244/tcp)
2025-10-25 17:50:07,355 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=4245/tcp)
2025-10-25 17:50:07,373 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=4245/tcp)
2025-10-25 17:50:07,755 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=4245/tcp)
2025-10-25 17:50:08,293 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=4222/tcp)
2025-10-25 17:50:08,334 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=4222/tcp)
2025-10-25 17:50:08,440 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=4245/tcp)
2025-10-25 17:50:08,737 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=4222/tcp)
2025-10-25 17:50:09,135 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=9966/tcp)
2025-10-25 17:50:09,185 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=9966/tcp)
2025-10-25 17:50:09,281 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=4222/tcp)
2025-10-25 17:50:09,596 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=9966/tcp)
2025-10-25 17:50:10,004 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=4250:4251/tcp)
2025-10-25 17:50:10,006 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=4250:4251/tcp)
2025-10-25 17:50:10,134 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=9966/tcp)
2025-10-25 17:50:10,586 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=4250:4251/tcp)
2025-10-25 17:50:10,836 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=6060:6062/tcp)
2025-10-25 17:50:10,841 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=6060:6062/tcp)
2025-10-25 17:50:10,982 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=4250:4251/tcp)
2025-10-25 17:50:11,443 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=6060:6062/tcp)
2025-10-25 17:50:11,636 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=9878:9879/tcp)
2025-10-25 17:50:11,671 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=9878:9879/tcp)
2025-10-25 17:50:11,810 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=6060:6062/tcp)
2025-10-25 17:50:12,306 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=9878:9879/tcp)
2025-10-25 17:50:12,547 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=9890:9893/tcp)
2025-10-25 17:50:12,584 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=9890:9893/tcp)
2025-10-25 17:50:12,660 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=9878:9879/tcp)
2025-10-25 17:50:13,180 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=9890:9893/tcp)
2025-10-25 17:50:13,429 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=9901/tcp)
2025-10-25 17:50:13,488 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=9901/tcp)
2025-10-25 17:50:13,500 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=9890:9893/tcp)
2025-10-25 17:50:14,033 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=9901/tcp)
2025-10-25 17:50:14,246 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=9962:9964/tcp)
2025-10-25 17:50:14,298 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=9962:9964/tcp)
2025-10-25 17:50:14,330 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=9901/tcp)
2025-10-25 17:50:15,061 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=51871/udp)
2025-10-25 17:50:15,140 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=51871/udp)
2025-10-25 17:50:15,192 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=9962:9964/tcp)
2025-10-25 17:50:15,860 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=9962:9964/tcp)
2025-10-25 17:50:15,955 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=4567/tcp)
2025-10-25 17:50:16,029 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=4567/tcp)
2025-10-25 17:50:16,058 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=51871/udp)
2025-10-25 17:50:16,095 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:50:16,096 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:50:16,096 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:50:16,096 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=airsynfcahowzlgkeuxvmydpxjaormob] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-airsynfcahowzlgkeuxvmydpxjaormob ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:50:16,141 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:50:16,528 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:50:16.658854", "end": "2025-10-25 10:50:16.779775", "delta": "0:00:00.120921", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:50:16,760 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=51871/udp)
2025-10-25 17:50:16,793 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=4567/udp)
2025-10-25 17:50:16,877 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=4567/udp)
2025-10-25 17:50:16,878 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=4567/tcp)
2025-10-25 17:50:17,592 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=4567/tcp)
2025-10-25 17:50:17,608 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=4568/tcp)
2025-10-25 17:50:17,715 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=4567/udp)
2025-10-25 17:50:17,722 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=4568/tcp)
2025-10-25 17:50:18,479 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=4444/tcp)
2025-10-25 17:50:18,485 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=4567/udp)
2025-10-25 17:50:18,567 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=4568/tcp)
2025-10-25 17:50:18,645 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=4444/tcp)
2025-10-25 17:50:19,392 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=4568/tcp)
2025-10-25 17:50:19,416 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=7443/tcp)
2025-10-25 17:50:19,484 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=4444/tcp)
2025-10-25 17:50:19,713 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=7443/tcp)
2025-10-25 17:50:20,280 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=4444/tcp)
2025-10-25 17:50:20,286 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=7345/tcp)
2025-10-25 17:50:20,305 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=7443/tcp)
2025-10-25 17:50:20,585 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=7345/tcp)
2025-10-25 17:50:21,118 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=8080/tcp)
2025-10-25 17:50:21,131 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=7345/tcp)
2025-10-25 17:50:21,142 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=7443/tcp)
2025-10-25 17:50:21,436 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=8080/tcp)
2025-10-25 17:50:21,959 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=8080/tcp)
2025-10-25 17:50:21,961 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=8443/tcp)
2025-10-25 17:50:22,017 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=7345/tcp)
2025-10-25 17:50:22,260 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=8443/tcp)
2025-10-25 17:50:22,781 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=8443/tcp)
2025-10-25 17:50:22,831 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=3306/tcp)
2025-10-25 17:50:22,849 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=8080/tcp)
2025-10-25 17:50:23,108 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=3306/tcp)
2025-10-25 17:50:23,628 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=3306/tcp)
2025-10-25 17:50:23,671 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=3307/tcp)
2025-10-25 17:50:23,725 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=8443/tcp)
2025-10-25 17:50:23,927 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=3307/tcp)
2025-10-25 17:50:24,465 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=3307/tcp)
2025-10-25 17:50:24,484 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=80/tcp)
2025-10-25 17:50:24,587 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=3306/tcp)
2025-10-25 17:50:24,784 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=80/tcp)
2025-10-25 17:50:25,273 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=80/tcp)
2025-10-25 17:50:25,291 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=443/tcp)
2025-10-25 17:50:25,424 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=3307/tcp)
2025-10-25 17:50:25,617 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=443/tcp)
2025-10-25 17:50:26,092 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=443/tcp)
2025-10-25 17:50:26,306 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=80/tcp)
2025-10-25 17:50:27,230 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=443/tcp)
2025-10-25 17:50:27,903 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Create RKE2 artifacts folder] ***************************************************
2025-10-25 17:50:27,904 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:50:27,954 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:50:27,964 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:50:27,973 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:50:28,721 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Download sha256 checksum file ( airgap mode )] **********************************
2025-10-25 17:50:28,721 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:50:28,723 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:50:28,740 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:50:28,754 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:50:29,499 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 artifacts and compare with checksums ( airgap mode )] *************
2025-10-25 17:50:29,499 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 17:50:29,525 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 17:50:29,560 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 17:50:29,590 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=rke2.linux-amd64.tar.gz)
2025-10-25 17:50:31,534 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:50:31,535 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:50:31,535 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:50:31,535 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=hkedddlfuyepimxnkteuhmcskkxioqdp] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-hkedddlfuyepimxnkteuhmcskkxioqdp ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:50:31,586 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:50:32,082 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:50:32.212914", "end": "2025-10-25 10:50:32.326615", "delta": "0:00:00.113701", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:50:33,415 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 17:50:33,495 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 17:50:33,537 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 17:50:35,046 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=rke2-images.linux-amd64.tar.gz)
2025-10-25 17:50:35,879 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 17:50:35,963 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 17:50:36,019 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 17:50:38,248 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=rke2-images-cilium.linux-amd64.tar.gz)
2025-10-25 17:50:38,295 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 17:50:38,411 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 17:50:38,437 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 17:50:39,385 p=66015 u=root n=ansible | ok: [k8s-w01] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 17:50:39,502 p=66015 u=root n=ansible | ok: [k8s-w02] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 17:50:39,524 p=66015 u=root n=ansible | ok: [k8s-w03] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 17:50:41,519 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=rke2-images-calico.linux-amd64.tar.gz)
2025-10-25 17:50:42,989 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=rke2-images-multus.linux-amd64.tar.gz)
2025-10-25 17:50:43,562 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 install script ( airgap mode )] ***********************************
2025-10-25 17:50:43,562 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:50:43,578 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:50:43,601 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:50:43,625 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:50:47,087 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:50:47,087 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:50:47,087 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:50:47,088 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=dpfbdtigvzzapbkxvsqfvarsfvnigjnk] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-dpfbdtigvzzapbkxvsqfvarsfvnigjnk ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:50:47,957 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:50:48,368 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:50:48.488770", "end": "2025-10-25 10:50:48.616912", "delta": "0:00:00.128142", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:50:50,008 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Populate service facts] *********************************************************
2025-10-25 17:50:50,009 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:50:50,050 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:50:50,057 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:50:50,241 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:50:50,600 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Get stats of the FS object] *****************************************************
2025-10-25 17:50:50,600 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:50:50,636 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:50:50,649 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:50:50,695 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:50:51,061 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Check if separate partition] ****************************************************
2025-10-25 17:50:51,061 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:50:51,075 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:50:51,101 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:50:51,145 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:50:51,228 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 bin path] **************************************************************
2025-10-25 17:50:51,228 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:50:51,248 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:50:51,250 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:50:51,266 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:50:51,923 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Check RKE2 version] *************************************************************
2025-10-25 17:50:51,923 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:50:51,925 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:50:51,927 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:50:51,951 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:50:52,020 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Set RKE2 versions] **************************************************************
2025-10-25 17:50:52,021 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:50:52,090 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:50:52,091 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:50:52,128 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:50:52,209 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Prevent accidental RKE2 downgrade] **********************************************
2025-10-25 17:50:52,211 p=66015 u=root n=ansible | ok: [k8s-m1] => changed=false 
  msg: |-
    Version check passed. Current running version: ['1.34.1'], desired version: ['1.34.1']."
2025-10-25 17:50:52,243 p=66015 u=root n=ansible | ok: [k8s-w01] => changed=false 
  msg: |-
    Version check passed. Current running version: ['1.34.1'], desired version: ['1.34.1']."
2025-10-25 17:50:52,244 p=66015 u=root n=ansible | ok: [k8s-w02] => changed=false 
  msg: |-
    Version check passed. Current running version: ['1.34.1'], desired version: ['1.34.1']."
2025-10-25 17:50:52,257 p=66015 u=root n=ansible | ok: [k8s-w03] => changed=false 
  msg: |-
    Version check passed. Current running version: ['1.34.1'], desired version: ['1.34.1']."
2025-10-25 17:50:52,876 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Find Active Server] *************************************************************
2025-10-25 17:50:52,975 p=66015 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/find_active_server.yml for k8s-m1, k8s-w01, k8s-w02, k8s-w03
2025-10-25 17:50:58,092 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Populate services facts] ********************************************************
2025-10-25 17:50:58,092 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:50:58,098 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:50:58,223 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:50:58,312 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:50:58,378 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Set the Active Server variable] *************************************************
2025-10-25 17:50:58,379 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=k8s-m1)
2025-10-25 17:50:58,421 p=66015 u=root n=ansible | ok: [k8s-m1 -> k8s-w01(10.0.6.14)] => (item=k8s-w01)
2025-10-25 17:50:58,423 p=66015 u=root n=ansible | ok: [k8s-m1 -> k8s-w02(10.0.6.15)] => (item=k8s-w02)
2025-10-25 17:50:58,432 p=66015 u=root n=ansible | ok: [k8s-m1 -> k8s-w03(10.0.6.16)] => (item=k8s-w03)
2025-10-25 17:50:59,117 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Download kubeconfig to ansible localhost] ***************************************
2025-10-25 17:50:59,167 p=66015 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/download_kubeconfig.yaml for k8s-m1
2025-10-25 17:50:59,813 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Download RKE2 kubeconfig to localhost] ******************************************
2025-10-25 17:50:59,813 p=66015 u=root n=ansible | changed: [k8s-m1]
2025-10-25 17:51:00,384 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Replace loopback IP by master server IP] ****************************************
2025-10-25 17:51:00,384 p=66015 u=root n=ansible | changed: [k8s-m1 -> localhost]
2025-10-25 17:51:00,460 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Prepare and join remaining nodes of the cluster] ********************************
2025-10-25 17:51:01,069 p=66015 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/remaining_nodes.yml for k8s-m1, k8s-w01, k8s-w02, k8s-w03
2025-10-25 17:51:01,559 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Create the RKE2 config dir] *****************************************************
2025-10-25 17:51:01,560 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:51:01,827 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:51:01,827 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:51:01,829 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:51:01,905 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Set server taints] **************************************************************
2025-10-25 17:51:01,906 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:51:02,032 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Set agent taints] ***************************************************************
2025-10-25 17:51:02,032 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:51:02,043 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:51:02,053 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:51:03,011 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Copy RKE2 config] ***************************************************************
2025-10-25 17:51:03,012 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:51:03,164 p=66015 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:51:03,171 p=66015 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:51:03,171 p=66015 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:51:03,374 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:51:03,375 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:51:03,375 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:51:03,375 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=pvxifxjisvqazaufcxcfiyohgrascepp] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-pvxifxjisvqazaufcxcfiyohgrascepp ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:51:03,438 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:51:03,831 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:51:03.957804", "end": "2025-10-25 10:51:04.080038", "delta": "0:00:00.122234", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:51:04,093 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Copy Containerd Registry Configuration file] ************************************
2025-10-25 17:51:04,094 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:51:04,123 p=66015 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:51:04,143 p=66015 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:51:04,148 p=66015 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:51:05,150 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Start RKE2 service on the rest of the nodes] ************************************
2025-10-25 17:51:05,151 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:51:18,836 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:51:18,837 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:51:18,837 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:51:18,837 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=nbfjncgucvbydudogsranmkdvukpipiw] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-nbfjncgucvbydudogsranmkdvukpipiw ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:51:18,882 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:51:19,248 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:51:19.383998", "end": "2025-10-25 10:51:19.499959", "delta": "0:00:00.115961", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:51:34,253 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:51:34,254 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:51:34,254 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:51:34,254 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=jyassfvxliwiictojylaybqqgkqdtrud] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-jyassfvxliwiictojylaybqqgkqdtrud ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:51:34,293 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:51:34,747 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:51:34.812626", "end": "2025-10-25 10:51:34.930391", "delta": "0:00:00.117765", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:51:49,752 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:51:49,752 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:51:49,752 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:51:49,753 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=qiesnmlvytxedhieyhdddhahqbdavsgy] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-qiesnmlvytxedhieyhdddhahqbdavsgy ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:51:49,794 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:51:50,162 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:51:50.302808", "end": "2025-10-25 10:51:50.415783", "delta": "0:00:00.112975", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:52:05,168 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:52:05,168 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:52:05,168 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:52:05,168 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=pzoivuybqzscjvgvintqggeorhyutugf] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-pzoivuybqzscjvgvintqggeorhyutugf ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:52:05,211 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:52:05,596 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:52:05.719566", "end": "2025-10-25 10:52:05.837970", "delta": "0:00:00.118404", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:52:20,601 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:52:20,602 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:52:20,602 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:52:20,602 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=lrwwfbokkomltpsxnrkdgpsommtiewyp] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-lrwwfbokkomltpsxnrkdgpsommtiewyp ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:52:20,642 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:52:21,047 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:52:21.158340", "end": "2025-10-25 10:52:21.297118", "delta": "0:00:00.138778", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:52:36,053 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:52:36,053 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:52:36,053 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:52:36,053 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=rbuyyxgtoocwpdurgmzooiuentwqfkfv] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-rbuyyxgtoocwpdurgmzooiuentwqfkfv ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:52:36,119 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:52:36,537 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:52:36.653699", "end": "2025-10-25 10:52:36.784174", "delta": "0:00:00.130475", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:52:48,048 p=66015 u=root n=ansible | changed: [k8s-w03]
2025-10-25 17:52:50,913 p=66015 u=root n=ansible | changed: [k8s-w01]
2025-10-25 17:52:51,542 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:52:51,542 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:52:51,543 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:52:51,543 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=frwaoyagpdpmznwtednqosqupfdpxzlp] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-frwaoyagpdpmznwtednqosqupfdpxzlp ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:52:51,587 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:52:51,996 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:52:52.115748", "end": "2025-10-25 10:52:52.237995", "delta": "0:00:00.122247", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:52:53,348 p=66015 u=root n=ansible | changed: [k8s-w02]
2025-10-25 17:52:54,271 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Mask other RKE2 service on the rest of the nodes] *******************************
2025-10-25 17:52:54,272 p=66015 u=root n=ansible | ok: [k8s-m1] => (item=agent)
2025-10-25 17:52:55,047 p=66015 u=root n=ansible | changed: [k8s-w02] => (item=server)
2025-10-25 17:53:07,002 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:53:07,003 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:53:07,003 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:53:07,003 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=mcgnenixodkopmbrdkkssqzqwtquhvrh] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-mcgnenixodkopmbrdkkssqzqwtquhvrh ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:53:07,061 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:53:07,471 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:53:07.603569", "end": "2025-10-25 10:53:07.725209", "delta": "0:00:00.121640", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:53:22,477 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:53:22,477 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:53:22,477 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:53:22,477 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=javcafdrbbjojjngjoaqraczlbmodiwn] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-javcafdrbbjojjngjoaqraczlbmodiwn ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:53:22,518 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:53:22,904 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "2", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:53:23.033315", "end": "2025-10-25 10:53:23.153032", "delta": "0:00:00.119717", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:53:36,495 p=66015 u=root n=ansible | changed: [k8s-w01] => (item=server)
2025-10-25 17:53:37,909 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:53:37,909 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:53:37,909 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:53:37,909 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=crbijtakixlesuzdpmcwyuffpexhfsbj] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-crbijtakixlesuzdpmcwyuffpexhfsbj ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:53:37,950 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:53:38,343 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "2", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:53:38.465744", "end": "2025-10-25 10:53:38.596052", "delta": "0:00:00.130308", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:53:53,349 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:53:53,349 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:53:53,349 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:53:53,349 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=ryzcpmeerpkzbkomjdtckmyezvvhlqpz] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-ryzcpmeerpkzbkomjdtckmyezvvhlqpz ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:53:53,389 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:53:53,777 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "2", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:53:53.909593", "end": "2025-10-25 10:53:54.026828", "delta": "0:00:00.117235", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:54:08,782 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:54:08,782 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:54:08,783 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:54:08,783 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=piyhpphvimxpethaxgrmucfdtwgrqblt] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-piyhpphvimxpethaxgrmucfdtwgrqblt ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:54:08,821 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:54:09,179 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "2", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:54:09.319738", "end": "2025-10-25 10:54:09.432396", "delta": "0:00:00.112658", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:54:24,184 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:54:24,184 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:54:24,185 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:54:24,185 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=iopwuhjuqztvrrmmgujkcclybocauanx] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-iopwuhjuqztvrrmmgujkcclybocauanx ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:54:24,223 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:54:24,636 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "2", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:54:24.772719", "end": "2025-10-25 10:54:24.891807", "delta": "0:00:00.119088", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:54:39,641 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:54:39,641 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:54:39,642 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:54:39,642 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=tzggwkdpilayacxxtisqrbceglipzqnq] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-tzggwkdpilayacxxtisqrbceglipzqnq ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:54:39,681 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:54:40,060 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "2", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:54:40.198207", "end": "2025-10-25 10:54:40.315815", "delta": "0:00:00.117608", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:54:55,065 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:54:55,065 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:54:55,065 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:54:55,065 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=mpeidxjqqiinjsrdqxagtdymqfnbenki] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-mpeidxjqqiinjsrdqxagtdymqfnbenki ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:54:55,103 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:54:55,478 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "2", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:54:55.607829", "end": "2025-10-25 10:54:55.730409", "delta": "0:00:00.122580", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:55:10,483 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:55:10,484 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:55:10,484 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:55:10,484 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=tusmhqjuralohzdiumszsbnirhvvjxpc] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-tusmhqjuralohzdiumszsbnirhvvjxpc ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:55:10,526 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:55:10,901 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "2", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:55:11.036872", "end": "2025-10-25 10:55:11.155089", "delta": "0:00:00.118217", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:55:25,910 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:55:25,910 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:55:25,911 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:55:25,913 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=ajrvdngsknksuftetmuxwvxvvrytrslr] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-ajrvdngsknksuftetmuxwvxvvrytrslr ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:55:25,968 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:55:26,367 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:55:26.485945", "end": "2025-10-25 10:55:26.622585", "delta": "0:00:00.136640", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:55:34,214 p=66015 u=root n=ansible | changed: [k8s-w03] => (item=server)
2025-10-25 17:55:41,372 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:55:41,372 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:55:41,373 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:55:41,373 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=npuzotmujagrbbtlovydseujkemgohcr] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-npuzotmujagrbbtlovydseujkemgohcr ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:55:41,411 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:55:41,795 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:55:41.927170", "end": "2025-10-25 10:55:42.051518", "delta": "0:00:00.124348", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:55:56,801 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:55:56,801 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:55:56,801 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:55:56,801 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=icvnmwbvuicxtzflnuqsxgxvfodpgufo] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-icvnmwbvuicxtzflnuqsxgxvfodpgufo ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:55:56,842 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:55:57,248 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "1", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:55:57.375965", "end": "2025-10-25 10:55:57.499299", "delta": "0:00:00.123334", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:56:12,254 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:56:12,254 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:56:12,255 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:56:12,255 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=jrxuxxivqlbfhlcbjkfqtxblkdxtfpqf] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-jrxuxxivqlbfhlcbjkfqtxblkdxtfpqf ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:56:12,297 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:56:12,669 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "3", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:56:12.801980", "end": "2025-10-25 10:56:12.919652", "delta": "0:00:00.117672", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:56:21,232 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Wait for remaining nodes to be ready - with CNI] ********************************
2025-10-25 17:56:21,233 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:56:21,744 p=66015 u=root n=ansible | RUNNING HANDLER [rke2-1.49.0 : Config file changed] *************************************************
2025-10-25 17:56:21,745 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:56:21,746 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:56:21,764 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:56:21,849 p=66015 u=root n=ansible | RUNNING HANDLER [rke2-1.49.0 : Service (re)started] *************************************************
2025-10-25 17:56:21,852 p=66015 u=root n=ansible | ok: [k8s-w01]
2025-10-25 17:56:21,852 p=66015 u=root n=ansible | ok: [k8s-w02]
2025-10-25 17:56:21,865 p=66015 u=root n=ansible | ok: [k8s-w03]
2025-10-25 17:56:22,103 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Final steps] ********************************************************************
2025-10-25 17:56:22,218 p=66015 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/summary.yml for k8s-m1, k8s-w01, k8s-w02, k8s-w03
2025-10-25 17:56:22,856 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : Prepare summary] ****************************************************************
2025-10-25 17:56:22,856 p=66015 u=root n=ansible | ok: [k8s-m1]
2025-10-25 17:56:22,923 p=66015 u=root n=ansible | TASK [rke2-1.49.0 : K8s nodes state] ****************************************************************
2025-10-25 17:56:22,925 p=66015 u=root n=ansible | ok: [k8s-m1] => 
  nodes_summary.stdout_lines:
  - NAME      STATUS   ROLES                AGE     VERSION          INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME         LABELS
  - k8s-m1    Ready    control-plane,etcd   20m     v1.34.1+rke2r1   10.0.6.11     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-m1,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=true,node-role.kubernetes.io/etcd=true
  - k8s-w01   Ready    <none>               3m32s   v1.34.1+rke2r1   10.0.6.14     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-w01,kubernetes.io/os=linux
  - k8s-w02   Ready    <none>               3m30s   v1.34.1+rke2r1   10.0.6.15     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-w02,kubernetes.io/os=linux
  - k8s-w03   Ready    <none>               3m35s   v1.34.1+rke2r1   10.0.6.16     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-w03,kubernetes.io/os=linux
2025-10-25 17:56:23,191 p=66015 u=root n=ansible | PLAY RECAP ******************************************************************************************
2025-10-25 17:56:23,191 p=66015 u=root n=ansible | k8s-m1                     : ok=41   changed=3    unreachable=0    failed=0    skipped=39   rescued=0    ignored=0   
2025-10-25 17:56:23,192 p=66015 u=root n=ansible | k8s-w01                    : ok=36   changed=5    unreachable=0    failed=0    skipped=44   rescued=0    ignored=0   
2025-10-25 17:56:23,192 p=66015 u=root n=ansible | k8s-w02                    : ok=36   changed=5    unreachable=0    failed=0    skipped=44   rescued=0    ignored=0   
2025-10-25 17:56:23,192 p=66015 u=root n=ansible | k8s-w03                    : ok=36   changed=5    unreachable=0    failed=0    skipped=44   rescued=0    ignored=0   
2025-10-25 17:56:27,675 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:56:27,675 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:56:27,675 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:56:27,676 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=mqrzvzykcuimwvotzwgsijzzoljnslbg] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-mqrzvzykcuimwvotzwgsijzzoljnslbg ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:56:27,725 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:56:28,105 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "4", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:56:28.244387", "end": "2025-10-25 10:56:28.359421", "delta": "0:00:00.115034", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:56:28,110 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Wait for remaining nodes to be ready - with CNI] *****************
2025-10-25 17:56:28,111 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  attempts: 44
  cmd: |-
    set -o pipefail
    /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep " Ready" | wc -l
  delta: '0:00:00.115034'
  end: '2025-10-25 10:56:28.359421'
  invocation:
    module_args:
      _raw_params: |-
        set -o pipefail
        /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep " Ready" | wc -l
      _uses_shell: true
      argv: null
      chdir: null
      creates: null
      executable: /bin/bash
      expand_argument_vars: true
      removes: null
      stdin: null
      stdin_add_newline: true
      strip_empty_ends: true
  msg: ''
  rc: 0
  start: '2025-10-25 10:56:28.244387'
  stderr: ''
  stderr_lines: <omitted>
  stdout: '4'
  stdout_lines: <omitted>
2025-10-25 17:56:28,345 p=55263 u=root n=ansible | META: triggered running handlers for k8s-m1
2025-10-25 17:56:28,379 p=55263 u=root n=ansible | RUNNING HANDLER [rke2-1.49.0 : Config file changed] **********************************
2025-10-25 17:56:28,380 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  ansible_facts:
    rke2_restart_needed: true
2025-10-25 17:56:28,506 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Rolling restart when config files change] ************************
2025-10-25 17:56:28,524 p=55263 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/change_config.yml for k8s-m1 => (item=k8s-m1)
2025-10-25 17:56:28,658 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/systemd.py
2025-10-25 17:56:28,659 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:56:28,659 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:56:28,660 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=tpgzcytkgsdmhsjrbkzxteuxunozhoff] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-tpgzcytkgsdmhsjrbkzxteuxunozhoff ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:56:28,704 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:58:10,420 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"name": "rke2-server.service", "changed": true, "status": {"Type": "notify", "ExitType": "main", "Restart": "always", "RestartMode": "normal", "NotifyAccess": "main", "RestartUSec": "5s", "RestartSteps": "0", "RestartMaxDelayUSec": "infinity", "RestartUSecNext": "5s", "TimeoutStartUSec": "infinity", "TimeoutStopUSec": "1min 30s", "TimeoutAbortUSec": "1min 30s", "TimeoutStartFailureMode": "terminate", "TimeoutStopFailureMode": "terminate", "RuntimeMaxUSec": "infinity", "RuntimeRandomizedExtraUSec": "0", "WatchdogUSec": "0", "WatchdogTimestampMonotonic": "0", "RootDirectoryStartOnly": "no", "RemainAfterExit": "no", "GuessMainPID": "yes", "MainPID": "9853", "ControlPID": "0", "FileDescriptorStoreMax": "0", "NFileDescriptorStore": "0", "FileDescriptorStorePreserve": "restart", "StatusErrno": "0", "Result": "success", "ReloadResult": "success", "CleanResult": "success", "UID": "[not set]", "GID": "[not set]", "NRestarts": "0", "OOMPolicy": "continue", "ReloadSignal": "1", "ExecMainStartTimestamp": "Sat 2025-10-25 10:32:53 UTC", "ExecMainStartTimestampMonotonic": "7092425848", "ExecMainExitTimestampMonotonic": "0", "ExecMainPID": "9853", "ExecMainCode": "0", "ExecMainStatus": "0", "ExecStartPre": "{ path=/sbin/modprobe ; argv[]=/sbin/modprobe overlay ; ignore_errors=yes ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStartPreEx": "{ path=/sbin/modprobe ; argv[]=/sbin/modprobe overlay ; flags=ignore-failure ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStart": "{ path=/usr/local/bin/rke2 ; argv[]=/usr/local/bin/rke2 server ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStartEx": "{ path=/usr/local/bin/rke2 ; argv[]=/usr/local/bin/rke2 server ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStopPost": "{ path=/bin/sh ; argv[]=/bin/sh -c systemd-cgls /system.slice/rke2-server.service | grep -Eo \'[0-9]+ (containerd|kubelet)\' | awk \'{print $1}\' | xargs -r kill ; ignore_errors=yes ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStopPostEx": "{ path=/bin/sh ; argv[]=/bin/sh -c systemd-cgls /system.slice/rke2-server.service | grep -Eo \'[0-9]+ (containerd|kubelet)\' | awk \'{print $1}\' | xargs -r kill ; flags=ignore-failure ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "Slice": "system.slice", "ControlGroup": "/system.slice/rke2-server.service", "ControlGroupId": "5969", "MemoryCurrent": "976089088", "MemoryPeak": "3275689984", "MemorySwapCurrent": "192512", "MemorySwapPeak": "192512", "MemoryZSwapCurrent": "0", "MemoryAvailable": "2589720576", "CPUUsageNSec": "328869368000", "EffectiveCPUs": "0-3", "EffectiveMemoryNodes": "0", "TasksCurrent": "136", "IPIngressBytes": "[no data]", "IPIngressPackets": "[no data]", "IPEgressBytes": "[no data]", "IPEgressPackets": "[no data]", "IOReadBytes": "[not set]", "IOReadOperations": "[not set]", "IOWriteBytes": "[not set]", "IOWriteOperations": "[not set]", "Delegate": "yes", "DelegateControllers": "cpu cpuset io memory pids", "CPUAccounting": "yes", "CPUWeight": "[not set]", "StartupCPUWeight": "[not set]", "CPUShares": "[not set]", "StartupCPUShares": "[not set]", "CPUQuotaPerSecUSec": "infinity", "CPUQuotaPeriodUSec": "infinity", "IOAccounting": "no", "IOWeight": "[not set]", "StartupIOWeight": "[not set]", "BlockIOAccounting": "no", "BlockIOWeight": "[not set]", "StartupBlockIOWeight": "[not set]", "MemoryAccounting": "yes", "DefaultMemoryLow": "0", "DefaultStartupMemoryLow": "0", "DefaultMemoryMin": "0", "MemoryMin": "0", "MemoryLow": "0", "StartupMemoryLow": "0", "MemoryHigh": "infinity", "StartupMemoryHigh": "infinity", "MemoryMax": "infinity", "StartupMemoryMax": "infinity", "MemorySwapMax": "infinity", "StartupMemorySwapMax": "infinity", "MemoryZSwapMax": "infinity", "StartupMemoryZSwapMax": "infinity", "MemoryLimit": "infinity", "DevicePolicy": "auto", "TasksAccounting": "yes", "TasksMax": "infinity", "IPAccounting": "no", "ManagedOOMSwap": "auto", "ManagedOOMMemoryPressure": "auto", "ManagedOOMMemoryPressureLimit": "0", "ManagedOOMPreference": "none", "MemoryPressureWatch": "auto", "MemoryPressureThresholdUSec": "200ms", "CoredumpReceive": "no", "EnvironmentFiles": "/usr/local/lib/systemd/system/rke2-server.env (ignore_errors=yes)", "UMask": "0022", "LimitCPU": "infinity", "LimitCPUSoft": "infinity", "LimitFSIZE": "infinity", "LimitFSIZESoft": "infinity", "LimitDATA": "infinity", "LimitDATASoft": "infinity", "LimitSTACK": "infinity", "LimitSTACKSoft": "8388608", "LimitCORE": "infinity", "LimitCORESoft": "infinity", "LimitRSS": "infinity", "LimitRSSSoft": "infinity", "LimitNOFILE": "1048576", "LimitNOFILESoft": "1048576", "LimitAS": "infinity", "LimitASSoft": "infinity", "LimitNPROC": "infinity", "LimitNPROCSoft": "infinity", "LimitMEMLOCK": "8388608", "LimitMEMLOCKSoft": "8388608", "LimitLOCKS": "infinity", "LimitLOCKSSoft": "infinity", "LimitSIGPENDING": "15158", "LimitSIGPENDINGSoft": "15158", "LimitMSGQUEUE": "819200", "LimitMSGQUEUESoft": "819200", "LimitNICE": "0", "LimitNICESoft": "0", "LimitRTPRIO": "0", "LimitRTPRIOSoft": "0", "LimitRTTIME": "infinity", "LimitRTTIMESoft": "infinity", "RootEphemeral": "no", "OOMScoreAdjust": "0", "CoredumpFilter": "0x33", "Nice": "0", "IOSchedulingClass": "2", "IOSchedulingPriority": "4", "CPUSchedulingPolicy": "0", "CPUSchedulingPriority": "0", "CPUAffinityFromNUMA": "no", "NUMAPolicy": "n/a", "TimerSlackNSec": "50000", "CPUSchedulingResetOnFork": "no", "NonBlocking": "no", "StandardInput": "null", "StandardOutput": "journal", "StandardError": "inherit", "TTYReset": "no", "TTYVHangup": "no", "TTYVTDisallocate": "no", "SyslogPriority": "30", "SyslogLevelPrefix": "yes", "SyslogLevel": "6", "SyslogFacility": "3", "LogLevelMax": "-1", "LogRateLimitIntervalUSec": "0", "LogRateLimitBurst": "0", "SecureBits": "0", "CapabilityBoundingSet": "cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read cap_perfmon cap_bpf cap_checkpoint_restore", "DynamicUser": "no", "SetLoginEnvironment": "no", "RemoveIPC": "no", "PrivateTmp": "no", "PrivateDevices": "no", "ProtectClock": "no", "ProtectKernelTunables": "no", "ProtectKernelModules": "no", "ProtectKernelLogs": "no", "ProtectControlGroups": "no", "PrivateNetwork": "no", "PrivateUsers": "no", "PrivateMounts": "no", "PrivateIPC": "no", "ProtectHome": "no", "ProtectSystem": "no", "SameProcessGroup": "no", "UtmpMode": "init", "IgnoreSIGPIPE": "yes", "NoNewPrivileges": "no", "SystemCallErrorNumber": "2147483646", "LockPersonality": "no", "RuntimeDirectoryPreserve": "no", "RuntimeDirectoryMode": "0755", "StateDirectoryMode": "0755", "CacheDirectoryMode": "0755", "LogsDirectoryMode": "0755", "ConfigurationDirectoryMode": "0755", "TimeoutCleanUSec": "infinity", "MemoryDenyWriteExecute": "no", "RestrictRealtime": "no", "RestrictSUIDSGID": "no", "RestrictNamespaces": "no", "MountAPIVFS": "no", "KeyringMode": "private", "ProtectProc": "default", "ProcSubset": "all", "ProtectHostname": "no", "MemoryKSM": "no", "RootImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "MountImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "ExtensionImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "KillMode": "process", "KillSignal": "15", "RestartKillSignal": "15", "FinalKillSignal": "9", "SendSIGKILL": "yes", "SendSIGHUP": "no", "WatchdogSignal": "6", "Id": "rke2-server.service", "Names": "rke2-server.service", "Requires": "system.slice sysinit.target", "Wants": "network-online.target", "WantedBy": "multi-user.target", "Conflicts": "shutdown.target rke2-agent.service", "Before": "multi-user.target shutdown.target", "After": "network-online.target basic.target system.slice systemd-journald.socket sysinit.target", "Documentation": "https://github.com/rancher/rke2#readme", "Description": "Rancher Kubernetes Engine v2 (server)", "LoadState": "loaded", "ActiveState": "active", "FreezerState": "running", "SubState": "running", "FragmentPath": "/usr/local/lib/systemd/system/rke2-server.service", "UnitFileState": "enabled", "UnitFilePreset": "enabled", "StateChangeTimestamp": "Sat 2025-10-25 10:35:37 UTC", "StateChangeTimestampMonotonic": "7257060396", "InactiveExitTimestamp": "Sat 2025-10-25 10:32:53 UTC", "InactiveExitTimestampMonotonic": "7092379564", "ActiveEnterTimestamp": "Sat 2025-10-25 10:35:37 UTC", "ActiveEnterTimestampMonotonic": "7257060396", "ActiveExitTimestampMonotonic": "0", "InactiveEnterTimestampMonotonic": "0", "CanStart": "yes", "CanStop": "yes", "CanReload": "no", "CanIsolate": "no", "CanFreeze": "yes", "StopWhenUnneeded": "no", "RefuseManualStart": "no", "RefuseManualStop": "no", "AllowIsolate": "no", "DefaultDependencies": "yes", "SurviveFinalKillSignal": "no", "OnSuccessJobMode": "fail", "OnFailureJobMode": "replace", "IgnoreOnIsolate": "no", "NeedDaemonReload": "no", "JobTimeoutUSec": "infinity", "JobRunningTimeoutUSec": "infinity", "JobTimeoutAction": "none", "ConditionResult": "yes", "AssertResult": "yes", "ConditionTimestamp": "Sat 2025-10-25 10:32:53 UTC", "ConditionTimestampMonotonic": "7092357804", "AssertTimestamp": "Sat 2025-10-25 10:32:53 UTC", "AssertTimestampMonotonic": "7092357806", "Transient": "no", "Perpetual": "no", "StartLimitIntervalUSec": "10s", "StartLimitBurst": "5", "StartLimitAction": "none", "FailureAction": "none", "SuccessAction": "none", "InvocationID": "b0d2a2fbe21c4c8f9d7bffc99a9906ef", "CollectMode": "inactive"}, "state": "started", "invocation": {"module_args": {"name": "rke2-server.service", "state": "restarted", "daemon_reload": false, "daemon_reexec": false, "scope": "system", "no_block": false, "enabled": null, "force": null, "masked": null}}}\n', b'')
2025-10-25 17:58:10,431 p=55263 u=root n=ansible | Notification for handler Service (re)started has been saved.
2025-10-25 17:58:10,433 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Restart RKE2 service on k8s-m1] **********************************
2025-10-25 17:58:10,445 p=55263 u=root n=ansible | changed: [k8s-m1] => changed=true 
  invocation:
    module_args:
      daemon_reexec: false
      daemon_reload: false
      enabled: null
      force: null
      masked: null
      name: rke2-server.service
      no_block: false
      scope: system
      state: restarted
  name: rke2-server.service
  state: started
  status:
    ActiveEnterTimestamp: Sat 2025-10-25 10:35:37 UTC
    ActiveEnterTimestampMonotonic: '7257060396'
    ActiveExitTimestampMonotonic: '0'
    ActiveState: active
    After: network-online.target basic.target system.slice systemd-journald.socket sysinit.target
    AllowIsolate: 'no'
    AssertResult: 'yes'
    AssertTimestamp: Sat 2025-10-25 10:32:53 UTC
    AssertTimestampMonotonic: '7092357806'
    Before: multi-user.target shutdown.target
    BlockIOAccounting: 'no'
    BlockIOWeight: '[not set]'
    CPUAccounting: 'yes'
    CPUAffinityFromNUMA: 'no'
    CPUQuotaPerSecUSec: infinity
    CPUQuotaPeriodUSec: infinity
    CPUSchedulingPolicy: '0'
    CPUSchedulingPriority: '0'
    CPUSchedulingResetOnFork: 'no'
    CPUShares: '[not set]'
    CPUUsageNSec: '328869368000'
    CPUWeight: '[not set]'
    CacheDirectoryMode: '0755'
    CanFreeze: 'yes'
    CanIsolate: 'no'
    CanReload: 'no'
    CanStart: 'yes'
    CanStop: 'yes'
    CapabilityBoundingSet: cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read cap_perfmon cap_bpf cap_checkpoint_restore
    CleanResult: success
    CollectMode: inactive
    ConditionResult: 'yes'
    ConditionTimestamp: Sat 2025-10-25 10:32:53 UTC
    ConditionTimestampMonotonic: '7092357804'
    ConfigurationDirectoryMode: '0755'
    Conflicts: shutdown.target rke2-agent.service
    ControlGroup: /system.slice/rke2-server.service
    ControlGroupId: '5969'
    ControlPID: '0'
    CoredumpFilter: '0x33'
    CoredumpReceive: 'no'
    DefaultDependencies: 'yes'
    DefaultMemoryLow: '0'
    DefaultMemoryMin: '0'
    DefaultStartupMemoryLow: '0'
    Delegate: 'yes'
    DelegateControllers: cpu cpuset io memory pids
    Description: Rancher Kubernetes Engine v2 (server)
    DevicePolicy: auto
    Documentation: https://github.com/rancher/rke2#readme
    DynamicUser: 'no'
    EffectiveCPUs: 0-3
    EffectiveMemoryNodes: '0'
    EnvironmentFiles: /usr/local/lib/systemd/system/rke2-server.env (ignore_errors=yes)
    ExecMainCode: '0'
    ExecMainExitTimestampMonotonic: '0'
    ExecMainPID: '9853'
    ExecMainStartTimestamp: Sat 2025-10-25 10:32:53 UTC
    ExecMainStartTimestampMonotonic: '7092425848'
    ExecMainStatus: '0'
    ExecStart: '{ path=/usr/local/bin/rke2 ; argv[]=/usr/local/bin/rke2 server ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'
    ExecStartEx: '{ path=/usr/local/bin/rke2 ; argv[]=/usr/local/bin/rke2 server ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'
    ExecStartPre: '{ path=/sbin/modprobe ; argv[]=/sbin/modprobe overlay ; ignore_errors=yes ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'
    ExecStartPreEx: '{ path=/sbin/modprobe ; argv[]=/sbin/modprobe overlay ; flags=ignore-failure ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'
    ExecStopPost: '{ path=/bin/sh ; argv[]=/bin/sh -c systemd-cgls /system.slice/rke2-server.service | grep -Eo ''[0-9]+ (containerd|kubelet)'' | awk ''{print $1}'' | xargs -r kill ; ignore_errors=yes ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'
    ExecStopPostEx: '{ path=/bin/sh ; argv[]=/bin/sh -c systemd-cgls /system.slice/rke2-server.service | grep -Eo ''[0-9]+ (containerd|kubelet)'' | awk ''{print $1}'' | xargs -r kill ; flags=ignore-failure ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'
    ExitType: main
    ExtensionImagePolicy: root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent
    FailureAction: none
    FileDescriptorStoreMax: '0'
    FileDescriptorStorePreserve: restart
    FinalKillSignal: '9'
    FragmentPath: /usr/local/lib/systemd/system/rke2-server.service
    FreezerState: running
    GID: '[not set]'
    GuessMainPID: 'yes'
    IOAccounting: 'no'
    IOReadBytes: '[not set]'
    IOReadOperations: '[not set]'
    IOSchedulingClass: '2'
    IOSchedulingPriority: '4'
    IOWeight: '[not set]'
    IOWriteBytes: '[not set]'
    IOWriteOperations: '[not set]'
    IPAccounting: 'no'
    IPEgressBytes: '[no data]'
    IPEgressPackets: '[no data]'
    IPIngressBytes: '[no data]'
    IPIngressPackets: '[no data]'
    Id: rke2-server.service
    IgnoreOnIsolate: 'no'
    IgnoreSIGPIPE: 'yes'
    InactiveEnterTimestampMonotonic: '0'
    InactiveExitTimestamp: Sat 2025-10-25 10:32:53 UTC
    InactiveExitTimestampMonotonic: '7092379564'
    InvocationID: b0d2a2fbe21c4c8f9d7bffc99a9906ef
    JobRunningTimeoutUSec: infinity
    JobTimeoutAction: none
    JobTimeoutUSec: infinity
    KeyringMode: private
    KillMode: process
    KillSignal: '15'
    LimitAS: infinity
    LimitASSoft: infinity
    LimitCORE: infinity
    LimitCORESoft: infinity
    LimitCPU: infinity
    LimitCPUSoft: infinity
    LimitDATA: infinity
    LimitDATASoft: infinity
    LimitFSIZE: infinity
    LimitFSIZESoft: infinity
    LimitLOCKS: infinity
    LimitLOCKSSoft: infinity
    LimitMEMLOCK: '8388608'
    LimitMEMLOCKSoft: '8388608'
    LimitMSGQUEUE: '819200'
    LimitMSGQUEUESoft: '819200'
    LimitNICE: '0'
    LimitNICESoft: '0'
    LimitNOFILE: '1048576'
    LimitNOFILESoft: '1048576'
    LimitNPROC: infinity
    LimitNPROCSoft: infinity
    LimitRSS: infinity
    LimitRSSSoft: infinity
    LimitRTPRIO: '0'
    LimitRTPRIOSoft: '0'
    LimitRTTIME: infinity
    LimitRTTIMESoft: infinity
    LimitSIGPENDING: '15158'
    LimitSIGPENDINGSoft: '15158'
    LimitSTACK: infinity
    LimitSTACKSoft: '8388608'
    LoadState: loaded
    LockPersonality: 'no'
    LogLevelMax: '-1'
    LogRateLimitBurst: '0'
    LogRateLimitIntervalUSec: '0'
    LogsDirectoryMode: '0755'
    MainPID: '9853'
    ManagedOOMMemoryPressure: auto
    ManagedOOMMemoryPressureLimit: '0'
    ManagedOOMPreference: none
    ManagedOOMSwap: auto
    MemoryAccounting: 'yes'
    MemoryAvailable: '2589720576'
    MemoryCurrent: '976089088'
    MemoryDenyWriteExecute: 'no'
    MemoryHigh: infinity
    MemoryKSM: 'no'
    MemoryLimit: infinity
    MemoryLow: '0'
    MemoryMax: infinity
    MemoryMin: '0'
    MemoryPeak: '3275689984'
    MemoryPressureThresholdUSec: 200ms
    MemoryPressureWatch: auto
    MemorySwapCurrent: '192512'
    MemorySwapMax: infinity
    MemorySwapPeak: '192512'
    MemoryZSwapCurrent: '0'
    MemoryZSwapMax: infinity
    MountAPIVFS: 'no'
    MountImagePolicy: root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent
    NFileDescriptorStore: '0'
    NRestarts: '0'
    NUMAPolicy: n/a
    Names: rke2-server.service
    NeedDaemonReload: 'no'
    Nice: '0'
    NoNewPrivileges: 'no'
    NonBlocking: 'no'
    NotifyAccess: main
    OOMPolicy: continue
    OOMScoreAdjust: '0'
    OnFailureJobMode: replace
    OnSuccessJobMode: fail
    Perpetual: 'no'
    PrivateDevices: 'no'
    PrivateIPC: 'no'
    PrivateMounts: 'no'
    PrivateNetwork: 'no'
    PrivateTmp: 'no'
    PrivateUsers: 'no'
    ProcSubset: all
    ProtectClock: 'no'
    ProtectControlGroups: 'no'
    ProtectHome: 'no'
    ProtectHostname: 'no'
    ProtectKernelLogs: 'no'
    ProtectKernelModules: 'no'
    ProtectKernelTunables: 'no'
    ProtectProc: default
    ProtectSystem: 'no'
    RefuseManualStart: 'no'
    RefuseManualStop: 'no'
    ReloadResult: success
    ReloadSignal: '1'
    RemainAfterExit: 'no'
    RemoveIPC: 'no'
    Requires: system.slice sysinit.target
    Restart: always
    RestartKillSignal: '15'
    RestartMaxDelayUSec: infinity
    RestartMode: normal
    RestartSteps: '0'
    RestartUSec: 5s
    RestartUSecNext: 5s
    RestrictNamespaces: 'no'
    RestrictRealtime: 'no'
    RestrictSUIDSGID: 'no'
    Result: success
    RootDirectoryStartOnly: 'no'
    RootEphemeral: 'no'
    RootImagePolicy: root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent
    RuntimeDirectoryMode: '0755'
    RuntimeDirectoryPreserve: 'no'
    RuntimeMaxUSec: infinity
    RuntimeRandomizedExtraUSec: '0'
    SameProcessGroup: 'no'
    SecureBits: '0'
    SendSIGHUP: 'no'
    SendSIGKILL: 'yes'
    SetLoginEnvironment: 'no'
    Slice: system.slice
    StandardError: inherit
    StandardInput: 'null'
    StandardOutput: journal
    StartLimitAction: none
    StartLimitBurst: '5'
    StartLimitIntervalUSec: 10s
    StartupBlockIOWeight: '[not set]'
    StartupCPUShares: '[not set]'
    StartupCPUWeight: '[not set]'
    StartupIOWeight: '[not set]'
    StartupMemoryHigh: infinity
    StartupMemoryLow: '0'
    StartupMemoryMax: infinity
    StartupMemorySwapMax: infinity
    StartupMemoryZSwapMax: infinity
    StateChangeTimestamp: Sat 2025-10-25 10:35:37 UTC
    StateChangeTimestampMonotonic: '7257060396'
    StateDirectoryMode: '0755'
    StatusErrno: '0'
    StopWhenUnneeded: 'no'
    SubState: running
    SuccessAction: none
    SurviveFinalKillSignal: 'no'
    SyslogFacility: '3'
    SyslogLevel: '6'
    SyslogLevelPrefix: 'yes'
    SyslogPriority: '30'
    SystemCallErrorNumber: '2147483646'
    TTYReset: 'no'
    TTYVHangup: 'no'
    TTYVTDisallocate: 'no'
    TasksAccounting: 'yes'
    TasksCurrent: '136'
    TasksMax: infinity
    TimeoutAbortUSec: 1min 30s
    TimeoutCleanUSec: infinity
    TimeoutStartFailureMode: terminate
    TimeoutStartUSec: infinity
    TimeoutStopFailureMode: terminate
    TimeoutStopUSec: 1min 30s
    TimerSlackNSec: '50000'
    Transient: 'no'
    Type: notify
    UID: '[not set]'
    UMask: '0022'
    UnitFilePreset: enabled
    UnitFileState: enabled
    UtmpMode: init
    WantedBy: multi-user.target
    Wants: network-online.target
    WatchdogSignal: '6'
    WatchdogTimestampMonotonic: '0'
    WatchdogUSec: '0'
2025-10-25 17:58:10,554 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:58:10,555 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:58:10,555 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:58:10,556 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=vtgjdpycgylsfmkdmunrrnmaxohxxmkg] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-vtgjdpycgylsfmkdmunrrnmaxohxxmkg ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:58:10,634 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:58:11,844 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "4", "stderr": "", "rc": 0, "cmd": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "start": "2025-10-25 10:58:11.949365", "end": "2025-10-25 10:58:12.099941", "delta": "0:00:00.150576", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -o pipefail\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep \\" Ready\\" | wc -l\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:58:11,852 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Wait for all nodes to be ready again] ****************************
2025-10-25 17:58:11,853 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  attempts: 1
  cmd: |-
    set -o pipefail
    /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep " Ready" | wc -l
  delta: '0:00:00.150576'
  end: '2025-10-25 10:58:12.099941'
  invocation:
    module_args:
      _raw_params: |-
        set -o pipefail
        /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes | grep " Ready" | wc -l
      _uses_shell: true
      argv: null
      chdir: null
      creates: null
      executable: /bin/bash
      expand_argument_vars: true
      removes: null
      stdin: null
      stdin_add_newline: true
      strip_empty_ends: true
  msg: ''
  rc: 0
  start: '2025-10-25 10:58:11.949365'
  stderr: ''
  stderr_lines: <omitted>
  stdout: '4'
  stdout_lines: <omitted>
2025-10-25 17:58:11,957 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Final steps] *****************************************************
2025-10-25 17:58:11,991 p=55263 u=root n=ansible | included: /opt/git/ansible/roles/rke2-1.49.0/tasks/summary.yml for k8s-m1
2025-10-25 17:58:12,069 p=55263 u=root n=ansible | Using module file /usr/lib/python3.12/site-packages/ansible/modules/command.py
2025-10-25 17:58:12,070 p=55263 u=root n=ansible | Pipelining is enabled.
2025-10-25 17:58:12,070 p=55263 u=root n=ansible | <10.0.6.11> ESTABLISH SSH CONNECTION FOR USER: ubuntu
2025-10-25 17:58:12,070 p=55263 u=root n=ansible | <10.0.6.11> SSH: EXEC ssh -o ControlMaster=auto -o ControlPersist=300s -o ServerAliveInterval=60 -o ServerAliveCountMax=10 -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o 'User="ubuntu"' -o ConnectTimeout=3000 -o 'ControlPath="/root/.ansible/cp/%h-%r"' 10.0.6.11 '/bin/sh -c '"'"'sudo -H -S -p "[sudo via ansible, key=dstgewwgkaqtwhlhzqhibjcztjapsaaf] password:" -u root /bin/sh -c '"'"'"'"'"'"'"'"'echo BECOME-SUCCESS-dstgewwgkaqtwhlhzqhibjcztjapsaaf ; /usr/bin/python3'"'"'"'"'"'"'"'"' && sleep 0'"'"''
2025-10-25 17:58:12,122 p=55263 u=root n=ansible | Escalation succeeded
2025-10-25 17:58:12,586 p=55263 u=root n=ansible | <10.0.6.11> (0, b'\n{"changed": true, "stdout": "NAME      STATUS   ROLES                AGE     VERSION          INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME         LABELS\\nk8s-m1    Ready    control-plane,etcd   22m     v1.34.1+rke2r1   10.0.6.11     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-m1,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=true,node-role.kubernetes.io/etcd=true\\nk8s-w01   Ready    <none>               5m21s   v1.34.1+rke2r1   10.0.6.14     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-w01,kubernetes.io/os=linux\\nk8s-w02   Ready    <none>               5m19s   v1.34.1+rke2r1   10.0.6.15     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-w02,kubernetes.io/os=linux\\nk8s-w03   Ready    <none>               5m24s   v1.34.1+rke2r1   10.0.6.16     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-w03,kubernetes.io/os=linux", "stderr": "", "rc": 0, "cmd": "set -e\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes -o wide --show-labels\\n", "start": "2025-10-25 10:58:12.690178", "end": "2025-10-25 10:58:12.836570", "delta": "0:00:00.146392", "msg": "", "invocation": {"module_args": {"executable": "/bin/bash", "_raw_params": "set -e\\n/var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes -o wide --show-labels\\n", "_uses_shell": true, "expand_argument_vars": true, "stdin_add_newline": true, "strip_empty_ends": true, "argv": null, "chdir": null, "creates": null, "removes": null, "stdin": null}}}\n', b'')
2025-10-25 17:58:12,589 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : Prepare summary] *************************************************
2025-10-25 17:58:12,590 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  attempts: 1
  cmd: |-
    set -e
    /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes -o wide --show-labels
  delta: '0:00:00.146392'
  end: '2025-10-25 10:58:12.836570'
  invocation:
    module_args:
      _raw_params: |-
        set -e
        /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes -o wide --show-labels
      _uses_shell: true
      argv: null
      chdir: null
      creates: null
      executable: /bin/bash
      expand_argument_vars: true
      removes: null
      stdin: null
      stdin_add_newline: true
      strip_empty_ends: true
  msg: ''
  rc: 0
  start: '2025-10-25 10:58:12.690178'
  stderr: ''
  stderr_lines: <omitted>
  stdout: |-
    NAME      STATUS   ROLES                AGE     VERSION          INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME         LABELS
    k8s-m1    Ready    control-plane,etcd   22m     v1.34.1+rke2r1   10.0.6.11     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-m1,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=true,node-role.kubernetes.io/etcd=true
    k8s-w01   Ready    <none>               5m21s   v1.34.1+rke2r1   10.0.6.14     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-w01,kubernetes.io/os=linux
    k8s-w02   Ready    <none>               5m19s   v1.34.1+rke2r1   10.0.6.15     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-w02,kubernetes.io/os=linux
    k8s-w03   Ready    <none>               5m24s   v1.34.1+rke2r1   10.0.6.16     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-w03,kubernetes.io/os=linux
  stdout_lines: <omitted>
2025-10-25 17:58:12,633 p=55263 u=root n=ansible | TASK [rke2-1.49.0 : K8s nodes state] *************************************************
2025-10-25 17:58:12,634 p=55263 u=root n=ansible | ok: [k8s-m1] => 
  nodes_summary.stdout_lines:
  - NAME      STATUS   ROLES                AGE     VERSION          INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME         LABELS
  - k8s-m1    Ready    control-plane,etcd   22m     v1.34.1+rke2r1   10.0.6.11     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-m1,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=true,node-role.kubernetes.io/etcd=true
  - k8s-w01   Ready    <none>               5m21s   v1.34.1+rke2r1   10.0.6.14     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-w01,kubernetes.io/os=linux
  - k8s-w02   Ready    <none>               5m19s   v1.34.1+rke2r1   10.0.6.15     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-w02,kubernetes.io/os=linux
  - k8s-w03   Ready    <none>               5m24s   v1.34.1+rke2r1   10.0.6.16     <none>        Ubuntu 24.04.3 LTS   6.8.0-86-generic   containerd://2.1.4-k3s2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-w03,kubernetes.io/os=linux
2025-10-25 17:58:12,699 p=55263 u=root n=ansible | RUNNING HANDLER [rke2-1.49.0 : Service (re)started] **********************************
2025-10-25 17:58:12,700 p=55263 u=root n=ansible | ok: [k8s-m1] => changed=false 
  ansible_facts:
    rke2_restart_needed: false
2025-10-25 17:58:12,729 p=55263 u=root n=ansible | PLAY RECAP ***************************************************************************
2025-10-25 17:58:12,730 p=55263 u=root n=ansible | k8s-m1                     : ok=46   changed=4    unreachable=0    failed=0    skipped=39   rescued=0    ignored=0   
